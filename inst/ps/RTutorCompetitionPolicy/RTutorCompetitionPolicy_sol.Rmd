# Competition Policy and Industrial Policy
### Exploring the Effects with R
###### Author: Julian Latzko

#< ignore
```{r "create_ps"}
library(RTutor)
setwd("C:/Users/ThinkPad User/Documents/RTutorCompetitionPolicy")
ps.name = "RTutorCompetitionPolicy"; sol.file = paste0(ps.name,"_sol.Rmd")
# character vector of all packages you load in the problem set
libs = c("ggplot2", "dplyr", "stargazer", "lfe", "treemap", "tidyr", "skimr", "gridExtra", "corrplot", "AER", "ggridges", "knitr") 
create.ps(sol.file=sol.file, ps.name=ps.name,libs=libs, rps.has.sol=TRUE, addons="quiz", extra.code.file = "addvar.R")
# Show the problem set in the webbrowser
show.ps(ps.name, sample.solution=FALSE, auto.save.code = TRUE)
stop.without.error()
```
#>

Welcome! This problem set is part of my master thesis at the University of Ulm. You are about to dive into one of the most fundamental and interesting issues in economics: the role of the state and its interventions via economic policies. Prominent examples for such interventions are **competition policies** - rules and measurements intended to enhance competition and to deter firms from engaging in anti-competitive behavior (Belleflamme and Peitz (2015), p. 729). A rich body of economic literature has shown that product market competition is beneficial to society by increasing innovation, productivity and social welfare (see, for example, Aghion et al. (2005) and Nickell (1996)). However, whether **competition policy** is an effective tool to improve economic outcomes is subject to a lasting discourse (Voigt (2006), p. 2). Should the state get involved in competition policy, after all?

The first part of this problem set is based on the article **"Competition Policy and Productivity Growth: an empirical Assessment"** by Buccirossi et al. (2013). Using an industry-level dataset consisting of 12 OECD countries, we investigate whether there is a causal link between competition policy and productivity growth.

Whereas competition policy aims at maintaining or enhancing competition, another type of policy intervention is commonly criticized precisely for obstructing competitive principles: **industrial policy** (Aiginger (2014), p. 8). Salient examples for industrial policies are subsidies, tax holidays and tariffs. The major theoretical justification for such measurements is to protect domestic firms that are not (yet) competitive from foreign pressure (Harrison and Rodr√≠guez-Clare (2009), p. 4041). However, favoring some firms and industries over others may undermine product market competition. Is there a way to reconcile industrial policy with sound principles of competition?

The second part of this problem set is based on the article **"Industrial Policy and Competition"** by the authors Aghion et al. (2015). Using a Chinese firm-level dataset, we explore whether **competition-friendly** industrial policies are more beneficial to productivity. Throughout this problem set, you will learn about econometric analysis, how to effectively perform such an analysis using R, and of course, the aforementioned policies. I hope you will enjoy it!

You can find article and data on the subject of competition policy here:
- https://www.mitpressjournals.org/doi/abs/10.1162/REST_a_00304
- https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/25293

Article and data on the subject of industrial policy are available here:
- https://www.aeaweb.org/articles?id=10.1257/mac.20120103


## Exercise Content

  1. Introduction to Competition Policy
  
  2. Exploring the Data
  
  3. A Simple Linear Regression Model
  
  4. Omitted Variable Bias and Endogeneity
  
  5. Econometric Model and Results
  
  6. Exploring the Competition Policy Indexes
  
  7. Instrumental Variable Estimation
  
  8. Remaining Issues and Heterogeneous Effects of Competition Policy
  
  9. Introduction to Industrial Policy
  
  10. Exploring Industrial Policy in China
  
  11. Industrial Policy and Competition
  
  12. Econometric Model and Results II
  
  13. Conclusion
  
  14. References
  
Apart from some basic economics and statistics, this problem set presumes little prior knowledge. Each exercise can be solved independently. In principal, you should also be able to understand the part on industrial policy without having (fully) solved the first part on competition policy. This is especially the case if you are already familiar with multiple linear regression and R. Yet I strongly recommend sticking to the given structure. The discussion regularly picks up on results from previous exercises. Here are some instructions on how to handle the upcoming tasks:

- To solve the first task of an exercise, you have to click the `edit` button in the upper left corner of the task panel.
- Enter the required code and press `check` to let R do the computations. Annotations indicate where to complete or replace R code.
- If you struggle with a task, clicking on `hint` shows a tip that might help you.
- If you are stuck, the `solution` button provides the full code. 

Some tasks are already prepared to avoid repetition and to keep the required input from you limited. In the first exercise, we briefly introduce some fundamentals of competition policy and the question that guides our analysis throughout the first part of this problem set.

## Exercise 1 -- Introduction to Competition Policy

Competition policy comprises rules intended to maintain or enhance product market competition (Belleflamme and Peitz (2015), p. 729). Market participants have to comply with these rules - or face punishment. Competition policy rests, broadly, on two pillars. *Antitrust*-related laws and authorities deal with direct violations of competitive principles, such as firms engaging in price-fixing or exclusionary practices (Belleflamme and Peitz (2015), Chapters 14, 16, 17). *Mergers*-related laws and authorities handle fusions of firms (Belleflamme and Peitz (2015), Chapter 15).

A prototype of modern competition policy is the *Sherman Antitrust Act*, passed by the United States in 1890 (Belleflamme and Peitz (2015), pp. 729-730). This legislation was a response to the formation of cartels and trusts in some key industries during the second half of the 19th century. The *Sherman Antitrust Act* categorically prohibited price agreements among competitors and other anti-competitive behaviors. Since then, competition policy has seen a remarkable surge. While there were only two countries with dedicated antitrust laws before 1900 (Canada and the US), this number increased to about 90 by the beginning of the 21st century (Voigt (2006), p. 29).

The economic justification of competition policy is its presumed effect on *innovation*, *consumer surplus* and *social welfare*. In their theoretical framework, Aghion and Schankerman (2004, p. 801) identify three sources of welfare gains obtained by competition-enhancing interventions:

  - Through increased competition, low-cost firms acquire a larger market share. This reduces the average production cost.
  - Intensified competition reduces the incentives for inefficient firms to enter the market.
  - Incumbent firms are encouraged to innovate and perform cost reduction investments.

The empirical evidence on competition policy is less clear-cut, though. To give an example, Baker (2003) states that the antitrust policies in the United States were successful, whereas Crandall and Winston (2003) argue that these policies have neither been effective in raising welfare, nor in deterring anti-competitive behavior. A key issue in the assessment is how to accurately quantify an entire policy regime. Some studies have focused on obtaining subjective opinions on whether certain policies are "good" (Voigt (2006), pp. 2-3). Whether the expectations of market participants adequately reflect the quality of such policies is questionable, to say the least.

Buccirossi et al. (2011) tackle this issue by presenting a set of indicators that measure the deterrence properties of competition policies in a jurisdiction. The **Competition Policy Indexes (CPIs)** are obtained by scoring the quality of relevant policies against a benchmark of generally agreed-upon best practices. Several low-level indicators, summarizing specific aspects of a policy regime, are combined in the **Aggregate Competition Policy Index (CPI)**, which scores the overall deterrence properties.

Using this indicator, Buccirossi et al. (2013) put forward an empirical assessment of competition policy in 12 OECD countries. In the following first part of this problem set, we investigate the presented approach aimed at estimating the *causal effect* of competition policy on country-industry productivity. Finding a positive link would provide some evidence that such policy interventions are economically beneficial. We also supplement the results presented in the article with some further interpretations and visualizations. In exercise 2, we begin our analysis with an exploration of the data and introduce useful R functionality.

## Exercise 2 -- Exploring the Data

### a) The Variables of Interest

To quickly dive into the analysis, I prepared the file `dat.rds`, which contains the most relevant variables of the dataset used by Buccirossi et al. (2013, henceforth referred to as "the authors"). To load the file into R, you can use the `readRDS()` function. Throughout this problem set, I provide information on how to use important functions and further details in info boxes. Just click on an info box to view its content. If you are already familiar with the function or the discussed subject, you might skip these boxes.

#< info "The readRDS() function"
Files in R's data format `.rds` can be loaded into R using the `readRDS()` function. The function takes the name of a file as an argument (including its format ending). Note that the file name has to be quoted. The content of the file can be assigned to a variable using the `=` operator. This code snippet loads a `.rds` file into R and assigns it to an object called `data`:  
```{r eval=FALSE}
data = readRDS("file.rds") 
```
The `readRDS()` function has more arguments, but we do not need them here. If you want to learn more about the function, check out the documentation.
https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/readRDS
#>

**Task:** Load the file `dat.rds` into R using the `readRDS()` function. Assign the data to a variable named `dat`. I already gave you part of the solution; just complete/replace the function call below and click `check`.

```{r}
#< fill_in
# Insert the correct expressions for the placeholders ___ below.
___ = readRDS(___)
#>
dat = readRDS("dat.rds")
#< hint
cat("Just refer to the info box above again.")
#>
```

#< award "Starter"
Welcome! Throughout this problem set, you will earn awards for completing important tasks and quizzes. Enjoy solving it!
#>

The object `dat` is a *data frame*, R's default format for tabular data. We can use the `head()` function to get a first look at its content. The function's output shows the column names and the upper rows of the data and omits the rest.

#< info "The head() function"
The `head()` function returns the upper rows of R objects, such as data frames. The first argument is the object name; here it is `dat`:
```{r eval=FALSE}
head(dat, n = 5)
```
The second argument, `n`, specifies the number of rows to be returned. If `n` is not chosen explicitly, the function uses a default value of `n = 6`. For more information, refer to the function documentation. https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head
#>

**Task:** Use the `head()` function to show the upper rows of `dat`. You can use the function's default value for `n`.  
```{r}
#< task
# Enter code below
#>
head(dat)
#< hint
cat("Just type in 'head(dat)'.")
#>
```

The data frame `dat` contains the full set of variables we will use throughout our analysis. Since we do not need all of them yet, we want to retain only the main variables of interest and drop the rest. We introduce the remaining variables as needed in later exercises. We can pick specific columns of a data frame in R using the `select()` function of the `dplyr` package. The `dplyr` package offers rich functionality for data handling. We will use several of its functions in the upcoming exercises. To keep this problem set concise, I provide the full code for tasks that would involve a lot of typing or require elaborate explanations. For those tasks, simply click on `check` to run the code. 

#< info "The select() function and the dplyr package"
The `dplyr` package offers a set of functions for data handling tasks, such as selecting or creating new variables. To use the package, we have to load it first:
```{r eval=FALSE}
library(dplyr)
```
The `select()` function allows to pick specific columns of a data frame and drop the rest. The first argument is the name of the data frame to be used. The subsequent arguments specify the columns to retain. This function calls creates a data frame `new`, which contains two columns `x1` and `x2` of `dat`.
```{r eval=FALSE}
new = select(dat, x1, x2)
```
We can also pick a set of consecutive columns using the `:` operator:
```{r eval=FALSE}
new = select(dat, x1:x5)
```
This code snippet picks `x1` and `x5` of `dat` and all columns in between. If you want to learn more about the `select()` function, refer to Wickham and Grolemund (2016, pp. 51-54) or check out the function documentation. 
https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/select
#>

**Task:** Select the columns `id`, `industry`, `country`, `year`, `tfp` and `cpi` of `dat`, and assign the output to `dat`. Use `head()` to show the upper entries again. The code is already given below; just click `check`.
```{r}
#< task_notest
library(dplyr)
dat = select(dat, id, industry, country, year, cpi, tfp)

head(dat)
#>
```

We loaded a *panel dataset*, where we follow a set of *country-industries* over time (Kennedy (2008), Chapter 18). The first column `id` is an identifier that takes on a unique integer value for a specific country-industry. This identifier was constructed by combining the columns `industry` and `country`. The column `industry` indicates an industry type using the *International Standard Industrial Classification (ISIC Rev.3)* code by the UNO (2002). For example, `01-05` in the first row stands for agriculture, forestry and fishing, whereas `10-14`, in the fourth row, represents the mining and quarrying industry. The variable `country` contains an ISO country code abbreviation; `Can` refers to Canada. Looking at the column `year`, we see that we are dealing with annual observations.

Both remaining columns contain our main variables of interest. The already mentioned aggregate index for the quality of competition policy regimes (CPI) is stored in `cpi`. By construction, the CPI takes on values between $0$ and $1$, where higher values indicate better deterrence properties of a competition policy regime. Since the CPI is derived from policy features on the level of a jurisdiction, all industries within a country share the same value for a given year. You can confirm this by comparing `cpi` for the Canadian agriculture industry, `01-05`, with the Canadian mining industry, `10-14`. In exercise 6, we investigate the construction of the CPI and what information it incorporates in more detail. 

The last column, `tfp`, contains the outcome variable we are interested in: total factor productivity (TFP) growth. TFP is an efficiency parameter relating output to inputs used in a production process (labor and capital). TFP growth is best understood as a measure for process innovation, achieved through technological progress and cost reduction investment (Aghion et al. (2015), p. 9). In our case, TFP growth was estimated on the country-industry level using the *growth accounting methodology* (see, for example, Griffith et al. (2004)). In the following, we introduce the core idea behind this concept. The framework is based on a differenced *Cobb-Douglas production function* of which the natural logarithm is taken (Kennedy (2008), p. 96). This allows decomposing industry output growth into growth of capital inputs, labor inputs, and productivity:
$$
\Delta TFP_{i,j,t} = ln(\frac{Y_{i, j, t}}{Y_{i,j,t-1}}) - \frac{1}{2}(\alpha_{i, j, t} + \alpha_{i, j, t-1}) \ ln(\frac{L_{i, j, t}}{L_{i,j,t-1}}) \\\ - (1 - \frac{1}{2}(\alpha_{i, j, t} + \alpha_{i, j, t-1})) \ ln(\frac{K_{i, j, t}}{K_{i,j,t-1}}) \tag{2.0}
$$
where $Y_{i, j, t}$ is industry output, measured in real value added, in country $i$, industry $j$ at time $t$, and $L_{i, j, t}$, $K_{i, j, t}$ are labor and capital inputs, respectively. By subtracting labor and capital input growth from output growth, we obtain TFP growth $\Delta TFP_{i,j,t}$. Intuitively, TFP growth is the residual output growth - that is to say, output growth that cannot be explained by changes in the labor and capital inputs (also called the *Solow residual*). To illustrate, suppose the food products industry in the United Kingdom (UK) increased its real value added by $3$ percent from year 2000 to 2001. If this output growth is due to an increase of labor and capital used in the production process, the industry has not become more efficient. TFP growth would be zero. If, however, the industry used the same amount of factor inputs in 2001 as in 2000, TFP growth will be about $3$ percent.

The term $\alpha_{i, j, t}$ in $(2.0)$ measures the labor share in value added. Since labor share is quite volatile, an average of the years $t$ and $t-1$ is used. A problematic assumption of the growth accounting framework is that product markets are perfectly competitive. To relax this assumption, the authors estimated country-industry specific markups using the following equation:
$$
Markup_{i,j,t} = \frac{ValueAdded_{i,j,t}}{LaborCosts_{i,j,t} + CapitalCosts_{i,j,t}} \tag{2.1}
$$
where $ValueAdded_{i,j,t}$ is country-industry value added and $LabourCosts_{i,j,t}$ and $CapitalCosts_{i,j,t}$ are compensation for labor and capital, respectively. The country-industry specific markups are then multiplied with the labor share $\alpha_{i, j, t}$ in $(2.0)$ to obtain TFP growth corrected for markups. For a more elaborate discussion on the *growth accounting framework*, refer to the online appendix of Buccirossi et al. (2013) and Griffith et al. (2004).

To learn more about the main variables in our analysis, we compute some basic summary statistics in the next task. The `skim()` function of the `skimr` package provides an easy way to obtain several summary statistics with a single function call. I customized a version of the function: `my_skim()`.  

#< info "The skim() function"
The `skim()` function computes several summary statistics for data frames and variables. The function uses a set of default statistics that can be changed using the `skim_with()` function. By assigning the output to a new variable, a customized version of the function is created.
```{r eval=FALSE}
library(skimr)
my_skim = skim_with(character = sfl(n_unique = n_unique), 
                    numeric = sfl(mean = ~ mean(., na.rm = TRUE), 
                                  sd = ~ sd(., na.rm = TRUE), 
                                  min = ~ min(., na.rm = TRUE), 
                                  max = ~ max(., na.rm = TRUE)), 
                    append = FALSE)
skim(dat)
```
The function `my_skim()`, which we will use throughout this problem set, focuses on the most basic statistics. To learn more about the `skimr` package, refer to its documentation. https://cran.r-project.org/web/packages/skimr/index.html
#>

**Task:** Use the `my_skim()` function to display summary statistics of the variables in `dat`. I already loaded `my_skim()` in the background. The code is already given below; just click `check`.
```{r}
#< task_notest
library(skimr)
print(my_skim(dat))
#>
```

Looking at the output of the function call, we see that the data contain $1847$ observations in total. Both columns `n_missing` and `complete_rate` indicate that there are no missing values. The column `n_unique` shows how many distinct values the four *character* variables take on. Thus, we study $256$ country-industries, consisting of $22$ industrial subsectors in $12$ OECD countries. Most countries are European, such as the UK, Italy and Germany, but there are also three non-European countries: Canada, Japan and the US. The panel covers a period of 10 years.

We proceed with our main variables of interest: `cpi` and `tfp`. The CPI's mean value is about $0.49$ with a standard deviation of $0.105$. In the sample, we observe values ranging from $0.31$ and $0.70$. The full range of possible values for the CPI is therefore not observed. The exact value of a country's CPI has, similar to other indexes, no meaning. The purpose of the CPI is to allow for cross-country and cross-time comparisons, where a larger CPI indicates better quality of competition policy. Looking at the summary statistics for `tfp`, we see that the mean annual TFP growth across the whole sample was about $1\% \ (= 0.0095)$. We can interpret this as follows: 

- Using the same amount of factor inputs (i.e. labor hours, machine hours, etc.), country-industries increased their real output by about $1$ percent per year, on average. Alternatively:
- Firms reduced their factor inputs by about $1$ percent per year and still produced the same output.

TFP growth rates are ranging from $-28\%$ and $27\%$. Some observations below and above these values (corresponding to the $1$st and $99$th percentile) have been treated as outliers and were discarded from the sample. To further get an intuition for the CPI and TFP growth, we plot two histograms in the next task. For most visualizations throughout this problem set, we use the `ggplot2` package. This package offers extensive functionality to create publishing quality plots in R. Since writing code for `ggplot2` takes a bit time getting used to, I provide the complete code for all visualizations to come.
 
#< info "The ggplot2 and the gridExtra package"
The `ggplot2` package offers a graphical language to plot layered graphics in R. Core part are the so-called `geoms`, which define the graphical elements of a plot.
```{r eval = FALSE}
library(ggplot2)
plot1 = ggplot(data = dat, aes(x = x1)) +
   geom_histogram()
plot1
```
For a simple example, the code snippet above plots a histogram of variable `x1` stored in `dat`. If you want to learn how to make your own graphs using `ggplot2`, check out Wickham & Grolemund (2016, Chapters 1, 5 and 22) or refer to the package documentation. 
https://cran.r-project.org/web/packages/ggplot2/index.html

Several plots created by the `ggplot()` function can be combined in a single visualization using the `grid.arrange()` function of the `gridExtra` package.
```{r eval = FALSE}
library(gridExtra)
grid.arrange(plot1, plot2, nrow = 2, ncol = 1)
```
This code fragment uses two `ggplot2` objects `plot1` and `plot2` and combines them in one visualization. For more information on the `gridExtra` package, refer to the documentation.
https://cran.r-project.org/web/packages/gridExtra/index.html
#>

**Task:** Plot histograms that show the distribution of `tfp` and `cpi` using `ggplot()` and `grid.arrange()`. Also include a line indicating the means of the variables. The code is already given below; just click `check`.
```{r out.width=550, dpi=700}
#< task_notest
library(ggplot2)
library(gridExtra)

# Histogram TFP growth
t = ggplot(data = dat, aes(x = tfp)) +
  geom_histogram(bins = 45, colour = "#6794a7", fill = "#6794a7",
                 alpha = 0.6) + 
  geom_vline(aes(xintercept = mean(tfp)), 
             color = "#d23e4e", linetype = 2, size = 0.6) +
  theme_blue() +
  labs(title = "TFP Growth", x = "", y = "Count", caption = "")

# Histogram CPI
c = ggplot(data = dat, aes(x = cpi)) +
  geom_histogram(bins = 30, colour = "#6794a7", fill = "#6794a7",
                 alpha = 0.6) + 
  geom_vline(aes(xintercept = mean(cpi)), 
             color = "#d23e4e", linetype = 2, size = 0.6) +
  theme_blue() +
  labs(title = "CPI", x = "", y = "Count", 
       caption = "Figure 2.0: Histogram TFP Growth and CPI")

# Combine histograms
grid.arrange(t, c, nrow = 2, ncol = 1)
#>
```

We can see that the TFP growth rates form roughly a bell-shaped distribution centered on the mean, indicated by the red line. The distribution of the CPI is less well behaved. It is more uniformly distributed across its range, with gaps in between. Recall that there are much less unique values for the CPI, since competition policy is defined on the level of a jurisdiction.
 
### b) The Food Products Industry in the UK

Whenever we want to illustrate an aspect throughout this first part of the problem set, we refer to an exemplary country-industry: the food products industry in the United Kingdom. The UK shows the most pronounced trajectory in its CPI among all countries and is therefore worth studying. First, we look at the CPI's development over the sample period. To do so, we isolate the rows of `dat` that meet the corresponding criteria for the `country` and `industry` variable. This kind of operation can easily be done with the `filter()` function. 

#< info "The filter() function"
The `filter()` function of the `dplyr` package creates a subset of a data frame based on one or more conditions. The first argument is the name of the data frame. The second argument specifies conditions using logical statements. The output of the function are the rows of the passed data frame where the conditions evaluate to "true". This function call isolates all rows of `dat` that take on the value `abc` for the column `x1`:
```{r eval=FALSE}
filter(dat, x1 == "abc") 
```
Note that the logical "is equal to" is written as `==`. Multiple criteria can be combined in `filter()`, using operators such as `&` ("and"). This function call isolates all rows of `dat` that take on the value `abc` for `x1` and `xyz` for `x2`:   
```{r eval=FALSE}
filter(dat, x1 == "abc" & x2 == "xyz") 
```
To store the output in an object, we can assign it to a new variable using the `=` operator.
```{r eval=FALSE}
new = filter(dat, x1 == "abc" & x2 == "xyz") 
```
If you want to learn more about the `filter()` function, check out Wickham and Grolemund (2016, p. 45) or read the function documentation. https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/filter
#>

**Task:** Use `filter()` to show the data on UK's food products industry. The `industry` code is `15-16` and the `country` code is `UK`. Combine these two criteria using the `&` operator. I already gave you part of the solution; just complete/replace the function call and click `check`.
```{r}
#< fill_in
# Insert the correct expressions for the placeholders ___ below. 
filter(dat, ___ & ___)
#>
filter(dat, industry == "15-16" & country == "UK")
#< hint
cat("If you struggle with this task, refer to the info box above.")
#>
```

#< award "dplyr Novice"
Congratulations! You successfully used filter() to slice a data frame.
#>

The output displays the development of the CPI and TFP growth from 1996 to 2005. The CPI of the UK shows a substantial increase over these years. Looking at `tfp`, we can see that the food products industry endured negative TFP growth until 2001. To illustrate these developments, we plot line charts in the next two tasks. We also want to compare the UK to the other countries by including the sample averages in the plots. To speed things up, I created an extra file `countries.rds` which simply contains the full data on the CPIs and the sample average for each year. We could also use `dat`, but the CPI was lagged by one year so we would lose one observation for each country. The data in the file has the same format as the output of the `filter()` function above.

**Task:** Load the file `countries.RDS`. Then, show a line graph of the CPI and the average CPI across all countries using `ggplot()`. The code is already given below; just click `check`.
```{r out.width=600, dpi=700}
#< task_notest
# Load data prepared for visualization
countries = readRDS("countries.RDS")

# Plot line chart
library(ggplot2)
countries %>% 
  filter(country %in% c("UK", "Average")) %>% 
  ggplot(aes(x = as.numeric(year), y = cpi, colour = country, 
             label = round(cpi, digits = 2))) + 
  geom_line(size = 1.0) + 
  geom_point(size = 1.5) + 
  geom_text(nudge_y = 0.03, size = 3.00) + 
  scale_x_continuous(breaks = 1995:2005) + 
  facet_wrap(~ country, nrow = 2, ncol = 1) + 
  coord_fixed(ratio = 12) + 
  theme_blue() + 
  scale_color_manual(values = colours_custom) + 
  labs(title = "CPI for the UK", x = "Year", y = "CPI", 
       caption = "Figure 2.1: CPI for the UK")
#>
```

As mentioned above, we see a substantial rise of the UK's CPI over the sample period. Its value started at about $0.31$ in 1995, and almost doubled until 2005. A particularly strong increase was in 2000. In exercise 6, we investigate the concrete policies that contributed to this trajectory. The panel below shows an upward trend for the average CPI: from $0.45$ in 1995 to $0.52$ in 2005. This suggests that the quality of competition policy has improved in the studied OECD countries, on average. In the next task, we visualize TFP growth of UK's food products industry and compare it to the sample average. Similar to the previous task, I created a file `food.RDS`, which contains the aggregate data on the industries.  

**Task:** Use `ggplot()` to illustrate TFP growth of UK's food products industry. Include the sample average of all food products industries. The code is already given below; just click `check`.
```{r out.width=600, dpi=700}
#< task_notest
# Load data prepared for visualization
food = readRDS("food.RDS")

# Plot line chart
food %>% 
  filter(country %in% c("UK", "Average")) %>% 
  ggplot(aes(x = as.numeric(year), y = tfp, colour = country, 
             label = round(tfp, digits = 2))) + 
  geom_line(size = 1.0) + 
  geom_point(size = 1.5) + 
  geom_text(nudge_y = 0.0175, size = 3.00) + 
  geom_hline(yintercept = 0, color = "black", size = 0.5,
             alpha = 0.6, linetype = 2) + 
  scale_x_continuous(breaks = 1996:2005) + 
  facet_wrap(~ country, nrow = 2, ncol = 1) + 
  coord_fixed(ratio = 25) + 
  theme_blue() + 
  scale_color_manual(values = colours_custom) + 
  labs(title = "Food Product Industry in the UK", 
       x = "Year", y = "TFP growth", 
       caption = "Figure 2.2: Food Product Industry in the UK")
#>
```

From 1996 to 2000, we observe negative TFP growth rates - the food product industry in the UK seems to have lost efficiency. In the second half of the sample period, the industry exhibited positive growth rates, also outperforming the industry average (with the exception of 2003). The second panel indicates that the food products business does not appear to be driven by major technological progress and innovation: productivity growth fluctuates roughly around a value of zero. Throughout the first part of this problem set, we are interested in the relationship between competition policy, as measured by the CPI, and productivity growth. If competition policy has, as hypothesized, a positive causal effect on productivity growth, we might expect a positive relationship between both variables in the data. To spot whether there is such an association, we display a scatterplot in the next task.

**Task:** Show a scatterplot with `cpi` on the x-axis and `tfp` on the y-axis using `ggplot()`. Additionally, include a linear regression line. The code is already given below, just click `check`.
```{r out.width=600, dpi=700}
#< task_notest
# Scatter Plot CPI and TFP
ggplot(data = dat, aes(x = cpi, y = tfp)) +
  geom_point(colour = "#6794a7", shape = 21, size = 1.0) +
  geom_smooth(method = "lm", formula = "y ~ x",
              se = FALSE, colour = "#d23e4e") +
  coord_fixed(ratio = 0.6) +
  theme_blue() +
  labs(title = "Scatterplot CPI and TFP",
       x = "CPI",
       y = "TFP growth",
       caption = "Figure 2.3: Scatterplot CPI and TFP")
#>
```

#< award "Visualization Novice"
Congratulations! You used ggplot2 in this exercise to create scatter plots, line charts and histograms.
#>

#< quiz "prediction"
question: Consider the plot above. For country-industries with a low CPI of 0.31 we would predict an average TFP growth...

sc:
    - ... well below zero.
    - ... about zero.*
    - ... well above zero. 
    
success: Great!
failure: Try again.
#>

#< quiz "prediction2"
question: Consider the plot above. For country-industries with a high CPI of 0.7 we would predict a TFP growth...

sc:
    - ... close to zero.
    - ... about 1.5 percent.
    - ... about 3.0 percent.*
    - ... about 4.5 percent.
    
success: Great!
failure: Try again.
#>


Each data point corresponds to an annual country-industry outcome. Obviously, there is a lot of variation in productivity outcomes. Yet we can see a positive correlation between both variables. The superimposed regression line, which we motivate more rigorously in the next exercise, magnifies the relationship: For countries with a low CPI of about $0.31$, we would predict TFP growth to be close to zero. For countries with a high CPI of about $0.7$, we would predict TFP growth to be roughly $3$ percent. Recall that the average productivity growth in the sample is approximately $1$ percent. Thus, a difference of $3$ percentage points seems to be relevant from an economic point of view. However, we have to be careful with our interpretation regarding the observed relationship. In the upcoming exercises, we elaborate on major issues that arise when we seek to infer causal relationships from observational data. Using econometric tools such as *multiple linear regression*, *fixed effects* and *instrumental variable estimation*, we properly investigate whether there is a causal link between competition policy and productivity growth. The next exercise introduces the linear regression model and its most basic application.

## Exercise 3 -- A Simple Linear Regression Model

### a) A Brief Introduction to Regression Theory

An empirical study in econometrics starts with an economic theory. Based on the theory, we specify a deterministic relationship among certain variables of interest. Written down in mathematical form, we call such a deterministic relationship a *population model*. By applying econometric tools to data, we estimate unknown parameters of the population model or try to evaluate the validity of the underlying theory. The most important tool in this context is linear regression (Greene (2003), Chapter 2.1). The linear regression model allows to study the relationship between a dependent variable $y$ and one or more independent variables $x_1, x_2,..., x_k$. The generic model can be written as
$$
y = \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_k + \varepsilon \tag{3.0}
$$
where $\beta_1,...,\beta_k$ are the population coefficients and $\varepsilon$ is an error term. The population coefficients $\beta_{1,...,k}$ quantify the effect a change in the respective independent variable has on $y$, holding the rest of the variables and the error term constant. All other factors that are not included in the model but influence $y$, are captured in the error term $\varepsilon$. The matrix form is a concise way to write the linear regression model:
$$
y = X \beta + \varepsilon \tag{3.1}
$$
where $y$ is a $n \times 1$ column vector, $X$ is a $n \times k$ matrix of independent variables, $\beta$ is the vector of $k$ population coefficients and $\varepsilon$ the $n \times 1$ column vector of disturbances (Greene (2003), Chapter 2.2). The linear regression model is based on a set of crucial assumptions about the underlying *data-generating process*. A data-generating process specifies how a dataset is produced from the population. We can state the assumptions of the *classical linear regression model (CLRM)* as follows:     

- **A1 (Linearity):** The population model is linear in parameters: $y = X \beta + \varepsilon$.
- **A2 (Full rank):** There are no exact linear relationships among any of the independent variables. That is to say, $X$ has full rank.
- **A3 (Exogeneity of the independent variables):** The expected value of each error term is zero given any values of the independent variables: $\mathbb{E}(\varepsilon_i|X) = 0$.
- **A4 (Homoscedasticity and zero autocorrelation):** Each error $\varepsilon_i$ has a finite, constant variance given any values of the independent variables, and the covariance among each error $\varepsilon_i$ and $\varepsilon_j$ is zero, for all $i \not{=} j$: $Var(\varepsilon|X) = \sigma^2I_{n}$.
- **A5 (Exogenously generated data):** $X$ may be fixed or random.
- **A6 (Normality):** The errors are normally distributed with zero mean and constant variance: $\varepsilon|X \sim N(0, \sigma^2I_{n})$.

If these assumptions are met, linear regression allows appropriate estimation and inference procedures (Greene (2003), Chapter 2.3). However, in most econometric applications one or more of the CLRM assumptions are violated (Kennedy (2008), pp. 40-41). Additional effort is required in order to allow for valid estimation and inference. We will encounter some of these issues throughout this problem set.

### b) The OLS Estimator and its Properties

The most frequently used method to estimate the unknown population coefficients $\beta$ is the method of *ordinary least squares (OLS)* (Greene (2003), Chapter 3 & 4). Using a sample of data, OLS minimizes the sum of squared residuals by choosing the optimal vector of coefficient estimates $\hat\beta$. In matrix form, the algebraic solution to the least squares problem is:
$$
\hat\beta = (X'X)^{-1}X'y. \tag{3.2}
$$
The *Gauss-Markov theorem* postulates beneficial statistical properties of the OLS estimator. Under the Assumptions **A1** to **A5**, the estimator is the minimum variance linear unbiased estimator (**MVLUE**). We can summarize these properties as follows:

* The estimator is **linear**.
* It is **unbiased**, i.e. the expected value of the coefficient estimates equals the population coefficients: $\mathbb{E}(\hat\beta) = \beta$.
* **Consistency** implies that the estimator converges to the population parameter as the sample size grows to infinity.
* It has **minimum variance** among all linear unbiased estimators. We also call this property **efficiency**.

With the addition of **A6**, normality of the error term, we obtain exact test statistics which are useful for inference (Greene (2003), p. 17). In the next section, we apply the CLRM to our question at hand. 

### c) A Simple Regression Model for Competition Policy

Is there a relationship between good competition policy and productivity growth? If so, how strong is this relationship? Now that we have laid the groundwork for regression analysis, we can translate these questions into a simple linear regression model: We want to estimate the causal effect of competition policy on TFP growth $\Delta TFP_{i,j,t}$ in country $i$, industry $j$ at time $t$: 
$$
\Delta TFP_{i,j,t} = \beta_0 + \beta_1 CPI_{i, t-1} + \varepsilon_{i,j,t} \tag{3.3}
$$
where $CPI_{i, t-1}$ is the index for the quality of competition policy in country $i$ at time $t-1$, and $\varepsilon_{i,j,t}$ is the error term. We call $(3.3)$ a "simple" regression model, since it includes only one independent variable. We are interested in estimating the CPI's population parameter $\beta_1$, which we consider the causal effect of competition policy on productivity growth. If we were to find a positive coefficient, this would support the hypothesis that good competition policy fosters productivity growth. Furthermore, the size of the coefficient can tell us something about the economic relevance the policy has. This is important, since a minor positive effect could still shed doubt on the justification of such policy regimes. In the upcoming tasks, we estimate the simple model in $(3.3)$ using OLS and interpret the results.

**Task:** Load `dat.rds` again and use `select()` to retain the main variables of interest. Show the upper entries of `dat` using the `head()` function. The code is already given below; just click `check`.
```{r}
#< task_notest
library(dplyr)
dat = readRDS("dat.rds")

# Select and display main variables of interest
dat = select(dat, id, industry, country, year, tfp, cpi)
head(dat, n = 5)
#>
```

We already know the displayed variables from the previous exercise. In order to estimate the simple regression model in $(3.3)$, we make use of the `felm()` function. While base R has a function to perform linear regression as well, we need some additional functionality throughout this problem set, which the `felm()` function offers.

#< info "The felm() function"
The `felm()` function is part of the `lfe` package, which offers extensive functionality to perform linear regression. To run a simple regression, we have to pass two arguments to `felm()`. The first argument specifies the formula - essentially our econometric model translated into R code. The dependent variable has to be separated from the independent variable by the `~` symbol. As a second argument, `data`, we have to pass the name of the data frame containing the variables. The following call regresses `y` on `x` using the data frame `dat`. The results are then stored in a variable `reg`. 
```{r eval=FALSE}
library(lfe)
reg = felm(y ~ x, data = dat)
```
We introduce additional functionality of `felm()` as needed. To learn more, check out the documentation of the `lfe` package. 
https://cran.r-project.org/web/packages/lfe/index.html
#>

**Task:** Use the `felm()` function to regress `tfp` on `cpi` and assign the results to a variable named `simple`. I already gave you part of the solution; just complete/replace the function call and click `check`.
```{r message=FALSE}
#< fill_in
library(lfe)

# Insert the correct expressions for the placeholders ___ below.
___ = felm(___, data = dat)
#>
library(lfe)
simple = felm(tfp ~ cpi, data = dat)
#< hint
cat("If you struggle with the syntax of the felm() function, read the info box above again.")
#>
```

#< award "Regression Novice"
Congratulations! You have successfully run a simple regression with the felm() function.
#>

To inspect the results, we make use of the `summary()` function. When passed a regression object, the function shows an overview of the most important results.

#< info "The summary() function" 
`summary()` is a generic function. That is, its output depends on the passed object. When passed a statistical model, the function prints a selection of relevant statistics.
```{r eval=FALSE}
summary(model)
```
The function can also be applied to other objects (such as data frames) to compute summary statistics, like means and medians. To learn more, refer to the function documentation.
https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/summary
#>

**Task:** Show the results of the regression model `simple` using `summary()`. Complete/replace the code below and click `check`.
```{r}
#< task
# Enter code below
#>
summary(simple)
#< hint
cat("If you stuggle with this task, read the info box above.")
#>
```

Looking at the output, we see that the function reports several statistics. We focus on the most important ones here. As stated earlier, of primary interest to us is the coefficient of `cpi`, which is estimated to be $0.067$. We can interpret this coefficient as follows: An increase of the CPI by $1$ is associated with an increase of TFP growth by $0.067$. Since the CPI ranges merely between about $0.31$ and $0.7$ in the sample, it is more appropriate to look at a "typical change" in the variable, though. In the previous exercise, we saw that the sample standard deviation of the CPI is about $0.11$. Thus, a one standard deviation increase of the CPI is associated with an increase of productivity growth by $0.00737$ ( $= 0.11 \cdot 0.067$ ) - about $0.74$ percentage points. In other words, if a country's CPI increases permanently by $0.11$, we would predict that industry-level TFP growth increases by $0.74$ percentage points, on average. Recall that the mean productivity growth in the sample was about $1$ percent. Thus, the estimated coefficient hints at a potentially relevant effect of competition policy. The estimate is also significantly different from zero at any conventional level, as indicated by the small p-value in the `Pr(>|t|)` column. It is unlikely to observe the coefficient's magnitude under the null hypothesis that the true parameter is zero. If you are not yet familiar with statistical inference, refer to Wooldridge (2016, Chapter 4 and Appendix C).

Can we interpret the coefficient $\hat{\beta_1} = 0.067$ in a *causal* way? To answer this question, we should first clarify what we deem to be the causal effect of interest here: The causal effect of competition policy is the additional productivity growth industries would receive, if a country had better competition policy (Angrist and Pischke (2009), p. 3 and Chapter 2). In the sciences, the most reliable approach to estimate causal relationships are randomized experiments. In an ideal experiment, we would randomly assign different competition policy regimes to several countries and measure how productivity outcomes change as a response (Angrist and Pischke (2009), pp. 3-4 and Chapter 2). Random assignment is crucial here, because it ensures that the characteristics of the studied units are, on average, the same for all groups within the experiment. If our data came from such an ideal experiment, the simple regression estimate could be interpreted in a causal way. However, in economics we often study units (such as countries or industries) that cannot be manipulated at will. It is obviously infeasible to "randomly assign" policies to countries. Policy regimes evolve over lengthy periods, subject to decision-making processes in the corresponding countries. Therefore, more often than not, we have to rely on observational data in economics. Simple regression estimates obtained from analyzing such data may have a causal interpretation but usually they have not. Thus, we have to be cautious with our claims regarding the estimated effect.

Fortunately, our attempt to shed light on the causal relationship between competition policy and productivity is not futile. A large part of the econometric toolbox consists of methods that try to *identify* causal effects in the absence of experimental data. "Identification of causal effects" in this context means the way we use observational data in order to approximate an ideal, randomized experiment (Angrist and Pischke (2009), p. 6). As part of our identification strategy in this problem set, we use popular methods such as *multiple linear regression*, *fixed effects*, *instrumental variable estimation* and the exploitation of *heterogeneous effects*. Before we proceed with these issues, we deal with an issue of statistical inference: correcting the standard errors of the regression estimates.

### d) Correcting the Standard Errors

The standard error (SE) of a regression coefficient is an estimate for the standard deviation of the coefficient's sampling distribution (Greene (2003), Chapter 4.4). We use the SE in computing t-statistics, p-values and confidence intervals. Since we consider a coefficient to be significant if its p-value is below a certain threshold (usually $5\%$), obtaining a valid SE is important in regression analysis. If the assumptions of the CLRM are met, the SEs of the regression coefficients $\hat\beta$ are obtained by computing the covariance matrix:
$$
Var(\hat{\beta}|X) = \sigma^2 (X'X)^{-1}. \tag{3.4}
$$
In the simple regression case (including an intercept), the SE of the coefficient $\hat{\beta_1}$ is given by the square root of the lower right element of the covariance matrix:
$$
SE(\hat{\beta}|X) = \sqrt{\frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2}}. \tag{3.5}
$$
The variance of the error term $\sigma^2$ is usually not known, but can be estimated using the regression residuals (Wooldridge (2016), pp. 48-50). Now recall assumption **A4** of the CLRM:
$$
Var(\varepsilon_i|X) = \sigma^2I_{n} \tag{3.6}
$$
where $I_n$ is an identity matrix with $n$ rows. The assumption states that the variance of the error term is constant given any values of the independent variables $X$. We call this property *homoscedasticity*. The assumption further implies that there is no *autocorrelation* in the error term: a zero covariance between each error $\varepsilon_i$ and $\varepsilon_j$, for all $i \not{=} j$. Unfortunately, **A4** does often not hold in practice. This is particularly the case, when we consider data that consists of different groups, or more precisely, *clusters* (Cameron and Miller (2015), pp. 317-327). Our panel dataset, where we study several countries and country-industries, is exemplary for such a scenario. One reason why the homoscedasticity assumption fails here is that the data are aggregated (Wooldridge (2016), pp. 257-259). Smaller countries and countries-industries should have more variation in productivity outcomes, since a single (large) firm contributes much more to the aggregated TFP growth. Hence, the idiosyncratic productivity realizations of firms do not cancel each other out as much in small country-industries. A reason why the errors might be correlated is that industries are subject to common regulations - like competition policy - set on the country-level (Cameron and Miller (2015), p. 318). Formal tests that detect whether **A4** is violated are readily available (Kennedy (2008), Chapter 8). 

When facing these issues, SEs derived from the assumptions of the CLRM are no longer valid. The consequences are usually too small SEs, p-values, and too narrow confidence intervals. *Cluster-Robust Standard Errors (CSEs)* relax the strict assumption on the error term and allow the variance of the errors to be correlated within clusters. CSEs can be consistently estimated provided a cluster-structure $C$ is given:
$$
Var_{clu}(\hat{\beta}|X) = (X' X)^{-1} \hat{B}_{clu} (X' X)^{-1} \tag{3.7}
$$
with
$$
\hat{B}_{clu} = \sum_{c=1}^{C} X_c' \hat{\varepsilon}_c \hat{\varepsilon}_c' X_c. \tag{3.8}
$$
The residual vector $\hat{\varepsilon}_c$ for cluster $c$ is calculated by $\hat{\varepsilon}_c = y_c - X_c \hat{\beta}$. CSEs can easily be estimated with the `felm()` function. In the following, we cluster the SEs on the country-level and compare them to the classical SEs.

#< info "The felm() function and CSEs"
To make use of CSEs, we have to expand the function call to `felm()`. The first argument has to be split into four parts, each separated by `|`:
```{r eval=FALSE}
ols = felm(y ~ x | 0 | 0 | cluster_var, data = dat)
```
As before, the first part describes the dependent and explanatory variables. We do not need the second and third part yet. Therefore, we set both to `0`. The fourth part allows specifying a cluster variable. The command above regresses `y` on `x` from `dat` using the variable `cluster_var` to compute the CSEs. To learn more about the `felm()` function, check out the documentation of the `lfe` package. 
https://cran.r-project.org/web/packages/lfe/index.html
#>

**Task:** Regress `tfp` on `cpi` and specify the variable `country` as cluster variable. Store the results in `simple_clust`. I already gave you part of the solution; just complete/replace the function call and click `check`.
```{r}
#< fill_in
# Insert the correct expressions for the placeholders ___ below. 
___ = felm(tfp ~ cpi | 0 | 0 | ___, data = dat)
#>
simple_clust = felm(tfp ~ cpi | 0 | 0 | country, data = dat)
#< hint
cat("If you struggle with the syntax of felm(), refer to the info box above.")
#>
```

To inspect the results, we do not rely on the `summary()` function again. From now on, we make use of the `stargazer()` function which produces well-formatted regression tables. This also allows us to combine the results of several regressions.

#< info "The stargazer() function"
The `stargazer()` function is part of the `stargazer` package. The function can be used to produce publishing quality regression tables. Several options to customize the table are available. Throughout the problem set, I provide the full code for all `stargazer()` tables. If you want to learn more about the function and its arguments, check out the package documentation. 
https://cran.r-project.org/web/packages/stargazer/index.html
#>

**Task:** Use the `stargazer()` function to compare the regression results `simple` and `simple_clust` in a common table. The code is already given below; just click `check` to view the table.
```{r results = 'asis', warning = FALSE}
#< task_notest
# Organize regression results in a table
library(stargazer)
stargazer(simple, simple_clust, 
          type = "html",
          title = "Table 3.0: Classical SEs versus CSEs",
          style = "aer", 
          digits = 3,
          column.labels = c("Non-Robust", "Cluster-Robust"))
#>
```


#< quiz "cse"
question: Consider the table above. By using CSE the estimate for the CPI's standard error...

sc:
    - ... decreased.
    - ... increased.*
    - ... stayed about the same. 
    
success: Great!
failure: Try again.
#>


Comparing the two results, we see that the CSE for `cpi` is about two thirds larger than the classical SE. Ignoring whether **A4** is violated can lead to a *Type 1 Error* in regression analysis, where the null-hypothesis ($H_0:\beta_1 = 0$) is true, but we reject it in favor of the alternative ($H_1:\beta_1 \not= 0$). We could falsely conclude that there is an effect although there is none (Wooldridge (2016), pp. 693-695). In our case, the coefficient of the CPI is still significant at the $1\%$-level when using CSE. In the upcoming exercise, we return to the question of causal interpretations of regression estimates.

## Exercise 4 -- Omitted Variable Bias and Endogeneity

### a) Omitted Variable Bias - A short Simulation Study

In exercise 3, we briefly discussed that coefficients of simple regression models can usually not be interpreted in a causal way, unless the data was generated by a randomized experiment. If we have to resort to observational data in order to establish causal inference, we face additional issues. In this exercise, we examine a major reason why this is the case: *omitted variable bias (OVB)* (Wooldridge (2016), pp. 78-81). In order to demonstrate the problem, we consider a hypothetical population model and run a brief simulation. After we have laid out the conceptual framework, we expand our analysis to the actual data. The value of simulation studies is that we know the exact properties of the data-generating processes and can therefore evaluate the properties of different models and estimators. For an introduction to simulation studies in econometrics refer to Kennedy (2008, Chapter 2.10).

For our simulation, we assume certain causal relationships among the variables of interest. Throughout this problem set, we occasionally use *causal graphs* to complement the analysis. Causal graphs visualize relationships among variables in *directed acyclic graphs* (Morgan and Winship (2015), Chapter 3). One advantage of such graphs is that they force us to state assumptions on causal links in systems of variables explicitly. This allows determining strategies on how to identify causal effects of interest. While these graphs can be formulated arbitrarily detailed, we use basic variants that capture the core issues we face. 

#< preknit
```{r echo=FALSE, warning=FALSE, results = 'asis', out.width=600, dpi=700}
suppressPackageStartupMessages(library(knitr))
include_graphics("C:/Users/ThinkPad User/Documents/CompetitionPolicy/Problem Set/mutual.jpg")
```
#>

Consider the causal graph above. We are interested in estimating the causal effect of competition policy on productivity growth, represented by the directed edge linking the nodes $CPI$ and $TFP$. In this exercise, we ask how estimates of this effect are influenced in the presence of other relevant variables. The graph shows such a variable, **product market regulation (PMR)**, which we also assume to have a causal effect on $TFP$. $PMR$ covers formal regulations of product markets, such as legal and administrative barriers of entrepreneurship and investment. We would expect such barriers to have a negative impact on productivity growth. For instance, strict regulations may prevent market entries of efficient firms or harm innovation. We could measure $PMR$ using an indicator, similar to $CPI$, where high values correspond to strict regulatory regimes.

The graph further shows that we assume $CPI$ and $PMR$ to be mutually dependent on one or more common causes. We indicate that these causes are generally unknown and unobserved by an empty node labeled $U$. Since both variables are of a regulatory nature, it seems plausible that they are mutually determined to some degree. For instance, governments following a "pro-market attitude" could be more dedicated to pass policies that enhance competition. Consistent with this attitude, these governments could also tend to impose fewer regulations on product markets. Thus, for our simulation, we assume $U$ contains the **market orientation** of policy makers. In the following, we formulate a population model and generate some data following the presuppositions stated in the causal graph:
$$
\Delta TFP = \beta_0 + \beta_1 CPI + \beta_2 PMR + \varepsilon. \tag{4.0}
$$
Productivity growth is therefore determined by the level of competition policy, product market regulation, and other random factors collected in the error term $\varepsilon$. By picking values for the parameters $\beta$, we can assess to what degree estimates of these parameters $\hat{\beta}$ match or mismatch their population counterparts. For $\beta$ we pick "true" values:
$$
\beta_0 = 0.01;\ \ \beta_1 = 0.05;\ \ \beta_2 = -0.1. \tag{4.1}
$$
The signs $\beta_1$ and $\beta_2$ follow the assumptions stated above: Competition policy has a positive and product market regulation a negative impact on TFP growth. The first step in our simulation is to assign these values to the population parameters $\beta$.

**Task:** Assign the values in $(4.1)$ to the variables `beta0`, `beta1` and `beta2`. The code is already given below; just click `check`.
```{r}
#< task_notest
# Assign parameter values
beta0 = 0.01
beta1 = 0.05
beta2 = -0.1
#>
```

To generate some data based on the population model in $(4.0)$, we draw random numbers from a normal distribution. First, we generate random numbers for $U$ (market orientation of policy makers). For $CPI$ and $PMR$ we also draw random numbers and incorporate a dependency on $U$. To keep it simple, we add $U$ to the random number drawn for $CPI$, and subtract $U$ from the random number drawn for $PMR$. Thus, the market attitude of policy makers influences $CPI$ positively, and $PMR$ negatively. Generating normally distributed random numbers can be done using R's `rnorm()` function.

#< info "The rnorm() function"
The `rnorm()` function samples random numbers following a normal distribution. The function has three arguments. The first argument `n` specifies how many values to generate. The arguments `mean` and `sd` define the mean and the standard deviation of the normal distribution. The output of the function is a vector of length `n`. If you want to learn more, check out the documentation.
https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Normal.html
#>

**Task:** Draw `1000` random numbers for the variable `u` using the `rnorm()` function. Compute the values for `cpi` as the sum of `u` and a random part. Compute `pmr` as a sum of `-u` and a random part. I already picked means and standard deviations of the distributions; complete/replace the code below and click `check`.
```{r}
#< fill_in
# Insert the correct expressions for the placeholders ___ below. 
___ = rnorm(___, mean = 0, sd = 0.11)
cpi = ___ + rnorm(1000, mean = 0.5, sd = 0.15)
pmr = ___ + rnorm(1000, mean = 0.75, sd = 0.15)
#>
u = rnorm(1000, mean = 0, sd = 0.11)
cpi = u + rnorm(1000, mean = 0.5, sd = 0.15)
pmr = - u + rnorm(1000, mean = 0.75, sd = 0.15)
```

I have picked the parameters of the population model and the distributions to ensure that we obtain significant and easily interpretable results. In the next task, we want to confirm that the variables `cpi` and `pmr` are, as intended, negatively associated. To do so, we calculate *Pearson's correlation coefficient* using R's `cor()` function.

#< info "Pearson's Correlation Coefficient and the cor() function"
Pearson's correlation coefficient measures the strength of a linear relationship between two variables (Field et al. (2012), pp. 208-210). It takes on values between $-1$ and $1$, where values below zero indicate negative associations, and values above zero indicate positive associations. The correlation coefficient $\rho_{x,y}$ between two variables $x$ and $y$ is calculated as follows:
$$
\rho_{x,y} = \frac{Cov(x,y)}{\sigma_x \cdot \sigma_y} \tag{4.2}
$$
where $Cov(x,y)$ is the covariance, $\sigma_x$ and $\sigma_y$ are the standard deviations of the variables $x$ and $y$. To compute $\rho_{x,y}$ in R, we can use the `cor()` function. If the variables of interest are two vectors $x$ and $y$, we simply pass them as arguments to the function.
```{r eval = FALSE}
cor(x,y)
```
For more information check out the function documentation.
https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/cor
#>

**Task:** Confirm that `cpi` and `pmr` are negatively correlated using the `cor()` function.
```{r}
#< task
# Enter code below
#>
cor(cpi, pmr)
#< hint
cat("Just read the info box above again.")
#>
```

The correlation coefficient shows a moderate negative association between the two variables, as expected. In the final step of the simulation, we compute the TFP growth rates based on the population model and the random variables. To do so, we simply plug in the generated data and draw an additional vector of random numbers for the error term $\varepsilon$.

**Task:** Draw `1000` random numbers for `eps`. Compute `tfp` given `cpi`, `pmr`, `eps` and the parameters `beta0`, `beta1` and `beta2`. I already gave you part of the solution; just complete/replace the formula below and click `check`. 
```{r}
#< fill_in
# Insert the correct expressions for the placeholders ___ below. 
___ = ___(1000, mean = 0, sd = 0.02)
tfp = beta0 + ___ + ___ + ___
#>
eps = rnorm(1000, mean = 0, sd = 0.02)
tfp = beta0 + beta1*cpi + beta2*pmr + eps
#< hint
cat("Consider equation 4.0 for the population model again.")
#>
```

#< award "Simulation Novice"
Congratulations! You have successfully simulated data that follow a population model.
#>

Now that we have some data to work with, we ask the following question: How is the estimate for the causal effect of competition policy $\tilde{\beta_1}$ affected, when we estimate a simple regression model, where we omit $PMR$:
$$
\Delta TFP = \tilde{\beta_0} + \tilde{\beta_1} CPI + v. \tag{4.3}
$$
We also estimate a model that includes both variables $CPI$ and $PMR$:
$$
\Delta TFP = \hat{\beta_0} + \hat{\beta_1} CPI + \hat{\beta_2} PMR + \varepsilon. \tag{4.4}
$$
For convenience, we refer to the models in $(4.3)$ and $(4.4)$ as the "short model" and the "long model", respectively. Again, we make use of the `felm()` function to obtain the OLS estimates.

#< info "Using more than one independent variable with felm()"
Including multiple independent variables is straightforward with `felm()`. We add the names of all variables in the first argument to `felm()` and separate each of them with `+`. The following call regresses `y` on both `x1` and `x2` in `dat`.
```{r eval=FALSE}
library(lfe)
ols = felm(y ~ x1 + x2 | 0 | 0 | cluster_var, data = dat)
```
To learn more, check out the documentation of the `lfe` package. https://cran.r-project.org/web/packages/lfe/index.html
#>

**Task:** Regress `tfp` on `cpi` and `pmr`. Assign the results to `long`. Regress `tfp` on `cpi` and assign the results to `short`. Note that you do not have to specify the `data` argument in the function call since the variables are vectors stored in the global environment. We also do not need a cluster variable here. Show both results for comparison in a `stargazer()` table. I already gave you part of the solution; just complete/replace the function call and click `check`. 
```{r results = 'asis', warning = FALSE}
#< fill_in
library(lfe)
# Insert the correct expressions for the placeholders ___ below. 
___ = felm(tfp ~ cpi + ___)
___ = felm(tfp ~ cpi)

# Organize results in a stargazer table
library(stargazer)
stargazer(long, short,
          type = "html",
          title = "Table 4.0: Short and Long Regression - Simulated Data",
          style = "aer", 
          digits = 3,
          column.labels = c("Long Model" ,"Short Model"))
#>
library(lfe)
long = felm(tfp ~ cpi + pmr)
short = felm(tfp ~ cpi)

library(stargazer)
stargazer(long, short,
          type = "html",
          title = "Table 4.0: Short and Long Regression - Simulated Data",
          style = "aer", 
          digits = 3,
          column.labels = c("Long Model" ,"Short Model"))
#< hint
cat("If you struggle with the syntax of felm(), refer to the info box above.")
#>
```


Starting with the long model in the left column, we can confirm that the OLS estimates $\hat{\beta}$ are close to the true population parameters $\beta$. Because we added some noise $\varepsilon$, minor deviations occur due to sampling error. Ultimately, the results are convincing: If we did not know the true coefficients, we would have gotten precise OLS estimates. This is not the case for the short model though. The coefficient for the CPI $\tilde{\beta_1}$ is considerably far away from the true parameter value $\beta_1 = 0.05$. To see the problem, consider the error term $v$ in the short model:
$$
v = \beta_2 PMR+ \varepsilon. \tag{4.5}
$$
By omitting $PMR$ from the regression model, we have put the variable into the error term. Now recall assumption **A3**, the *conditional independence assumption* of the CLRM:
$$
\mathbb{E}(v_i|X) = 0. \tag{4.6}
$$
The assumption states that the expected value of the error term is zero, given any values of the independent variables $X$. This assumption is violated in the short model. Recall that we presupposed $CPI$ and $PMR$ to be mutually caused by the market orientation of policy makers, which resulted in a negative correlation between the two variables. This association allows us to make statements regarding the expected value of error term. For instance, when we observe an above average value for $CPI$, we know that value for $PMR$ tends to be below average. That is to say, $X$ carries useful information for the prediction of the error term, which stands in contrast to the conditional independence assumption **A4** in $(4.6)$ (Greene (2003), p. 10). Correlation of an independent variable with the error term implies that the OLS estimator is biased:
$$
\mathbb{E(\tilde{\beta_1})} \neq \beta_1. \tag{4.7}
$$
That is, the expected value of the OLS estimator $\tilde{\beta_1}$ does not match the population parameter $\beta_1$. Because this bias is introduced by omitting a relevant variable, it is called *omitted variable bias (OVB)* (Greene (2002), pp. 148-149). It can be shown that the expected value of $\tilde{\beta_1}$ is given by:
$$
\mathbb{E}(\tilde\beta_1) = \beta_1\ +\ \beta_2 \cdot \gamma_{1} \tag{4.8} 
$$
where $\beta_1$, $\beta_2$ are the population coefficients and $\gamma_1$ is the slope parameter of a simple regression of $PMR$ on $CPI$. The bias in our case can therefore be written as:
$$
Bias(\tilde\beta_1) = \mathbb{E}(\tilde\beta_1) - \beta_1 \\
= \beta_2 \cdot \frac{Cov(CPI, PMR)}{Var(CPI)}. \tag{4.9} 
$$
Looking at the formula, we see that there are two possible cases where $\tilde{\beta_1}$ is unbiased. Firstly, the population coefficient $\beta_2$ is zero. In other words, $PMR$ has no effect on $TFP$. Secondly, $CPI$ and $PMR$ are uncorrelated, that is, they have a zero covariance. If either one of these conditions hold (or both), OLS is unbiased and we obtain valid estimates for the causal effect of $CPI$. However, we generated the data in such a way that $\beta_2 = - 0.1$ and $Cov(CPI, PMR) < 0$. Hence, the short regression does not allow unbiased estimation. Consider formula $(4.9)$ and think about the direction of the bias.


#< quiz "bias"
question: In what direction is the coefficient biased?

sc:
    - Upwards*
    - Downwards
    
success: Great!
failure: Try again.
#>


We know that the population coefficient $\beta_2$ is negative and that the covariance is negative. Hence, $\tilde\beta_1$ is distorted upwards. We would overestimate the effect competition policy has on productivity growth. We can also quantify the size of the bias in our example using the OVB formula.

**Task:** Compute the bias using formula $(4.9)$. The coefficient for `pmr` is `beta2`. Use the `var()` and `cov()` functions to obtain the sample variance and covariance, respectively. I already gave you part of the solution; just complete/replace the code below and click `check`.
```{r}
#< fill_in
# Insert the correct expressions for the placeholders ___ below.
beta2 * (cov(___, ___) / var(___))
#>
beta2 * (cov(cpi, pmr) / var(cpi))
```

The size of the bias is not negligible compared to the true population parameter ($\beta_1 = 0.05$). Qualitatively we are still correct here since the true effect is also positive. However, suppose the true effect was $\beta_1 = 0$. The distortion could make us conclude that there is an effect, whereas there is none. Now that we have laid out the conceptual problem of omitted variables in regression analysis, we assess the problem using the actual data.

### b) Assessing OVB in the Data

For their analysis, the authors collected data on an index measuring PMR, which is part of the *OECD Product Market Regulation* database. This index covers regulations of business enterprises, barriers to entrepreneurship and barriers to international trade and investment on the country-level. It takes on values between $0$ and $6$, where higher values indicate stricter regulatory regimes. In the next task, we confirm that the CPI and the PMR index are, as hypothesized, negatively correlated.

**Task:** Load the dataset `dat.rds` again and assign it to the variable `dat`. Compute the correlation coefficient of `cpi` and `pmr`. The code is already given below; just click `check`.
```{r}
#< task_notest
dat = readRDS("dat.rds")
cor(dat$cpi, dat$pmr)
#>
```

For the `cor()` call, we extract the variable vectors `cpi` and `pmr` from `dat` using the `$` operator. As expected, the two indicators are negatively associated with a correlation coefficient of about $-0.3$. In the next task, we obtain the OLS estimates for the data. 

**Task:** Regress `tfp` on `cpi` and `pmr`, and assign the results to `long`. Regress `tfp` on `cpi` and assign the results to `short`. Show both results in a `stargazer` table for comparison. The code is already given below; just click `check`.
```{r results = 'asis', warning = FALSE}
#< task_notest
# Long and short regression
long = felm(tfp ~ cpi + pmr | 0 | 0 | country, data = dat)
simple = felm(tfp ~ cpi | 0 | 0 | country, data = dat)

# Organize results in a stargazer table
stargazer(long, simple,
          type = "html",
          title = "Table 4.1: Short and Long Regression",
          style = "aer", 
          digits = 3,
          column.labels = c("CPI and PMR", "CPI"))
#>
```


When we include $PMR$ the coefficient for $CPI$ is $0.051$, which is, as expected, lower than in the simple model. We would overestimate the effect competition policy has when omitting the degree of product market regulation. As before, we estimate the bias introduced by omitting $PMR$ in the next task. To obtain the coefficient for the OVB formula, we can use the `coef()` function. 

#< info "The coef() function"
The `coef()` function allows to extract coefficients from R model objects. It takes the name of a model as argument and returns the vector of estimates. We can select specific coefficients by using R's standard approaches to handle vectors. This call returns the first coefficient of an object called `model`:
```{r eval=FALSE}
coef(model)[1]
```
To learn more about the function, refer to the documentation.
https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/coef
#>

**Task:** Estimate the bias introduced by omitting the variable `pmr`. Extract the coefficient of `pmr` from `long` by selecting the third element of the vector returned by `coef()`. The code is already given below; just click `check`.
```{r}
#< task_notest
coef(long)[3] * cov(dat$cpi, dat$pmr) / var(dat$cpi)
#>
```


#< quiz "sizebias"
question: Why is the estimated bias here smaller than in our simulation?

sc:
    - The coefficient of PMR is substantially smaller.*
    - The correlation between CPI and PMR is substantially smaller.
    
success: Great!
failure: Try again.
#>


By omitting $PMR$, we overestimate the effect of $CPI$ by approximately $0.016$, or about $30$ percent. The size of the estimated bias is smaller here compared to our simulation, because the coefficient of $PMR$ in the long model is smaller ($Corr(CPI, PMR)$ is roughly the same).

From here on it should be clear that the problem of OVB extends to all other **relevant** variables that are omitted from the regression model. "Relevant" in this context means that a variable has a non-zero population coefficient in the population model and is correlated with one or more independent variables included in the regression model. However, deriving the bias in more general cases is difficult. It depends on the population coefficients (which we do not know) and the correlation patterns among all relevant variables (Wooldridge (2016), p. 81). Finally, it is important to stress that bias relates to an estimation procedure (the estimator), not to a concrete estimate obtained using a sample (Kennedy (2008), p. 9). We may obtain an estimate that is close to the true parameter value of interest, despite using a biased estimator. However, the bias tells us that we obtain, on average, too small or too large estimates.

### c) Endogeneity

In the previous section we demonstrated how omitting relevant variables induces bias in the OLS estimator. OVB is one example of how the issue of *endogeneity* arises in regression analysis. When an explanatory variable is correlated with the error term, we say that the corresponding variable is *endogenous* (Kennedy (2008), p. 139). This violates the crucial *conditional independence assumption* **A3** of the CLRM. We can express the problem using a causal graph:

#< preknit
```{r echo=FALSE, warning=FALSE, results = 'asis', out.width=600, dpi=700}
suppressPackageStartupMessages(library(knitr))
include_graphics("C:/Users/ThinkPad User/Documents/CompetitionPolicy/Problem Set/endo.jpg")
```
#> 

We are interested in estimating the causal effect of competition policy on $TFP$. However, $CPI$ might be correlated with the error term $\varepsilon$ (for instance, because we omitted a relevant variable such as $PMR$). The dotted two-sided arrow indicates this association with the error term. We use this this kind of arrow in DAGs as a shorthand - when we do not want to specify the exact relationship that connects two variables (Morgan and Winship (2015), pp. 79-81). Consequence of the endogeneity is that estimation of the causal effect of interest will be biased and inconsistent. Apart from the problem of omitted variables, there are other reasons why endogeneity may arise.

One example in our case could be potential *reverse causality* between the CPI and TFP growth (Kennedy (2008), pp. 139-140). Reverse causality could arise if policy makers react to weak productivity outcomes by imposing stricter competition rules to encourage innovation. Hence, not only the policies might have an effect on productivity, productivity might as well affect the policies. This could lead to a negative correlation between the CPI and the error term (countries with lower productivity outcomes might be more engaged in competition policy). The authors argue that reverse causality should not be a major issue in our case. They stress that competition policy regimes evolve over a longer timeframe and are therefore unlikely to react to short-term productivity outcomes. We could further support their claim: Competition policy is for the most part set on the national level, making it somewhat unlikely to respond to the outcomes of individual industries (assuming industry productivities are somewhat heterogeneous on the country-level). Still, to mitigate potential bias induced by reverse causality, the authors propose using the lagged CPI (and other variables such as PMR) in their regression models. We already saw this in our simple model, where we regressed $\Delta TFP_{i,j,t}$ on $CPI_{i, t-1}$. This approach rests on the assumption that the CPI at period $t-1$ is uncorrelated with the error term in period $t$. By using a measure that is predetermined, we block possible reverse causal links from productivity to competition (Wooldridge (2016), pp. 511-513). In exercise 12, we elaborate on this approach to mitigate the endogeneity issue in more detail.

According to the authors, of primary concern regarding the possible endogeneity of the CPI is the existence of omitted variables. We demonstrated the issue using a variable that measures the degree of product market regulation above. Our results indicated that countries with better competition policy also tend to have less regulated product markets. Less regulated product markets seem to be better for productivity growth. Thus, if we ignore PMR, competition policy "takes credit" for the lower adverse effects imposed on productivity growth by less strict product market regimes. To obtain valid estimates for the causal effect of the policy, we therefore have to recognize the presence of other factors determining productivity growth. The standard approach to account for such omitted factors is the *control variable approach* (Morgan and Winship (2015), pp. 194-200). Consider the causal graph below.

#< preknit
```{r echo=FALSE, warning=FALSE, results = 'asis', out.width=600, dpi=700}
suppressPackageStartupMessages(library(knitr))
include_graphics("C:/Users/ThinkPad User/Documents/CompetitionPolicy/Problem Set/control.jpg")
```
#>

Using prior knowledge, we identify other variables $X$ that should have an effect on $TFP$ (they are part of the "true" population model determining productivity outcomes) and include them in our model. As discussed above, of particular importance are variables that are associated with $CPI$, the causal variable of interest. Variables that are uncorrelated with $CPI$ and other included covariates could be omitted, but we usually do not know this beforehand (unless the data comes from a randomized experiment). We might still include them for efficiency reasons (Kennedy (2008), Chapter 2.6). After conditioning on a set of control variables $X$, the correlation with the error term $\varepsilon$ is at best removed. If this strategy is successful, estimates can be interpreted in a causal way (Angrist and Pischke (2009), Chapter 3.2). Before we introduce additional control variables (also called *controls* or *covariates*), we discuss a related method to deal with the endogeneity issue: *fixed effects*.

### d) Addressing OVB with Fixed Effects

The *fixed effects transformation* is a panel data method that allows to remove unobserved time-invariant effects that could otherwise induce bias in the OLS estimator (Wooldridge (2016), pp. 434-438). To see how it works, consider this adjusted version of our simple regression model from exercise 3:
$$
\Delta TFP_{i,j,t} = \beta_1 CPI_{i, t-1} + \psi_{i,j} + \varepsilon_{i,j,t} \tag{4.10}
$$
where $\psi_{i,j}$ is a *country-industry fixed effect* (notice how there is no time index $t$). This term captures unobserved, time-invariant effects that result in productivity differences across country-industries. Ignoring $\psi_{i,j}$ puts these unobserved factors into the error term. If $CPI_{i, t-1}$ is correlated with $\psi_{i,j}$, we face to consequences of OVB, as discussed above. To give a concrete example, labor market policies might exist that account for a difference in productivity outcomes between the food products industry in the UK and, say, the food products industry in the US. If policy makers tend to be engaged in both, such labor market policies and competition policies, estimates of $\beta$ will be biased. Fortunately, there is a simple way to account for these fixed effects. If we average $(4.10)$ over time for each country-industry $i,j$, we obtain: 
$$
\Delta \overline{TFP}_{i,j} =  \beta_1 \overline{CPI}_{i} + \psi_{i,j} + \overline\varepsilon_{i,j} \tag{4.11}
$$
where $\overline{\Delta TFP}_{i,j,t}$ is given by $\frac{1}{T}\sum_{i,j}^{T}{\Delta TFP}_{i,j,t}$, and so on. Then, we subtract $(4.11)$ from $(4.10)$, to obtain:
$$
{\Delta TFP}_{i,j,t}^* =  \beta_1 {CPI}_{i, t-1}^* + \varepsilon_{i,j,t}^* \tag{4.12}
$$
where $*$ indicates the *time-demeaned* variables. As we can see, the country-industry fixed effect $\psi_{i,j}$ has been removed from the equation. Thus, the fixed effects transformation suggests using deviations from the variable's group-means (here country-industries $i,j$) to estimate the parameters of interest. The impact on the regression coefficients is equivalent to performing a *dummy variable regression*, where we estimate the original model including indicator variables for each unit (Wooldridge (2016), pp. 438-439). This allows each unit to have its own intercept (see info box below). The major advantage of the fixed effects transformation is that less parameters have to be estimated (In the second part of this problem set, we consider firm-level fixed effects using a dataset of hundred thousands of firms). Similarly, we can also add *time fixed effects* $\phi_{t}$, to account for macroeconomic shocks affecting all country-industries at the same time (Wooldridge (2016), pp. 403-406). The `felm()` function applies the fixed effects transformation with ease.

#< info "Dummy Variables in Regression Analysis"
Indicator variables that only take on two values are also referred to as *dummy variables* or simply *dummies* (Kennedy (2008), Chapter 15). These variables usually relate to categorical or ordinal attributes that are coded using the values $0$ and $1$, where a $1$ implies that a certain feature is present. For example, we could add a dummy variable `foodUK` to the data, which is $1$ for each observation relating to UK's food products industry and $0$ for all other observations.

Interpreting dummy variables in a regression context is straightforward. With one explanatory variable, the coefficient of a dummy is the **difference in the average outcome** of observations where the feature is present and observations where the feature is not present. To illustrate, consider regressing `tfp` on `foodUK`. The intercept of that regression is the average TFP growth of all country-industries, except the food products industry in the UK. The coefficient of `foodUK` marks the difference in TFP growth of UK's food product industry and all other country-industries.   
#>

#< info "The felm() function and fixed effects"
The fixed effects transformation is performed by using the second part of the first argument passed to `felm()`. We simply add the names of the variables identifying the relevant groups. Multiple fixed effects are separated by `+`. The following command regresses `y` on `x1` and `x2` from the data set `dat`. The variables `f1` and `f2` are used to remove the fixed effects.
```{r eval=FALSE}
reg = felm(y ~ x1 + x2 | f1 + f2 | 0 | cluster_var, data = dat)
```
To learn more about the `felm()` function check out the documentation of the `lfe` package.
https://cran.r-project.org/web/packages/lfe/index.html
#>

**Task:** Regress `tfp` on `cpi` and `pmr` and use the country-industry identifier `id` and the variable `year` to remove the fixed effects. Store the results in `fe`. Compare `fe`, `long` and `simple` in a stargazer table. I already gave you part of the solution; just complete/replace the function call below and click `check`.
```{r results = 'asis', warning = FALSE}
#< fill_in
# Insert the correct expressions for the placeholders ___ below.
fe = felm(tfp ~ cpi + pmr | ___ + ___ | 0 | country, data = dat) 

# Organize results in a table
stargazer(fe, long, simple, 
          type = "html", 
          title = "Table 4.2: Fixed Effects",
          style = "aer", 
          digits = 3,
          column.labels = c("CPI, PMR + FE" ,"CPI + PMR", "CPI"),
          add.lines = list(c("Country-Industry Fixed Effects",
                             "Yes", rep("No", 2)),
                           c("Time Fixed Effects", "Yes", rep("No", 2))))
#>
fe = felm(tfp ~ cpi + pmr | id + year | 0 | country, data = dat) 

stargazer(fe, long, simple, 
          type = "html", 
          title = "Table 4.2: Fixed Effects",
          style = "aer", 
          digits = 3,
          column.labels = c("CPI, PMR + FE" ,"CPI + PMR", "CPI"),
          add.lines = list(c("Country-Industry Fixed Effects",
                             "Yes", rep("No", 2)),
                           c("Time Fixed Effects", "Yes", rep("No", 2))))
#< hint
cat("If you struggle with the syntax of the felm() function, refer to the info boxes above.")
#>
```


Locking at the results, we see that the coefficient of the CPI is $0.089$ and still highly significant. Its size has even increased in magnitude compared to the models we estimated earlier. In the next exercise, we specify the full econometric model, discuss the control variables and interpret the OLS estimates.

## Exercise 5 -- Econometric Model and Results

### a) Specification of the Model

The specification we want to estimate builds on an *endogenous growth model* developed by Griffith et al. (2004). Their approach presumes that productivity growth by means of research and development has "two faces". Firstly, firms can innovate by developing new technologies to increase their productivity. Secondly, firms can adopt productivity-enhancing technologies from more advanced firms. This approach recognizes that innovations are not necessarily restricted to the innovator, but can be absorbed by other firms. In our context, we presume that this technology transfer occurs from the *technological frontier* to less advanced industries. We define the technological frontier as the country-industry $L,j,t$ with the highest TFP level relative to a common reference point.

The rate of technology transfer to country-industry $i,j,t$ is determined by the TFP level $\Delta TFP_{L,j,t}$ at the frontier, and the technology gap $\frac{TFP_{L,j,t}}{TFP_{i,j,t}}$ to the frontier. Intuitively, the more advanced the leading industry the more potential for the adoption of productivity-enhancing technologies by laggard industries. Likewise, the larger the technology gap, the more possibilities for the adoption of technologies (refer to the online appendix of Buccirossi et al. (2013) for a more detailed explanation of the model). Incorporating $\Delta TFP_{L,j,t}$ and $\frac{TFP_{L,j,t}}{TFP_{i,j,t}}$ leads us to the following specification we want to estimate:
$$
\Delta TFP_{i,j,t} = \beta CPI_{i, t-1} + \delta\Delta TFP_{L,j,t} + \sigma \frac{TFP_{L,j,t}}{ TFP_{i,j,t}} \\\ + \gamma X_{i,j,t-1} + \chi Z_{i, t-1} + \psi_{i,j} + \phi_{t} + \varepsilon_{i,j,t} \tag{5.0}
$$
where we also add a vector of industry-specific control variables $X_{i,j,t-1}$ and a vector of country-specific control variables $Z_{i, t-1}$. As discussed in exercise 4, $\psi_{i,j}$ are country-industry fixed effects and $\phi_{t}$ are time fixed effects. Before we estimate the model, we take a closer look at the controls. To account for the fact that most variables in economics cannot simply be measured, but must be constructed or estimated, we compute an exemplary control variable in the tasks ahead.

### b) The Control Variables

The authors propose **trade openness** as a control variable. Country-industries that are subject to more open markets should face more intense competition - domestic firms are effectively competing with other firms all around the world. A higher level of competition should weed out inefficient firms and incentivize incumbent firms to perform cost reduction investments. Therefore, we would expect more open country-industries exhibit higher productivity growth. It is also plausible that the CPI and trade openness are correlated. "Pro-market" politicians could tend to be engaged in both competition policy and promoting open trade.

To construct an appropriate measure for trade openness, we perform a simple calculation. The replication data contain annual imports aggregated on the country-industry level. To account for different sizes of country-industries, we divide the total value of imports by total country-industry value added (value added is a measure for the total production output minus certain inputs, approximately):
$$
Trade_{i,j,t-1} = \frac{Imports_{i,j,t-1}}{ValueAdded_{i,j,t-1}} \tag{5.1}
$$
where $Trade_{i,j,t-1}$ is a measure for the openness to trade in industry $j$ in country $i$ at time $t-1$. We could also call this variable "import penetration" - the ratio of industry imports over industry value added. Firstly, we have to load a new file `raw.rds` containing both variables on the right-hand side of $(5.1)$.

**Task:** Load `raw.rds` using the `readRDS()` function and assign it to `raw`. Then, show its upper entries using `head()`. The code is already given below; just click `check`.
```{r}
#< task_notest
raw = readRDS("raw.rds")
head(raw, n = 5)
#>
```

Both columns `id` and `year` are already known from previous exercises. The variable `imports` quantifies the country-industry specific product imports; `valuerealppp` measures the total value added adjusted for purchasing price parity. Computing new variables, such as ratios, can easily be done using the `mutate()` function of the `dplyr` package.

#< info "The mutate() function"
Variables can be computed using the `mutate()` function of the `dplyr` package. The first argument specifies a data frame. The second argument is used to define one or more formulas that can contain columns of the passed data frame. For instance, this code snippet creates a new variable `x_new`, which is the sum of `x1` and two times `x2` stored in `dat`:
```{r eval = FALSE}
mutate(dat, x_new = x1 + 2*x2)
```
We can append `x_new` to `dat` by assigning the function call to `dat`:
```{r eval = FALSE}
dat = mutate(dat, x_new = x1 + 2*x2)
```
If you want to learn more about the function, check out Wickham and Grolemund (2016, p. 54) or read the documentation.
https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/mutate
#>

**Task:** Compute `trade` using `mutate()` and the formula in $(5.1)$. The operator for division in R is `/`. Append the new variable to `raw`. I already gave you part of the solution; just complete/replace the function call and click `check`. 
```{r}
#< fill_in
library(dplyr)
# Insert the correct expressions for the placeholders ___ below.
raw = mutate(raw, ___)
#>
library(dplyr)
raw = mutate(raw, trade = imports / valuerealppp)
#< hint
cat("If you struggle with the syntax of mutate(), refer to the info box above.")
#>
```

In the task above, we computed the trade openness for each country-industry at time $t$. However, our model in $(5.0)$ states that the controls are supposed to be lagged by one year to mitigate potential endogeneity issues (see exercise 4). Hence, in the next step, we replace each value for trade openness with the value of the preceding year $t-1$. If the data had a simple time series structure, this could be done with one function call. However, since the data have a panel structure, we have to put in some additional effort. The transformation of the variable has to be done *within* each country-industry. Such transformations within groups can be done using the `group_by()` function.

#< info "The group_by() function"
The `group_by()` function is part of the `dplyr` package and converts a data frame into a grouped data frame. Operations on a grouped data frame are applied within specified grouping variables. The function's first argument is a data frame. The subsequent arguments are the names of the columns that serve as grouping variables.
```{r eval = FALSE}
dat = group_by(dat, group_var1, group_var2)
```
The group structure can be removed using the `ungroup()` function.
```{r eval = FALSE}
dat = ungroup(dat)
```
On its own, the function is not particularly useful. The purpose of `group_by()` is to use it in conjunction with other functions, as we will see below. If you want to learn more, check out Wickham and Grolemund (2016, p. 59) or read the function documentation.
https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/group_by
#>

Before we proceed with our task, we introduce the "pipe" operator, `%>%`, which is especially useful when combined with `group_by()` and some other functions of the `dplyr` package. The "pipe" allows chaining multiple function calls in a concise sequence. The easiest way to understand the operator is by examining an example (see info box below).

#< info "The %>% operator"
A large part of R code focuses on the objects being created or transformed. In most function calls, we explicitly state the object we want to use and create a new object containing the intermediate results. This creates redundancies in R code, which are at times also difficult to read and understand. The `%>%` operator can simplify some of these tasks. Consider this example:
```{r eval = FALSE}
dat %>%
  mutate(avg = mean(x1))
```
This code fragment uses a data frame `dat` and "pipes" it into the `mutate()` function, which computes the mean of `x1` in `dat` under the name `avg`. Note that the function call to `mutate()` does no longer require the first argument (the object to use). We can store the results in the usual way:
```{r eval = FALSE}
dat = dat %>%
  mutate(avg = mean(x1))
```
Of course, this application of `%>%` is not too useful. The benefit of the operator becomes clear when we combine multiple function calls:  
```{r eval = FALSE}
dat = dat %>%
  group_by(group_var) %>%
  mutate(avg = mean(x1)) %>%
  ungroup()
```
The code above groups `dat` based on a specified variable `group_var`. Then, the `mutate()` function computes the mean of `x1` within each group. Lastly, `ungroup()` removes the grouping. Note that none of the functions uses `dat` as first argument anymore. An intuitive way to think about the operator is to read "then" whenever `%>%` appears in the code. The "pipe" operator is not part of base R, but some packages, such as `dplyr` support it. If you want to learn more about the "pipe", check out Wickham and Grolemund (2016, pp. 60-61 and Chapter 14).
#>

Recall that we want to replace the value of trade openness with the value of the preceding year, within each country-industry. Before we are able to perform this task, we have to introduce one last function: `lag()`.

#< info "The lag() function"
The `lag()` function is part of the `dplyr` package and allows determining "previous" values of a vector. The first argument specifies the variable we want to lag. The second argument, `n`, defines how many steps we want to go back in the vector. The default value is `1`. Lastly, the optional `order_by` argument specifies another vector to be used for the ordering of the lagged values. In a time series context this is usually a time variable such as years or days. 
```{r eval = FALSE}
lag(x, n = 1, order_by = time)
```
If you want to learn more about the `lag()` function, check out Wickham and Grolemund (2016, p. 57) or read the function documentation.
https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/lead-lag
#>

**Task:** Replace the value of `trade` with the value of its preceding year within each country-industry. To do so, combine the functions `group_by()`, `mutate()`, `lag()` and `ungroup()` using the `%>%` operator. The grouping variable is `id`. The `order_by` argument for `lag()` is `year`. Assign the output to a new data frame `raw_l`. I already gave you part of the solution; just complete/replace the code below and click `check`.
```{r}
#< fill_in
# Insert the correct expressions for the placeholders ___ below.
raw_l = raw %>% 
  group_by(___) %>%
  mutate(___ = lag(___, n = 1, order_by = ___)) %>%
  ungroup()
#>
raw_l = raw %>% 
  group_by(id) %>%
  mutate(trade = lag(trade, n = 1, order_by = year)) %>%
  ungroup()
#< hint
cat("If you struggle with the syntax of lag() refer to the info box above.")
#>
```

The variable `trade` now has the appropriate format to estimate our model in $(5.0)$. As a final task, we have to merge the computed values in `raw_l` with the rest of the data in `dat`. In doing so, it is important that each country-industry in `dat` receives its corresponding value in `raw_l`. That is to say, we have to match the observations in `dat` with the observations in `raw_l`, based on both variables `id` and `year`. Such tasks can be done using the family of *join* functions. Here, we use the `left_join()` function.  

#< info "The left_join() function"
The `dplyr` package provides several functions to merge data from different sources. For instance, this code fragment merges the tables `a` and `b`:
```{r eval=FALSE}
left_join(a, b, by = c(x = x, y = y))
```
The argument `by` allows specifying one or more columns to act as a key. The key ensures that the records in both tables are matched correctly. If not stated explicitly, the function uses all columns that have equal names in both tables. The function is called `left_join()` because it keeps the full set of observations in the left table `a`, even if there is no match in table `b`. For more information on this family of functions, check out Wickham and Grolemund (2016, Chapter 10) or read the documentation.
https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/join
#>

**Task:** Load the file `dat.RDS` using `readRDS()` and store it in `dat_trade` to indicate that it lacks the `trade` variable. Merge the two data frames `dat_trade` and `raw_l` using the `left_join()` function. Assign the result to `dat`. You do not have to specify the `by` argument here. I already gave you part of the solution; just complete/replace the function call and click `check`.
```{r}
#< fill_in
# Load file
dat_trade = readRDS("dat.RDS")

# Insert the correct expressions for the placeholders ___ below.
dat = ___
#>
dat_trade = readRDS("dat.RDS")
dat = left_join(dat_trade, raw_l)
#< hint
cat("If you struggle with the syntax of left_join(), refer to the info box above.")
#>
```

#< award "dplyr Apprentice"
Congratulations! You know now a set of useful functions to wrangle data including the pipe operator.
#>

We did not need to use the `by` argument here because both `dat` and `raw_l` contained two columns with identical names. The function automatically detected that `id` and `year` are the columns to join by. The preceding tasks showed that data handling takes some effort, especially when working with data from different sources. While all steps could have been done with base R functionality, the `dplyr` functions and the "pipe" helped keeping the code clear and concise. In the next task, we look at the set of control variables we use to estimate our specification.

**Task:** Use `select()` and `head()` to show the names and the upper five values of the control variables. The code is already given below; just click `check`.
```{r}
#< task_notest
dat %>%
  select(tfpleader, tecgap, trend, trade, pmr) %>%
  head(n = 5)
#>
```

As described earlier, `tfpleader` is the TFP level of the country-industry at the technological frontier. The technology gap between a country-industry and the technological frontier is given by `tecgap`. This variable is zero for the technology leader. For the next control, the authors computed an industry trend (in value added) to account for business cycles (for more information on the use of trends in regression analysis with a time dimension, refer to Wooldridge (2016), pp. 329-335). We already know `pmr`, the degree of product market regulation, from exercise 4. The authors collected more control variables, such as industry expenditures on research and development or the quality of institutions in a country. Some of these additional variables have a substantial amount of missing values. Therefore, the authors decided not to include sparse controls to maintain a large sample size. Nevertheless, they assess how robust their findings are with respect to the inclusion of these additional controls. The results indicate that omitting these covariates does not have a substantial impact on the estimation. For the robustness checks, refer to the article Buccirossi et al. (2013) and its online appendix.

### c) Bad Controls and Causality

Before we finally estimate the model, we briefly elaborate on one important topic in causal inference. Recall our discussion of OVB in exercise 4. We established that omitting a variable that is correlated with the outcome and the causative variable of interest induces bias. Does this imply that we have to control for *all* relevant variables to obtain a reliable estimate of a causal effect?

For instance, consider the **level of competition** as a potential control variable candidate. Clearly, we expect competition to be related to productivity growth. An association between competition and the quality of competition policy seems also very plausible. First and foremost, the policy is intended to enhance competition.


#< quiz "bad controls"
question: Should we control for the level of competition in our regression model?

sc:
    - Yes
    - No*
    - Does not matter
    
success: Great!
failure: Try again.
#>


The level of competition would be a poor choice as a control here. This simple causal graph illustrates the issue:

#< preknit
```{r echo=FALSE, warning=FALSE, results = 'asis', out.width=600, dpi=700}
suppressPackageStartupMessages(library(knitr))
include_graphics("C:/Users/ThinkPad User/Documents/CompetitionPolicy/Problem Set/mediation.jpg")
```
#>

The purpose of competition policy is, by definition, to maintain or enhance competition. Furthermore, we assume that more intense competition forces firms to innovate, and accordingly, increase their productivity. That is to say, the causal effect of competition policy on productivity growth goes through its effect on competition. If we were to control for competition, we would block this causal mechanism and estimates of the effect should be zero (Pearl (2018), pp. 152-153 and Chapter 9). 

Competition here is an example for what is called a *bad control* in regression analysis (Angrist and Pischke (2009), pp. 63-67). A rule of thumb when thinking about "good" and "bad" controls is to consider the time of determination. Variables that were already fixed when the causative variable of interest is determined usually make for good controls. Variables that are determined *after* the causal variable are likely bad controls. These variables might be a channel through which a causal effect goes, or might be part of what we actually want to measure.

Such an intermediate variable in the causal graph above is also called a *mediator*, because it *mediates* the causal effect of another variable (Pearl (2018), Chapter 9). Conditioning on a mediator blocks causal effects and completely changes the interpretation of what we estimate. However, mediation analysis has its place in research. Causal relationships are often not as simple as we assume here. A cause often affects an outcome through multiple channels. Intentionally blocking a mediator can provide deeper insights into the causal mechanism at play. In our case, we could ask whether other channels exist through which competition policy affects productivity growth. Conceivably, competition policy might strengthen the legal system and the institutional environment in general. The analysis of mechanisms is not part of our investigation, though. We merely care about the total effect of competition policy.

To sum up, we conclude that controlling for every covariate we could possibly think of can do more harm than good. Using prior knowledge and introspection, we should have a clear idea of the causal relationships among variables, and only include variables that are justified to answer the scientific question of interest.

### d) OLS Estimation and Results

Now that we have discussed the model and the controls, we are ready to obtain the OLS estimates. We introduced all needed functionality in previous exercises. We just modify our fixed effects regression model from exercise 4 by including the other controls. 

**Task:** Regress `tfp` on `cpi`, `tfpleader`, `tecgap`, `trend`, `trade` and `pmr`. Specify `id` and `year` as fixed effects and store the results in `reg`. The code is already given below; just click `check`.
```{r}
#< task_notest
library(lfe)
reg = felm(tfp ~ cpi + tfpleader + tecgap + trend + trade + pmr |
                 id + year | 0 | country, data = dat)
#>
```

#< award "Regression Apprentice"
Congratulations! You successfully the multiple regression problem including the fixed effects transformation.
#>

We also want to compare the results to our previous findings. I prepared the next task to replicate the simple regression from exercise 3 and the fixed effects model (including PMR) from exercise 4.

**Task:** Show the results of `reg` and compare them to our previous findings in a `stargazer` table. The code is already given below; just click `check`.
```{r results = 'asis', warning = FALSE}
#< task_notest
# PMR + fixed effects regression from exercise 4
fe = felm(tfp ~ cpi + pmr | id + year | 0 | country, data = dat) 

# Simple regression from exercise 3
simple = felm(tfp ~ cpi | 0 | 0 | country, data = dat)

# Organize results in a stargazer table
library(stargazer)
stargazer(reg, fe, simple,
          type = "html",
          title = "Table 5.0: Main Specification",
          style = "aer", 
          digits = 3,
          column.labels = c("Full Model", "PMR + FE", "Simple"),
          omit = c("Constant"),
          add.lines = list(c("Country-Industry Fixed Effects",
                             rep("Yes", 2), "No"),
                           c("Time Fixed Effects", rep("Yes", 2), "No")))
#>
```


The left column of the table shows the results of our main specification. The coefficient of the CPI is $\hat{\beta} = 0.092$ and still significant at the $1\%$-Level. A $95\%$ confidence interval for the coefficient is $[0.049, 0.135]$ ($\approx \hat\beta \pm 1.96 \times CSE$, see Wooldridge (2016), pp. 122-124 for a discussion on confidence intervals). The confidence interval indicates a considerable range of plausible parameter values. However, even the lower bound is not "close" to zero. A one standard deviation increase in the CPI is estimated to increase TFP growth by approximately $1$ percentage point ($= 0.092 \cdot 0.11$), which is arguably an economically relevant effect. The coefficient is slightly larger compared to the estimates from previous exercises. We elaborate on the covariates in the next section.

### e) Comparing Effect Sizes using Standardized Coefficients

Unfortunately, we cannot directly compare the coefficients in a multiple regression setting unless the variables are measured on the same scale. The fact that the CPI has the largest coefficient in table 5 does not imply that it is the most important determinant of TFP growth. However, we can transform the variables to be on a common scale. By subtracting the mean and dividing by the standard deviation, we obtain the *z-score* of a variable:
$$
z_x = \frac{X-\mu_x}{\sigma_x}. \tag{5.2}
$$
We call such a transformed variable *standardized*, because its mean is zero with a standard deviation of one. If we transform all variables in a regression model to z-scores, we can directly compare the coefficient estimates (Wooldridge (2016), pp. 169-170). Note that this approach does not make sense for all types of variables and distributions (think of dummies and distributions that are far away from being normal, etc.). For the next task, I wrote a custom function called `scale_dat()`, which replaces numeric variables in a data frame with their z-scores. If you want to see the code, or are interested in writing your own functions in R, check out the info box below.

#< info "Writing custom functions in R - scale_dat()"
Writing custom functions in R can be useful to automate repetitive tasks. Here is a short description of `scale_dat()`. The initial `stopifnot()` statement checks whether the passed argument is a data frame. If not, the function stops and displays an error message. The second part specifies a function `sc()` that computes the z-score of an argument `x`. Non-numeric variables are returned without applying any transformation. This function is then used in `lapply()` to transform each column of the data frame. The family of `apply()` functions in R is very handy for performing iterative tasks without using `for`-loops. All columns are then stored in a data frame and returned by the function as final output.
```{r eval=FALSE}
scale_dat = function(dat){
  stopifnot("Argument 'dat' must be a data frame" = (is.data.frame(dat)))
  
  # Function for lapply
  sc = function(x){
    if (is.numeric(x)){
      x = as.numeric(scale(x, center = TRUE, scale = TRUE))
    }
    return(x)  
  }
  
  # Create data frame with lapply  
  scaled = as.data.frame(lapply(dat, FUN = sc))
  return(scaled)
}
```
If you want to learn more about writing functions in R, refer to Kabakoff (2015, pp. 107-109), Wickham and Grolemund (2016, Chapter 15) or check out the R documentation.
https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/function
#>

**Task:** Standardize the variables in `dat` using `scale_dat()`. I already loaded the function in the background. Store the output in a new data frame `dat_z`. Then, run the regression again using `dat_z` and assign the results to `reg_z`. The code is already given below; just click `check`.
```{r}
#< task_notest
# Compute standardized variables
dat_z = scale_dat(dat)

# Regression using standardized variables
reg_z = felm(tfp ~ cpi + tfpleader + tecgap + trend + trade + pmr | 
                   id + year | 0 | country, data = dat_z)
#>
```

Instead of looking at yet another regression table, we create a visualization of the results in the next tasks. This way we can also include confidence intervals for the estimates. I wrote a second function `confint_frame()`, which extracts the coefficients from a `felm` object and computes confidence intervals using CSE. If you are also interested in the function code, check out the info box below.

#< info "The confint_frame() function"
The function takes an object of class `felm` and a confidence level `alpha` as an argument. The initial `stopifnot()` statement confirms that the argument `reg` is `felm` object and `alpha` lies between zero and one. If either one fails, the function stops and returns an error message. Then a data frame is created that contains columns for the coefficients, variable names and the boundaries of the confidence intervals. It is constructed using the corresponding quantile of the t-distribution and uses CSE.
```{r eval=FALSE}
confint_frame = function(reg, alpha = 0.05){
  stopifnot(
    "Argument 'reg' has to be class 'felm'" = (class(reg) == "felm"),
    "Alpha has to be a single value" = (length(alpha) == 1),
    "Alpha must be greater than 0" = (alpha > 0),
    "Alpha must be smaller than 1" = (alpha < 1)
  )
  
  p = 1 - alpha/2
  df = data.frame(coef = coef(reg),
                  var = names(coef(reg)),
                  lower = coef(reg) - qt(p = p, df = reg$df) * reg$cse,
                  upper = coef(reg) + qt(p = p, df = reg$df) * reg$cse)
  return(df)
}
```
If you want to learn more about writing functions in R refer to Kabakoff (2015, pp. 107-109), Wickham and Grolemund (2016, Chapter 15) or check out the R documentation.
https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/function
#>

**Task:** Apply the `confint_frame()` function to `reg_z` and store the results in `dat_confint`. Use `ggplot()` to display the standardized coefficients and their $95\%$ confidence intervals. The code is already given below; just click `check` to view the results.
```{r out.width=600, dpi=700}
#< task_notest
# Generate data frame
dat_confint = confint_frame(reg_z, alpha = 0.05)

# Plot standardized coefficients and 95% confidence intervals
ggplot(dat_confint, aes(x = coef, y = reorder(var, coef))) +
  geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.15,
                 size = 0.75, colour = "#6794a7") +
  geom_point(aes(x = coef, y = reorder(var, coef)), 
             colour = "#6794a7", fill = "#6794a7", size = 3.25, shape = 25) +
  geom_vline(xintercept = 0, color = "black", size = 0.7,
             alpha = 0.5, linetype = "dotted") +
  theme_blue(base_size = 11) +
  scale_x_continuous(breaks = seq(-0.6, 0.6, by = 0.15)) +
  labs(title = "Standardized Coefficients",
       subtitle = "Including 95% Confidence Intervals",
       x = "Coefficient",
       y = "",
       caption = "Figure 5.0: Standardized Coefficients")
#>
```

The plot shows the variable's coefficients ordered by size. We can interpret the coefficient $\hat{\beta}_z$ for the CPI as follows: A one standard deviation increase of the CPI is associated with an increase of TFP growth by about $0.14$ standard deviations. The main advantage of this representation is that we can directly compare the effect sizes of the independent variables. The variables `trade` and `trend` have the largest coefficients. An increase of trade openness by one standard deviation is associated with an increased TFP growth by roughly $0.38$ standard deviations. It seems plausible that the economic forces behind trade openness and strong industry growth have a major impact on productivity. The coefficients for `tfpleader` and `tecgap` are also positive, as expected. Laggard industries have higher productivity growth if there is a more advanced technology leader and a larger technology gap. The coefficient for the variable `pmr` is negative - stricter regulation is associated with lower TFP growth. Although the CPI has not the largest coefficient, the impact of competition policy is not negligible when compared to the other variables in the model.

The confidence intervals for `trade` and `pmr` are particularly interesting. A reason for the relatively large CSEs in this case is the fixed effects transformation. Because we only use within country-industry variation, there is little variation left to precisely estimate coefficients for variables that do not change a lot over time (Kennedy (2008), p. 286). The import penetration of country-industries is fairly stable over the period of a few years. This problem is elevated for `pmr`, where we only have three observations for each country - the OECD PMR Index is only updated every few years. The confidence interval for the CPI is narrower, since there is considerable variation within countries. For example, recall exercise 2 where we showed that the CPI for the UK has increased almost two-fold over the sample period. We can also estimate the coefficient more precisely compared to `trade` because we got a whole set of country-industries that "respond" to changes in competition policies on the country-level. In the next section, we dive a bit deeper into the workings of multiple regression. 

### f) Regression Anatomy

In exercise 2, we used a scatterplot to visualize the relationship between the CPI and TPF growth as observed in the data. To do so, we displayed the independent variable on the x-axis, the dependent variable on the y-axis, and included a regression line. Unfortunately, this approach reaches its limits quickly in the multivariate case. While a third variable could be included on the z-axis, further extensions are difficult since we cannot draw more coordinate axes. However, using *regression anatomy* we can obtain a two-dimensional representation of the relationship between two variables in a multiple regression model (Angrist and Pischke (2009), pp. 34-35). This approach is based on the *Frisch-Waugh-Lovell Theorem*, which states that the coefficient $\hat\beta_k$ of variable $x_k$ in a multiple linear regression can also be obtained by performing two sequential steps:

1. Regress $x_k$ on the $k - 1$ covariates of the original regression model and obtain the residual $\tilde x_k$.
2. Regress the dependent variable $y$ on the residual $\tilde x_k$ from step one.

In the following, we apply regression anatomy to our case, where we want to uncover the coefficient of the CPI. First, we regress $CPI_{i, t-1}$ on the covariates:
$$
CPI_{i, t-1}  = \gamma_0 + \gamma_1 \Delta TFP_{L,j,t} + ... + \widetilde{CPI}_{i,t-1} \tag{5.3}
$$
where I omitted the other independent variables. Hence, $\widetilde{CPI}_{i,t-1}$ is part of variation in $CPI_{i, t-1}$ that cannot be explained by any of the other variables. We call this component henceforth the "residualized CPI".

**Task:** Perform the first step of regression anatomy. To do so, regress `cpi` on the variables `tfpleader`, `tecgap`, `trend`, `pmr`, `id` and `year` using the `felm()` function. Store the result in a variable `cpi_reg`. The code is already given below; just click `check`.
```{r}
#< task_notest
cpi_reg = felm(cpi ~ tfpleader + tecgap + trend + trade + pmr +
                     id + year, data = dat)
#>
```

We are not interested in the coefficients of this regression. This first step merely serves to obtain the residuals. The `felm` object stores, besides other statistics we already discovered, also the residuals. We can easily extract them with the `resid()` function, and append them to our data.

#< info "The resid() function"
The `resid()` function extracts the residuals of a regression model. As an argument, we pass the object that stores the model results.
```{r eval=FALSE}
resid(model)
```
The function returns an ordered vector. For more details check out the function documentation.
https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/residuals
#>

**Task:** Obtain the residuals of `cpi_reg` using the `resid()` function. Use `mutate()` to create a new variable `cpi_res`, stored in `dat`. I already gave you part of the solution; just complete/replace the function call and click `check`.
```{r}
#< fill_in
# Insert the correct expressions for the placeholders ___ below.
dat = mutate(dat, ___)
#>
dat = mutate(dat, cpi_res = resid(cpi_reg))
```

As the second step, we regress the dependent variable on the residualized independent variable to obtain the OLS coefficient $\hat \beta_1$:
$$
\Delta TFP_{i,j,t} = \beta_0 + \beta_1 \widetilde{CPI}_{i,t-1} + \varepsilon_{i,j,t}. \tag{5.4}
$$
**Task:** Regress `tfp` on `cpi_res` and store the results in `reg_anat`. Show the results with `stargazer()`. The code is already given below; just click `check`.
```{r results = 'asis', warnings = FALSE}
#< task_notest
# Perform bivariate regression
reg_anat = felm(tfp ~ cpi_res, data = dat)

# Organize results in a stargazer table
stargazer(reg_anat, 
          type = "html",
          title = "Table 5.1: Regression Anatomy",
          style = "aer", 
          digits = 3)
#>
```


Looking at the results, we see that the obtained coefficient $\hat\beta_1 = 0.092$ exactly matches the coefficient obtained in our multiple regression model. Note that the SE is not correct when obtaining the coefficient in this way. Using the residualized CPI, we can produce a plot similar to the one in exercise 2. Instead of displaying the CPI on the x-axis, we use the residualized CPI.

**Task:** Show a scatterplot of `cpi_res` on the x-axis and `tfp` on the y-axis using `ggplot()`. Additionally, overlay the slope of the regression line. The code is already given below; just click `check`.
```{r out.width=600, dpi=700}
#< task_notest
# Scatter plot residualized CPI and TFP
ggplot(data = dat, aes(x = cpi_res, y = tfp)) +
        geom_point(colour = "#6794a7", shape = 21, size = 1.0) +
        geom_smooth(method = "lm", formula = y ~ x,
                    se = FALSE, colour = "#d23e4e") +
        annotate("text", x = -0.09, y = 0.25,
                 label = "italic(beta) == 0.092", 
                 parse = TRUE, colour = "#F8766D", size = 4) +
        coord_fixed(ratio = 0.3) +
        theme_blue() +
        ggtitle("Regression Anatomy",
                subtitle = "") +
        labs(x = "residualized CPI",
             y = "TFP growth",
             caption = "Figure 5.1: Regression Anatomy")
#>
```

The plot still shows a positive relationship, most easily visible by the imposed regression slope. For low residualized values, we predict about zero productivity growth, whereas for high values we predict roughly two percent. Regression anatomy is not only useful to produce such visualizations. Most importantly, it sheds light on what actually happens in linear regression: The coefficient in a multiple regression is the bivariate slope after "partialling out" all other independent variables (Angrist and Pischke (2009), p. 35). This clarifies what we actually mean with expressions such as "controlling for", or "holding everything else fixed". In experiments, random assignment ensures that we solely manipulate the variable of interest in order to obtain a valid estimate for a causal effect. This approach holds other determinants of the outcome fixed and gives estimates a causal interpretation. In multiple regression analysis of observational data, we try to approximate such an experiment by isolating variation in a variable of interest that cannot be explained by any other determinant. We hope that the remaining variation is "quasi-experimental", and therefore causally related to the outcome (Wooldridge (2016), p. 69). As final step in our interpretation, we compute the *elasticity* of productivity growth with respect to the CPI in the next task. Then we use the estimated elasticity to compute a counterfactual realization of productivity growth for UK's food products industry.

### g) An Example for the Economic Relevance of Competition Policy

In economics, we commonly express relationships using *elasticities* (Wooldridge (2016), pp. 637-639). An elasticity measures the percentage change in one variable when another, causative variable changes by $1$ percent. An advantage of this representation is that it summarizes the relationship without using any units. We can calculate the elasticity of $y$ with respect to $x$ as follows:
$$
E = \frac{\%\Delta y}{\%\Delta x} = \frac{\Delta y}{\Delta x} \cdot \frac{x}{y} \tag{5.5}
$$
where $\Delta y$ and $\Delta x$ are the changes in $y$ and $x$, respectively. Equation $(5.5)$ clearly shows that $E$ depends on the level of $x$ and $y$. If $y$ is a linear function of $x$, the elasticity is given by
$$
E = \beta_1 \frac{x}{y} = \beta_1 \frac{x}{\beta_0 + \beta_1 x}.  \tag{5.6}
$$
Thus, we can use the estimated coefficient of a linear model to compute $E$ at any values $x$ and $y$. In the upcoming task, we compute the elasticity of TFP growth with respect to the CPI at the mean value of the variables.

**Task:** Compute the elasticity of `tfp` with respect to `cpi` at the mean values of the variables. Recall that $\hat\beta = 0.092$. The code is already given below; just click `check`.
```{r}
#< task_notest
0.092*(mean(dat$cpi)/mean(dat$tfp))
#>
```


#< quiz "elasticity"
question: The elasticity suggests that a one percent increase in the CPI is estimated to increase TFP growth by approximately...
sc:
    - ... 4.72 percentage points.
    - ... 4.72 percent.*
    
success: Great!.
failure: Try again.
#>


The elasticity of TFP growth with respect to the CPI is about $4.72$. If we assume that the relationship is indeed causal, an increase of the index by $1$ percent translates into an increase of TFP growth by about $4.72$ percent. To highlight the implication, the authors give a concrete example, which we replicate below (see Buccirossi et al. (2013), p. 1313). To do so, we return to the food products industry in the UK. 

**Task:** Show `cpi` and `tfp` of UK's food products industry for the years `2001` and `2002`. The code is already given below below; just click `check`.
```{r}
#< task_notest
# Display relevant data
dat %>%
  mutate(cpi = lead(cpi, n = 1, order_by = id)) %>%
  filter(industry == "15-16" & country == "UK" &
         year %in% c("2001", "2002")) %>%
  select(year, tfp, cpi)
#>
```

In the code above, we use `lead()` to account for the fact that the CPI was lagged by one year in the data. The function call reverses this transformation so that the values correspond to the actual observations. TFP growth was about $3.22$ percent in 2001 and $5.22$ percent in 2002. In those years, the CPI increased from $0.518$ to $0.542$. We now ask how much of the increase in productivity growth is actually accounted for by improvements in competition policy. First, we calculate the percentage changes from 2001 to 2002:
$$
\delta TFP = \frac{0.0522-0.0322}{0.0322} = 0.6211 \approx 62\% \tag{5.7}
$$
$$
\delta CPI = \frac{0.542 - 0.518}{0.518} = 0.0463 \approx 4.6\%. \tag{5.8}
$$
TFP growth increased by about $62$ percent; the CPI by about $4.6$ percent. Considering the estimated elasticity, improvements in competition policy explain about $21.7$ percent ($=4.6 \cdot 4.72$) of the increase in TFP growth. We can further use this calculation to compute the *counterfactual* TFP growth for 2002 - that is, the hypothetical TFP growth if the improvement in competition policy would not have been realized: 
$$
\widehat{\Delta TFP} = 0.0322 \cdot(1 + 0.6211 - 0.217) = 0.0452 \approx 4.5\% \tag{5.9}
$$
Hence, TFP growth would have been around $4.5$ percent in 2002, instead of $5.2$ percent (Buccirossi et al. (2013), p. 1313). This further illustrates that the results point at a substantial effect of competition policy on productivity outcomes. 

In this exercise, we estimated the main econometric specification of the article. We have shown that the effect of competition policy appears to be both statistically and economically relevant. Before we continue with our formal analysis, we take a closer look at the CPI in exercise 6.

## Exercise 6 -- Exploring the Competition Policy Indexes

### a) The Construction of the CPIs

In previous exercises, we used the aggregate CPI as an indicator for the quality of competition policy regimes, without considering what information it actually incorporates. In this exercise, we fill this gap and shed some light on its construction. Instead of performing a lengthy formal discussion, we explore the components of the CPI and the applied aggregation procedure by using an interesting visualization technique: *treemaps* (for examples, see R Graph Gallery (2020)). Treemaps are well suited for hierarchically structured data or elements. In the following, we decompose the aggregate CPI step-by-step into its constituents. Using this approach, we will also learn more about competition policy regimes in general. The CPIs are explained in detail in the article **"Measuring the Deterrence Properties of Competition Policy: The Competition Policy Indexes"** by Buccirossi et al. (2011). Like previously, I provide the full code for the visualizations in the upcoming tasks.

#< info "The treemap package"
The `treemap` package offers the `treemap()` function, which can be used to create and customize treemaps in a convenient way. In order to plot a treemap, data have to be prepared in a hierarchical structure. Numerical values (weights or shares) determine the area of the rectangles. The `treemap()` function provides arguments to specify the visual appearance, like different colors or fonts. If you want to learn more about the package and its function, refer to the documentation.
https://cran.r-project.org/web/packages/treemap/index.html
#>

**Task:** Use the `treemap()` function to visualize the high-level components of the aggregate CPI. I already prepared data and code below; just click `check`.
```{r out.width=500, dpi=600, warning=FALSE}
#< task_notest
# Load list with structured data
cpis = readRDS("cpis.rds")

# Display high-level CPI
library(treemap)
treemap(cpis[["cpi_top"]],
        index = c("group", "subgroup"),
        vSize = "weight",
        type = "index",
        palette = c("#d23e4e", "#014d64"),
        border.lwds=c(5,2), 
        fontsize.labels=c(15,12),
        fontcolor.labels=c("white","orange"),
        bg.labels = c("transparent"),
        fontface.labels=c(2,1),
        align.labels = list(c("center", "center"), 
                            c("left", "bottom")),                   
        title = "The Aggregate CPI",
        fontsize.title = 15)
#>
```

The idea of treemaps is to split an element into its constituents, by using a set of rectangles. Arrangement and colors of the rectangles emphasize the underlying hierarchical structure. The area of a rectangle represent the share (or weight) an element has from its higher order element. Looking at the treemap, we see that the aggregate CPI consists of an antitrust-related and a mergers-related component. Both components are summarized by an **Antitrust-CPI** and a **Mergers-CPI**, which are linearly combined to form the aggregate CPI (Buccirossi et al. (2011), pp. 183-186). The weight each component has is indicated by the size of the rectangles; here it is $3:1$. Going one hierarchy level deeper, the Antitrust-CPI itself consists of three elements. **Abuses** relates to features of competition policy regimes dealing with firms that exploit a dominant market position, for example through the fact of predatory pricing. Cases where competing firms coordinate their market conduct, like price-fixing, fall into the **Hardcore Cartels** category. Further competition violations are collected under the **Other Agreements** component (for a formal treatment of these anti-competitive behaviors, refer to Belleflamme and Peitz (2015), Chapters 14, 16, 17).

Mergers-related components have more of a "precautionary" character. To prevent the accumulation of excessive market power, which my lead to higher prices for consumers and reduced social welfare, competition law sets hurdles for the fusion of firms (Belleflamme and Peitz (2015), Chapter 15). Competition authorities investigate whether a potential merger endangers competition and deny the fusion if this were the case. Features that deal with these issues are collected in the Mergers-CPI. 

The treemap above shows the high-level components of the aggregate CPI. Each rectangle itself consists of several subcomponents. As an example, we pick Hardcore Cartels and show its construction with another treemap.

**Task:** Use the `treemap()` function to display the cartel-related components of the CPI. I already prepared data and code below; just click `check`.
```{r out.width=500, dpi=600, warning=FALSE}
#< task_notest
# Display antitrust treemap
treemap(cpis[["cartels"]],
        index = c("group", "subgroup"),
        vSize = "weight",
        type = "index",
        palette = c("#d23e4e", "#014d64"),
        border.lwds=c(5,2), 
        fontsize.labels=c(15,12),
        fontcolor.labels=c("white","orange"),
        bg.labels = c("transparent"),
        fontface.labels=c(2,1),
        align.labels = list(c("center", "center"), 
                            c("left", "bottom")),                   
        title = "Hardcore Cartels",
        fontsize.title = 15)
#>
```

The **Hardcore Cartels** component of the CPI consists of institutional and enforcement features. The written law and the architecture of related competition authorities determine the institutional component. One example for the former is the severity of potential sanctions imposed on cartels. Draconian punishment presumably results in better deterrence properties of competition policy regimes (Buccirossi et al. (2011), p. 169). Another important feature is the formal independence of competition authorities. If politicians (and economic interests) could influence cartel cases, outcomes should be worse from an economic perspective (Buccirossi et al. (2011), pp. 173-174). 

The enforcement features of **Hardcore Cartels** are determined by available resources and prior success of competition authorities. We zoom in one step further to obtain the low-level indicators. First, we select **Enforcement**.

**Task:** Use the `treemap()` function to display the enforcement features of the **Hardcore Cartel** component. I already prepared data and code below; just click `check`.
```{r out.width=500, dpi=600, warning=FALSE}
#< task_notest
# Display enforcement treemap
treemap(cpis[["cartels_enf"]],
        index = c("group", "subgroup"),
        vSize = "weight",
        type = "index",
        palette = c("#d23e4e", "#014d64"),
        border.lwds=c(5,2), 
        fontsize.labels=c(15,12),
        fontcolor.labels=c("white","orange"),
        bg.labels = c("transparent"),
        fontface.labels=c(2,1),
        align.labels = list(c("center", "center"), 
                            c("left", "bottom")),                   
        title = "Enforcement Features of Hardcore Cartels",
        fontsize.title = 15)
#>
```

Resources available to competition authorities are measured by budget, staff skills and size of staff. Better-equipped authorities should be better at prosecuting infringements of antitrust law. The enforcement features also include success in prior cases handled by the authorities, measured by maximum jail terms imposed and number of cases. This approach accounts for the fact that written sanctions should matter less if they are not actually issued (Buccirossi et al. (2011), p. 181). The authors collected data on these low-level indicators directly from the competition authorities. In the final treemap, we visualize the low-level indicators of the institutional features of **Hardcore Cartels**.

**Task:** Use the `treemap()` function to display the institutional features of **Hardcore Cartels**. I already prepared data and code below; just click `check` to plot the treemap.
```{r out.width=500, dpi=600, warning=FALSE}
#< task_notest
# Display insitutions treemap
treemap(cpis[["cartels_inst"]],
        index = c("group", "subgroup"),
        vSize = "weight",
        type = "index",
        palette = c("#d23e4e", "#00887d", "#6794a7", "#014d64", "#ee8f71"),
        border.lwds=c(5,2), 
        fontsize.labels=c(15,11),
        fontcolor.labels=c("white","orange"),
        bg.labels = c("transparent"),
        fontface.labels=c(2,1),
        align.labels = list(c("center", "top"), 
                            c("left", "bottom")),                   
        title = "Institutional Features of Hardcore Cartels",
        fontsize.title = 15)
#>
```

#< award "Visualization Apprentice"
Congratulations! You successfully used treemaps to visualize hierarchically structured data.
#>

The authors measured most of the institutional features on a binary or ordinal scale. If, for example, a policy regime has a *leniency program*, the corresponding indicator in **Quality of the Law** takes on a value of $1$, otherwise it is $0$. Leniency programs offer corporate "whistleblowers" reduced sanctions for reporting cartels. Such programs increase the likelihood of discovery and therefore improve the deterrence properties of competition policy (Buccirossi et al. (2011), p. 177). As a final example, consider the **Sanctions and Damages** category. Besides sanctions applicable to firms and individuals, it also contains whether private actions against cartels are possible, for instance through class action lawsuits (Buccirossi et al. (2011), p. 180). 

A linear combination of the institutional and enforcement-related low-level indicators scores the **Hardcore Cartels** component of a competition policy regime. The authors gave each indicator an equal weight in the aggregation, unless theory suggested that some feature should be more important (Buccirossi et al. (2011), pp. 181-183). This approach was also applied to the other three categories **Abuses**, **Other Agreements** and **Mergers**. The aggregate CPI therefore incorporates a broad spectrum of information on competition policy regimes to allow for cross-country and cross-time comparisons.

### b) Competition Policy in the UK

Now that we have a better understanding for the CPIs and competition policy in general, we return to the data and look at the high-level disaggregated indexes. From now on, we load the dataset `dat2.rds`, which is identical to the prior file except that it also contains the control variable `trade` we computed in exercise 5.

**Task:** Load `dat2.rds` and use the `head()` function to show the upper entries of the disaggregated indexes. The code is already given below; just click `check`.
```{r}
#< task_notest
library(dplyr)
dat = readRDS("dat2.rds")

# Show CPIs
dat %>% 
  select(id, year, cpi, antitrust, mergers, institutions, enforcement) %>%
  head(n = 5)
#>
```

We see four new variables: `antitrust`, `mergers`, `institutions` and `enforcement`. These columns contain the values of the disaggregated CPIs, as described above. Identical to the aggregated CPI, their values range from $0$ and $1$. In the upcoming two tasks, we want to come back to our exemplary country, the UK, and investigate its CPIs.

#< info "The tidyr package"
The next task uses the `pivot_longer()` function of the `tidyr` package to transform the data into a "longer" format. This function reduces the number of columns of a data frame by collecting their information in additional rows. It is a simple transformation that is useful for some visualizations with `ggplot()`.
```{r eval=FALSE}
library(tidyr)
pivot_longer(cols, names_to, values_to)
```
The first argument `cols` specifies which columns to remove. The arguments `names_to` and `values_to` set the names of the two newly created columns that store the information of the removed columns. If you want to learn more about the package and its functions, refer to the package documentation. 
https://cran.r-project.org/web/packages/tidyr/index.html
#>

**Task:** Use the `ggplot()` function to plot a line chart for `cpi`, `antitrust` and `mergers` in the UK and also include the sample averages. The code is already given below; just click `check`.
```{r out.width=600, dpi=700}
#< task_notest
library(ggplot2)
library(tidyr)
countries = readRDS("countries.rds")
# Arrange disaggregated indexes in a long format
uk = countries %>% 
  filter(country %in% c("UK", "Average")) %>% 
  pivot_longer(cols = c(cpi, antitrust, mergers, institutions, enforcement), 
               names_to = "type", values_to = "cpi")

# Plot line chart
uk %>% 
  filter(type %in% c("cpi", "antitrust", "mergers")) %>% 
  ggplot(aes(x = as.numeric(year), y = cpi, colour = type)) + 
  geom_line(size = 1.0) + 
  geom_point(size = 1.5) + 
  scale_x_continuous(breaks = 1995:2005) + 
  facet_wrap(~ country, nrow = 2, ncol = 1) + 
  coord_fixed(ratio = 8) + 
  theme_blue() + 
  theme(legend.position = "top", legend.title = element_blank()) + 
  scale_colour_manual(values = c("#d23e4e", "grey", "#014d64")) + 
  labs(title = "CPIs of the UK", x = "Year", y = "CPI", 
       caption = "Figure 6.4: Disaggregated CPIs for the UK")
#>
```

Looking at both panels, we observe that the indexes are, by construction, closely related. As discussed above, the aggregate CPI is mostly determined by the antitrust component, with a ratio of $3:1$. UK's CPI has increased almost two-fold over the sample period. The average CPI of all countries shows also an increase due to improvements in the antitrust component. What reasons explain the remarkable upward trend in the UK?

The UK passed two substantial reforms in the years 1998 and 2002. The *Competition Act* (1998) largely harmonized competition policy in the UK with European competition policy. This framework introduced prohibitions of agreements between firms aiming at limiting output, fixing prices and others. The act also has a chapter on abuses of market power, such as price discrimination and refusal to supply. Such practices were forbidden for the most part. Even though the UK had competition law before 1998, it lacked clear objectives and enforcement capabilities (Belleflamme and Peitz (2015), pp. 730-731). The *Competition Act* became active in 2000, hence the sudden rise of the CPIs in this year.

The second reform was the *Enterprise Act* (2002). This framework expanded competition policy in the UK with several new rules. To give one concrete example, the act introduced jail terms of a maximum of five years for directors convicted of forming cartels. Both reforms improved the antitrust and mergers features of competition policy in the UK substantially, as measured by the CPIs. We investigate the institutional and enforcement features in the next task.

**Task:** Use the `ggplot()` function to plot a line chart for `cpi`, `institutions` and `enforcement` in the UK and also include the sample averages. The code is already given below; just click `check`.
```{r out.width=600, dpi=700}
#< task_notest
# Plot line chart
uk %>%  
  filter(type %in% c("cpi", "institutions", "enforcement")) %>% 
  ggplot(aes(x = as.numeric(year), y = cpi, colour = type)) + 
  geom_line(size = 1.0) + geom_point(size = 1.5) + 
  scale_x_continuous(breaks = 1995:2005) + 
  facet_wrap(~ country, nrow = 2, ncol = 1) + 
  coord_fixed(ratio = 6) + 
  theme_blue() + 
  theme(legend.position = "top", legend.title = element_blank()) +
  scale_colour_manual(values = c("grey", "#d23e4e", "#014d64")) + 
  labs(title = "Disaggregated CPIs of the UK (II)", x = "Year", y = "CPI", 
       caption = "Figure 6.5: Disaggregated CPIs for the UK")
#>
```

The upper panel shows that the enforcement features of UK's competition policy score lower than the institutional features. However, this is also the case for the sample average of all countries displayed in the lower panel. The movement of the **Enforcement-CPI** is "smoother" than the **Institutions-CPI**, which comes from the fact that it is mostly determined by numerical measures, such as budget or staff count. The institutional component consists of binary or ordinal features, as discussed above. Again, the effects of the *Competition Act* and the *Enterprise Act* are visible. As an example on the institutional side, the *Enterprise Act* made the authority *Office of Fair Trading* formally independent from the government. We have seen above that the CPIs incorporate this information, which leads to better scores. Enforcement-related improvements include the allocation of more resources towards competition authorities, as indicated by the upward trend of the Enforcement-CPI.

### c) Importance of the individual Features of Competition Policy 

Now that we have introduced the disaggregated CPIs, we ask if some features of competition policy are more important than others. For instance, has the institutional or the enforcement component a bigger impact on TFP growth? We can answer to this question by replacing the aggregated CPI with the disaggregated indexes in our regression model. We run a separate regression for each disaggregate CPI to detect whether there are substantial differences in slopes.

**Task:** Estimate our main specification from exercise 5 using `cpi`, `institutions`, `enforcement`, `antitrust` and `mergers` separately. Combine the results in a `stargazer` table. The code is already given below; just click `check`.
```{r results = 'asis', warnings = FALSE}
#< task_notest
library(lfe)
# Regressions using aggregated and disaggregated CPIs
cpi = felm(tfp ~ cpi + tfpleader + tecgap + trade + pmr + trend |
                 id + year | 0 | country, data = dat)
inst = felm(tfp ~ institutions + tfpleader + tecgap + trade + pmr + trend | 
                  id + year | 0 | country, data = dat)
enf = felm(tfp ~ enforcement + tfpleader + tecgap + trade + pmr + trend |
                 id + year | 0 | country, data = dat)
anti = felm(tfp ~ antitrust + tfpleader + tecgap + trade + pmr + trend | 
                  id + year | 0 | country, data = dat)
merg = felm(tfp ~ mergers + tfpleader + tecgap + trade + pmr + trend |
                  id + year | 0 | country, data = dat)

# Organize results in a table
library(stargazer)
stargazer(cpi, inst, enf, anti, merg, 
          type = "html",
          title = "Table 6.0: Disaggregated Indexes",
          style = "aer", 
          digits = 3,
          column.labels = c("CPI", "Inst", "Enf", "Anti", "Merg"),
          omit = c("tfpleader", "tecgap", "trade", "pmr", "trend"),
          add.lines = list(c("Full Set of Covariates", rep("Yes", 5)),
                           c("Country-Industry Fixed Effects",
                             rep("Yes", 5)),
                           c("Time Fixed Effects", rep("Yes", 5))))
#>
```


#< quiz "importance"
question: Which component of competition policy seems to be the most important?
sc:
    - Institutional Features
    - Enforcement Features
    - Antitrust Features*
    - Merger Features
    
success: Great!.
failure: Try again.
#>


#< award "Competition Politician"
  Congratulations! You are no longer a rookie in competition policy.
#>

For better readability, I omitted the controls in the table above. The left column shows the already known result for the aggregate CPI. All four coefficients of the disaggregated indexes are still significant at either the $95\%$ or the $99\%$ level. Looking at the magnitude of the coefficients, institutional features seem to have a larger effect on TFP growth than enforcement-related features. Furthermore, the antitrust component seems to be more important than the mergers component. However, the CSE indicate relatively wide confidence intervals. We could employ formal tests to see if the differences are actually statistically significant, but we omit them here. In exercise 8, we perform such a formal comparison in another context.

In this exercise, we shed some light on the aggregate CPI and its construction. Using the UK as an example, we saw what kind of concrete policies contribute to the index. All four high-level disaggregated indexes are also significantly associated with higher country-industry performance. In the following, we return to the problem of endogeneity and introduce the method of *instrumental variable estimation*.

## Exercise 7 -- Instrumental Variable Estimation

### a) Instrumental Variable Estimation and Two Stage Least Squares

In our attempt to assess the effectiveness of competition policy, we identified the potential endogeneity of the CPI as primary issue. If the CPI is correlated with the error term, for instance because we omitted an important variable, estimates of the causal effect of competition policy are biased and inconsistent. In exercise 4 and 5, we developed how control variables and fixed effects can be harnessed to solve or at least mitigate the problem of OVB. Still, there might be relevant variables left we have no data on, or we do not even know. In this exercise, we use another approach to tackle the endogeneity issue: *instrumental variable (IV) estimation* (Kennedy (2009), Chapter 9). This method explicitly recognizes the presence of one or more endogenous variables, and still allows consistent estimation of the population parameters of interest. In this section, we introduce the conceptual idea of IV estimation before we apply it to the data.

The success of the IV approach depends on being able to find one or more suitable *instrumental variables* (short: *instruments*) for the endogenous independent variable(s) $X$ (Wooldridge (2016), pp. 462-464). In the simple regression case, a variable $z$ has to fulfill two crucial assumptions in order to be a valid instrument for an endogenous variable $x$:

- **IV1 (Instrument Exogeneity):** The instrumental variable $z$ is not correlated with the error term of the regression model: $\mathbb{E}(\varepsilon_i|z) = 0$ 
- **IV2 (Instrument Relevance):** $z$ has to be correlated with the endogenous variable $x$: $Cov(z, x) \not= 0$.


#< quiz "Instrument Assumptions"
question: Which of the two assumptions is the most problematic in the sense that we cannot ultimately verify whether it holds?
sc:
    - IV1 (Instrument Exogeneity)*
    - IV2 (Instrument Relevance)
    
success: Great!.
failure: Try again.
#>


**IV1** assumes that the instrument is exogenous, i.e. uncorrelated with the error term. We already discussed in previous exercises that we cannot ultimately test this conjecture. Hence, justifying an instrument also involves economic reasoning and plausible argumentation. Confirming **IV2** is straightforward, as we will see below.

The approach we employ in the following is the *Two Stage Least Squares (2SLS)* estimator (Kennedy (2008), pp. 151-152). This method can deal with multiple instruments, exogenous and endogenous variables at once. As the name suggests, we obtain the 2SLS estimates by performing two steps. In the first stage, we regress the independent variables $X$ on the (presumed) exogenous variables in $X$ and the instruments, collected in matrix $Z$, to obtain the fitted values $\hat{X}$:
$$
\hat{X} = Z(Z'Z)^{-1}Z'y. \tag{7.0}
$$
Note that the exogenous explanatory variables serve as their own instruments. The fitted values of the endogenous variables $\hat{X}$ are thus a linear combination of the exogenous variables and instruments. In the second stage, we replace $X$ with $\hat{X}$ and obtain the OLS estimates of the population coefficients $\hat{\beta}^{IV}$:
$$
\hat{\beta}^{IV} = (\hat{X}'\hat{X})^{-1} \hat{X}'y \\\
= [X'Z(Z'Z)^{-1}Z'X]^{-1}X'Z(Z'Z)^{-1}Z'y. \tag{7.1}
$$
Hence, IV estimation relies on exogenous variation in one or more instruments to isolate part of the variation in the endogenous variable(s) that is "quasi-experimental", i.e. uncorrelated with the error term (Angrist and Pischke (2009), p. 121). This variation is then used to estimate the population coefficients of interest. IV methods such as 2SLS are intended to provide better estimates for causal effects in the presence of endogenous variables. Usually $\hat{\beta}^{IV}$ is not unbiased, so we rely on the large sample properties of the estimator (consistency). The critical task when working with IV methods is to come up with valid IVs. In the next section, we discuss the instruments used in the article and assess whether they seem valid.

### b) The Instruments

The authors propose **political variables** as instruments for the CPI. These variables measure the programmatic position of incumbent political parties towards certain economic and regulatory topics. They were constructed by screening party programs for favorable mentions. To account for government coalitions, the variable's values were averaged using party votes as weights.

**Task:** Load the `dat2.rds` again. Show the upper rows of the instruments using `select()` and `head()`. The code is already given below; just click `check`.
```{r}
#< task_notest
library(dplyr)
dat = readRDS("dat2.rds")

# Show instruments
dat %>%
  select(id, year, per108, per403, per404, per505) %>%
  head(n = 5)
#>
```

The variables starting with `per` are the four IV candidates we consider in the following. The first instrument, `per108`, measures an endorsing attitude towards the European Union and the desirability of expanding its competency. The variable is zero for non-European countries. A desire for market regulation is measured by the variable `per403`. This includes actions against monopolies and trusts, defending consumer rights and encouraging competition. The variable `per404` relates to statements favoring long standing economic planning. A desire for welfare state limitations is comprised in the fourth IV candidate, `per505`. This covers the rejection of introducing, expanding or maintaining social services or social securities. For a more detailed explanation of the instruments, refer to the online appendix of Buccirossi et al. (2013), pp. 15-16. We can view the aforementioned variables as proxies for **political attitudes (PA)** of incumbent policy makers towards certain economic topics. Are these variables viable instruments for the CPI? To think about this question, consider the causal graph below.

#< preknit
```{r echo=FALSE, warning=FALSE, results = 'asis', out.width=600, dpi=700}
suppressPackageStartupMessages(library(knitr))
include_graphics("C:/Users/ThinkPad User/Documents/CompetitionPolicy/Problem Set/iv.jpg")
```
#>

This graph illustrates the presumed relationships among the variables, if $PA$ was a valid IV. Firstly, we assume that $PA$ has a causal effect on $CPI$. That is, involvement in competition policy can be partially explained by the ideological position of the government. This seems like a plausible assumption. For example, we could argue that pro-market attitudes of policy makers should lead to more involvement in competition policy (as we did in exercise 4). We can test whether the instruments are associated with the CPI in the first stage regression. At least one of the instruments has to be significantly related with the CPI in order for **IV2 (Instrument Relevance)** to hold.

The other assumption is more challenging to assess. Essential for the IV approach to work properly is that the effect of the instrument on the outcome goes **solely** through its effect on the endogenous variable (Angrist and Pischke (2009), p. 127). Thus, in our case, after conditioning on the set of covariates $X$, there must not be any link between $PA$ and $TFP$. The causal graph below shows the situation if this were the case.

#< preknit
```{r echo=FALSE, warning=FALSE, results = 'asis', out.width=600, dpi=700}
suppressPackageStartupMessages(library(knitr))
include_graphics("C:/Users/ThinkPad User/Documents/CompetitionPolicy/Problem Set/noniv.jpg")
```
#>

If there is, for instance, a relevant omitted variable that is influenced by $PA$, then $PA$ is also correlated with the error term $\varepsilon$. Assumption **IV1 (Instrument Exogeneity)** would be violated. Unfortunately, we cannot ultimately test this (although there are statistical tests). If at least one instrument is endogenous, consistent estimation of the causal effect of competition policy fails. Whether political attitudes are uncorrelated with other omitted factors is difficult to judge. There might be other policies left that have an impact on productivity and are also correlated with competition policy (similar to PMR in exercise 4). We elaborate on this issue in the next exercise.

We first try to anticipate the link of the political variables with the CPI. It is important to assess the relationships between the instruments and endogenous variables when working with IV methods. For instance, if theoretical considerations suggest that an instrument should be positively correlated with the endogenous variable, but we were to find a negative coefficient in the first stage, this would raise severe concerns about the validity of the instrument (Wooldridge (2016), p. 464).


#< quiz "instrument per108"
question: If per108 measures a positive attitude towards the EU, what sign do you expect for its coefficient in the first stage regression?

sc:
    - Positive*
    - Negative
    - Zero
    
success: Great!
failure: Try again.
#>

#< quiz "instrument per403"
question: If per403 measures the desire for market regulation, what sign do you expect for its coefficient in the first stage regression?

sc:
    - Positive
    - Negative*
    - Zero
    
success: Great!
failure: Try again.
#>

#< quiz "instrument per404"
question: If per404 measures a favorable attitude towards long-term economic planning, what sign do you expect for its coefficient in the first stage regression?

sc:
    - Positive
    - Negative*
    - Zero
    
success: Great!
failure: Try again.
#>

#< quiz "instrument per505"
question: If per505 measures the desire for welfare state limitations, what sign do you expect for its coefficient in the first stage regression?

sc:
    - Positive*
    - Negative
    - Zero
    
success: Great!
failure: Try again.
#>


The EU has a history of acting in favor of product market competition and competition policy, hence we would expect `cpi` and `per108` to be positively correlated. Governments that have a proclivity towards product market regulation (`per403`) should be less involved in competition policy (recall the negative association between the CPI and PMR in exercise 4). A desire for long-term economic planning (`per404`) should be inconsistent with strong involvement in competition policy (competition policy reflects an openness to allocate decisions to markets, not towards centralized state planning). Intentions to limit the services of the welfare state (`per505`) should probably be positively related to competition policy. In the next task, we test these conjectures by running the first stage regression.

**Task:** Confirm that the instruments are associated with the CPI. To do so, regress `cpi` on `per108`, `per403`, `per404`, `per505` and the other controls. Store the results in `fstage` and show them in a `stargazer()` table. I already gave you part of the solution; just complete/replace the code below and click `check`.
```{r results = 'asis', warning = FALSE}
#< fill_in
library(lfe)
# Insert the correct expressions for the placeholders ___ below.
fstage = felm(cpi ~ ___ + tfpleader + tecgap + trend + trade + pmr | id + year | 0 | 0, data = dat)

# Organize results in stargazer table
library(stargazer)
stargazer(fstage, 
          type = "html",
          title = "Table 7.0: First Stage Regression",
          style = "aer", 
          digits = 3,
          column.labels = c("First Stage"),
          omit = c("tfpleader", "tecgap", "trade", "pmr", "trend"),
          add.lines = list(c("Full Set of Covariates", rep("Yes", 1))))
#>
library(lfe)

fstage = felm(cpi ~ per108 + per403 + per404 + per505 + tfpleader + tecgap + trend + trade + pmr | id + year | 0 | 0, data = dat)

library(stargazer)
stargazer(fstage, 
          type = "html",
          title = "Table 7.0: First Stage Regression",
          style = "aer", 
          digits = 3,
          column.labels = c("First Stage"),
          omit = c("tfpleader", "tecgap", "trade", "pmr", "trend"),
          add.lines = list(c("Full Set of Covariates", rep("Yes", 1))))
#< hint
cat("Just replace the placeholder with the four instruments.")
#>
```


I omitted the covariates in the table above. Looking at the results, we see that the first instrument `per108` is highly significant in the first stage regression. As expected, the positive coefficient indicates that a favorable attitude towards the EU is positively associated with higher levels of competition policy. The instrument in the second row, `per403`, is also highly significant and has a negative sign. This also conforms to our expectations: A desire for market regulation is negatively associated with involvement in competition policy. The third instrument `per404` is significant and has a positive sign. The sign does not necessarily conform to our expectations. The desire for long-term economic planning does not seem to be consistent with involvement in competition policy. The idea behind the latter is to delegate decisions to markets, not centralized government planning. Unfortunately, the authors do not mention this issue in their discussion on the first stage regression (see Buccirossi et al. (2013), pp. 1131-1132). The fourth instrument `per505`, measuring the desire for welfare state limitations, is not significant in the first stage. The sing of the coefficient is, as anticipated, positive. Overall, the first stage suggests that the political variables seem to work properly as instruments for the CPI (with a potential caveat regarding `per404`).

### c) IV Regression

Now that we have discussed the instruments, we are ready to obtain the IV estimates for our main specification. We do not manually perform the two-stage procedure here. The `felm()` function performs 2SLS with a single function call. 

#< info "The felm() function and 2SLS"
The `felm()` function allows to perform 2SLS using the third part of the first argument. The endogenous variables are specified and separated from the instruments using `~`. The call below regresses the variable `y` on the endogenous variable `e1` and the exogenous variable `x2`. The second part of the function call specifies the variables `f1` and `f2` as fixed effects. The endogenous variable `e1` is instrumented by `z1` and `z2`. Note that the formula in the third part has to be put in brackets.  
```{r eval=FALSE}
reg = felm(y ~ e1 + x2 | f1 + f2 | (e1 ~ z1 + z2) | cluster_var, data = dat)
```
To learn more about the `felm()` function check out the documentation of the `lfe` package.
https://cran.r-project.org/web/packages/lfe/index.html
#>

**Task:** Perform 2SLS using the `felm()` function. To do so, instrument `cpi` with `per108`, `per403`, `per404` and `per505`. Store the results in `iv`. Perform the regression from exercise 5 using OLS again and compare both results in a `stargazer` table. I already gave you part of the solution; just complete/replace the code below and click `check`.
```{r results = 'asis', warning = FALSE}
#< fill_in
# Insert the correct expressions for the placeholders ___ below.
iv = felm(tfp ~ tfpleader + tecgap + trade + pmr + trend | id + year |
                ___, data = dat)

# OLS
ols = felm(tfp ~ cpi + tfpleader + tecgap + trend + trade + pmr | 
                 id + year | 0 | country, data = dat)

# Organize results in stargazer table
stargazer(iv, ols, 
          type = "html",
          title = "Table 7.1: IV Regression",
          style = "aer", 
          digits = 3,
          column.labels = c("2SLS", "OLS"),
          add.lines = list(c("Country-Industry Fixed Effects",
                             rep("Yes", 2)),
                           c("Time Fixed Effects", rep("Yes", 2))))
#>
iv = felm(tfp ~ tfpleader + tecgap + trade + pmr + trend | id + year |
                (cpi ~ per108 + per403 + per404 + per505), data = dat)
          
ols = felm(tfp ~ cpi + tfpleader + tecgap + trend + trade + pmr | 
                 id + year | 0 | country, data = dat)

stargazer(iv, ols, 
          type = "html",
          title = "Table 7.1: IV Regression",
          style = "aer", 
          digits = 3,
          column.labels = c("2SLS", "OLS"),
          add.lines = list(c("Country-Industry Fixed Effects",
                             rep("Yes", 2)),
                           c("Time Fixed Effects", rep("Yes", 2))))
#< hint
cat("If you struggle with the syntax of felm(), refer to the info box above.")
#>
```


#< award "Regression Master"
Congratulations! You know now how to run instrumental variable regressions using 2SLS.
#>

Note that the coefficient for `cpi` is displayed in the last row (`cpi(fit)`). The estimated coefficient $\hat\beta^{IV}$ is $0.222$, which is more than twice the size of the OLS estimate and significant at the $5\%$ level. This provides further confidence that there is in fact a positive effect of competition policy on productivity growth.


#< quiz "sizecse"
question: The difference between both estimates appears to be relatively large. Is this difference likely to be statistically significant?

sc:
    - Yes.
    - No.*
    
success: Great!
failure: Try again.
#>


Note that the CSE for $\hat\beta^{IV}$ is more than four times larger than the CSE of the OLS estimate. Thus, a $95\%$ confidence interval actually includes the OLS estimate. The difference between the two coefficients should not be overstated. This shows that IV approaches usually come at a cost of much larger standard errors (Kennedy (2008), p. 141). The amount of variation used in an endogenous variable to estimate the slope is much smaller, which results in less precise estimates. Before concluding, we look at some test statistics that are useful when working with IV methods. We can obtain them easily by using the `ivreg()` function of the `AER` package.

#< info "The ivreg() function of the AER package"
The `ivreg()` function is part of the `AER` package and allows performing 2SLS. The syntax is similar but not identical to the `felm()` function. The advantage of `ivreg()` is that it automatically computes diagnostic tests for IV models. These statistics can be obtained with the `summary()` function, setting the argument `diagnostics = TRUE`. If you want to learn more about the `ivreg()` function, check out the package documentation. 
https://cran.r-project.org/web/packages/AER/index.html
#>

**Task:** Run the instrumental variable regression again using the `ivreg()` function. Show the computed diagnostics with `summary()`. The code is already given below; just click `check`.
```{r}
#< task_notest
# IV regression
library(AER)
iv_diag = ivreg(tfp ~ cpi + tfpleader + tecgap + trade + pmr + trend + id + year | tfpleader + tecgap + trade + pmr + trend + id + year + per108 + per403 + per404 + per505, data = dat)

# Obtain diagnostics
print(summary(iv_diag, diagnostics = TRUE)[12], digits = 3)
#>
```

In the following discussion, we focus on the basic idea of each test and leave out a formal treatment. Refer to the references for more detailed information. The first statistic reported is an *F-test* for joint significance of the instruments in the first stage regression (Wooldridge (2016), pp. 476-477). This test statistic is important for two reasons. Firstly, none of the instruments might be individually significant in the first stage (because of high correlation among the instruments leading to inflated SEs). However, it is still possible that at least one instrument is partially correlated with the endogenous variable. Thus, a significant F-test allows us to reject the null hypothesis that none of the instruments is relevant. Secondly, it has been shown that a "weak" association between the endogenous variable and the instruments leads to very poor properties of the IV estimator (Kennedy (2008), p. 145). Although consistent, the bias of the IV estimator can be so severe that even in large samples we do not obtain reasonably precise estimates. A rule of thumb in the literature suggests that the F-statistic should be at least $10$ to not run into this issue. The reported test statistic of $51$ is well above this threshold and allows us to conclude that the instruments show sufficient partial correlation with the CPI. 

The second row shows the test statistic of a *Wu-Hausman test* for endogeneity (Greene (2003), pp. 80-83). The original idea of this test is to check whether there is a significant difference between the OLS estimate $\hat\beta^{OLS}$ and the IV estimate $\hat\beta^{IV}$. If the CPI were indeed endogenous, the OLS estimate should be significantly different from the 2SLS estimate (because the 2SLS estimator is consistent, OLS is not). The Wu-Hausman test statistic has a p-value of $0.21$ - we cannot reject the null-hypothesis of exogeneity of the CPI. In other words, there is no evidence for endogeneity of the CPI. Recall that we have stated earlier that we cannot ultimately test whether there is correlation with the error term, so this test should not be overstated. For example, both 2SLS and OLS might be biased in the same direction, leading to an insignificant difference between both estimates.

The third row shows the results of a *Sargan test* for *overidentification restrictions* (Kennedy (2008), p. 154-155). This test exploits the fact that we have more instruments than actually needed. In such a case, we could obtain multiple IV estimates by using subsets of the available instruments to estimate the parameters. If all instruments are indeed exogenous, all obtained IV estimates should be quiet similar. The Sargan test is implicitly performing this comparison. The p-value here is $0.45$ - we cannot reject the null hypothesis that the instruments are exogenous. In other words, there is no evidence that any instrument is invalid.

To sum up, using political variables as instruments for the CPI, we still find a practically and statistically significant positive effect of competition policy on productivity growth. This provides further evidence that the found link can be interpreted in a causal way. Although consistent, the 2SLS estimator is not efficient in the absence of endogeneity (Wooldridge (2016), pp. 466-467). Therefore, since the Wu-Hausman test was insignificant, we consider OLS the preferred estimator. In exercise 8, we continue to discuss the problem of omitted variables and employ the last step in our identification strategy.

## Exercise 8 -- Remaining Issues and Heterogeneous Effects of Competition Policy

### a) Remaining Omitted Variables

In previous exercises, we aimed at estimating the causal effect of competition policy on productivity growth. The results we obtained suggest the effectiveness of such policy regimes. However, despite the use of control variables, fixed effects and instrumental variables, we cannot ultimately confirm that the found link is of a causal nature (we would have to conduct a solid randomized experiment to do so, see discussion in exercise 3). The authors argue that the primary remaining issue might be the potential effects of other omitted policies: It is possible that productivity-enhancing policies exist, that are also correlated with the CPI. The causal graph below illustrates this issue. 

#< preknit
```{r echo=FALSE, warning=FALSE, results = 'asis', out.width=600, dpi=700}
suppressPackageStartupMessages(library(knitr))
include_graphics("C:/Users/ThinkPad User/Documents/CompetitionPolicy/Problem Set/op.jpg")
```
#>

Even after conditioning on a set of controls $X$, there are still unknown or unobserved policies $P$ left that have a causal effect on $TFP$. If not controlled for, $P$ is left in the error term. Since $P$ is correlated with $CPI$, we face the consequences of endogeneity: biased and inconsistent estimation.

It is conceivable that our estimate is driven by other productivity-enhancing policies. In exercise 4, we demonstrated the problem of OVB using a variable measuring the degree of product market regulation (PMR). PMR can be viewed as a proxy for omitted policies that determine the regulatory environment with respect to product markets. The issue of OVB might as well extend to other public policies, such as labor market policies or fiscal policies. The IV approach could also be susceptible to the existence of such policies. In exercise 7, we used political variables as instruments for the CPI. We saw that some attitudes of policy makers are related to their involvement in competition policy. The same mechanism that creates this link might as well apply to other public policies. The causal graph below illustrates this issue.

#< preknit
```{r echo=FALSE, warning=FALSE, results = 'asis', out.width=600, dpi=700}
suppressPackageStartupMessages(library(knitr))
include_graphics("C:/Users/ThinkPad User/Documents/CompetitionPolicy/Problem Set/opiv.jpg")
```
#>

Governments advocating for competitive markets might also be engaged in other policies, such as the deregulation of labor markets. The instruments could be endogenous if these policies also have an effect on productivity. Both OLS estimator and IV estimator could therefore be biased upwards. Is there a way to demonstrate that the estimated effect is unlikely to be driven by other omitted policies? For the remainder of this exercise, we employ an informal approach to answer this question.

### b) Industry-specific Heterogeneous Effects of Competition Policy

A way to gain more confidence that a relationship is of a causal nature, is to study an effect within subgroups (Morgan and Winship (2015), p. 360). If we have justifiable reasons to believe that an effect **varies in predictable ways** across subgroups, detecting such a pattern in the data can provide further evidence that the relationship is in fact causal. In our case, the idea is to state situations where we expect competition policy to have a differential effect on productivity growth. Finding such *heterogeneous effects* of competition policy might strengthen our confidence that the link is causal, if other omitted policies are unlikely to induce such a pattern of heterogeneity.

Until this point, we have treated the slope parameter of the CPI $\beta$ as either a constant or some kind of an average. If we assume that this slope differs across some subgroups $g$, then there are multiple slope parameters $\beta_{g1}, \beta_{g2}, ...$ and so on. We call such a pattern *heterogeneous effects* (Angrist and Pischke (2009), Chapter 3.3). In their article, the authors investigate two types of heterogeneity. We analyze one type in the tasks ahead and briefly discuss the other findings thereafter.

In the following, we focus on potential **industry-specific** heterogeneous effects of competition policy. We exploit the fact that the data contain manufacturing industries (food products, machinery, etc.), as well as service industries (telecommunication, financial intermediation, etc.). Services are typically contingent upon relatively strict sector-specific regulations (think of regulation of financial markets, telecommunications regulation, etc.). That is to say, the competitive structure in service industries is more determined by sectoral rules and authorities compared to manufacturing industries, which leaves less room for the impact of competition policy. Thus, competition policy should be less effective in service-related industries. 

We can test this hypothesis by allowing competition policy to have a distinct effect for manufacturing and services industries, respectively. To do so, we modify our regression model using *interaction terms* (Wooldridge (2016), pp. 177-179). Interaction terms allow a variable to have a differential effect based on the values of other variables. We study the simple case where we interact the CPI with two indicator variables. Instead of estimating one slope parameter $\hat{\beta}$, we obtain two slope parameter estimates $\hat{\beta}_1$ and $\hat{\beta}_2$. We can modify our specification from exercise 5 in the following way:
$$
\Delta TFP_{i,j,t} = \beta_1 CPI_{i, t-1} \cdot Manufacturing_{i,j} + \beta_2 CPI_{i, t-1} \cdot Service_{i, j} \\ + ... + \psi_{i,j} + \phi_{t} + u_{i,j,t} \tag{8.0}
$$
where I omitted the already known covariates. The variables $Manufacturing_{i,j}$ and $Service_{i,j}$ are dummy variables that indicate whether a country-industry $i,j$ is a manufacturing or a service industry, respectively. As a first step, we have to do some preparation by computing the two dummies. To differentiate between the two sectors, we can use the variable `isic` in the data.

**Task:** Load the file `dat2.rds` again and assign it to `dat`. Use the `class()` function and the `range()` function on the variable `isic` to show its properties. The code is already given below; just click `check`.  
```{r}
#< task_notest
dat = readRDS("dat2.rds")

class(dat$isic)
range(dat$isic)
#>
```

The variable `isic` is a modified version of the already known `industry` variable. It labels each industry using an integer value from $1$ to $22$. We can use this variable to differentiate between manufacturing and services: All industries below $16$ are manufacturing-related; industries above $15$ are service-related. Using this information, we can compute dummy variables by combining the `if_else()` function with the already known `mutate()` function.

#< info "The if_else() function and dummy variables"
The `if_else()` function of the `dplyr` package allows to test a condition and generate an output depending on whether the condition evaluates to `TRUE` or `FALSE`. The function takes on the following generic form:
```{r eval=FALSE}
if_else(conditition, true, false) 
```
The first argument specifies a condition using logical operators such as `==`, `>`, `<`, and so on. The second and third argument define the function's output if the condition evaluates to `TRUE` or `FALSE`, respectively. Dummy variables can easily be computed by combining `mutate()` and `if_else()`:
```{r eval=FALSE}
dat = mutate(dat, dummy = if_else(condition, 1, 0))
```
Using a data frame `dat`, this code fragment generates an indicator variable `dummy` that takes on a value of `1` if a condition is met, and `0` otherwise. For more information on `if_else()`, check out the function documentation.
https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/if_else
#>

**Task:** Compute two dummies named `manufacturing` and `service` using `mutate()` and `if_else()`. Manufacturing industries take on values for `isic` below `16`, whereas service industries take on values above `15`. I already gave you part of the solution; just complete/replace the function call below and click `check`.
```{r}
#< fill_in
library(dplyr)
# Insert the correct expressions for the placeholders ___ below.
dat = mutate(dat, manufacturing = if_else(___),
                  service = if_else(___))
#>
library(dplyr)
dat = mutate(dat, manufacturing = if_else(isic < 16, 1, 0),
                  service = if_else(isic > 15, 1, 0))
#< hint
cat("If you struggle with the syntax of if_else() refer to the info box above.")
#>
```

#< award "dplyr Master"
Congratulations! You successfully combined mutate() and if_else() to compute dummy variables.
#>

Now that we have created the dummies, we can interact them with the CPI. An interaction term is simply the product of two variables (Wooldridge (2016), pp. 177-179 and pp. 218-219). There are two ways to create interaction terms in R. We can either specify the interaction terms directly in the function call to `felm()`, or compute a new set of variables. Since the former needs some further explanation, we use the latter approach here (refer to the documentation of the *AsIs* function `I()` for the former approach).

**Task:** Compute two interaction terms using the `mutate()` function. The first interaction term, `cpi_manufacturing`, is obtained by multiplying `cpi` with `manufacturing`. The second interaction term, `cpi_service`, is obtained by multiplying `cpi` with the `service`. Show the upper entries of the generated interaction terms using `head()`. I already gave you part of the solution; just complete/replace the code below and click `check`.
```{r}
#< fill_in
# Insert the correct expressions for the placeholders ___ below.
dat = mutate(dat, cpi_manufacturing = ___,
                  cpi_service = ___)

# Display interaction terms
dat %>% 
  select(id, year, cpi_manufacturing, cpi_service) %>%
  head(n = 5)
#>
dat = mutate(dat, cpi_manufacturing = cpi*manufacturing,
                  cpi_service = cpi*service)

dat %>% 
  select(id, year, cpi_manufacturing, cpi_service) %>%
  head(n = 5)
```

The interaction terms are easy to interpret here: We multiplied the CPI by either $1$ or $0$. Thus, `cpi_manufacturing` takes on the actual value of the CPI if an industry is a manufacturing-related, and zero otherwise. The exact opposite is true for the variable `cpi_service`. By including the interaction terms in our regression model, we allow for heterogeneous effects of competition policy: The coefficient of `cpi_manufacturing` quantifies the effect for manufacturing industries, whereas the coefficient of `cpi_service` quantifies the effect for service industries. In the next task, we obtain the OLS estimates.

**Task:** Modify the main regression from exercise 5 by replacing `cpi` with `cpi_manufacturing + cpi_service` and store the results in `interact`. Run the main regression again and show both results for comparison in a `stargazer `table. I already gave you part of the solution; just complete/replace the function call below and click `check`.
```{r results = 'asis', warning = FALSE}
#< fill_in
library(lfe)
# Insert the correct expressions for the placeholders ___ below.
interact = felm(tfp ~ ___ + tfpleader + tecgap + 
                      trend + trade + pmr | id + year | 0 | country, 
                      data = dat) 

# Main Regression from Exercise 5
main = felm(tfp ~ cpi + tfpleader + tecgap + trend + trade + pmr |
                  id + year | 0 | country, data = dat) 

# Organize results in stargazer table
library(stargazer)
stargazer(interact, main, 
          type = "html",
          title = "Table 8.0: Interaction Regression",
          style = "aer", 
          digits = 3,
          column.labels = c("Interaction Regression", "Main Regression"),
          omit = c("tfpleader", "tecgap", "trend", "trade", "pmr"),
          add.lines = list(c("Full Set of Covariates", rep("Yes", 2)),
                           c("Country-Industry Fixed Effects", 
                             rep("Yes", 2)),
                           c("Time Fixed Effects", rep("Yes", 2))))
#>
library(lfe)
interact = felm(tfp ~ cpi_manufacturing + cpi_service + tfpleader + tecgap + 
                      trend + trade + pmr | id + year | 0 | country, 
                      data = dat) 

main = felm(tfp ~ cpi + tfpleader + tecgap + trend + trade + pmr |
                  id + year | 0 | country, data = dat) 

library(stargazer)
stargazer(interact, main, 
          type = "html",
          title = "Table 8.0: Interaction Regression",
          style = "aer", 
          digits = 3,
          column.labels = c("Interaction Regression", "Main Regression"),
          omit = c("tfpleader", "tecgap", "trend", "trade", "pmr"),
          add.lines = list(c("Full Set of Covariates", rep("Yes", 2)),
                           c("Country-Industry Fixed Effects", 
                             rep("Yes", 2)),
                           c("Time Fixed Effects", rep("Yes", 2))))
```


#< quiz "interact"
question: Are the data consistent with our hypothesis that competition policy is less effective in service industries?

sc:
    - Yes.*
    - No.
    - There is no difference.
    
success: Great!
failure: Try again.
#>


#< award "Regression Expert"
Congratulations! You know now how to use interaction terms in regression models to allow for a difference in slopes.
#>

I omitted the covariates from the table above. In the first column, we see that the results do indeed support our hypothesis: Competition policy seems to be substantially less effective in service industries. The estimated coefficient for manufacturing industries is $0.13$, whereas for service industries it merely is $0.017$. Although we still obtain a positive estimate for services, the effect is not significantly different from zero. Column 2 shows that the coefficient of the main regression lies, unsurprisingly, between both interaction coefficients. Under heterogeneous effects, the regression coefficient is a variance-weighted average of the individual effects (Angrist and Pischke (2009), pp. 73-76).

To infer in a statistically sound manner that the slopes are not equal, we can conduct a *t-test*, with $H_0: \hat{\beta}_1 = \hat{\beta}_2$ against $H_1: \hat{\beta}_1 \ne \hat{\beta}_2$. The test statistic is:
$$
t = \frac{\hat{\beta}_1 - \hat{\beta}_2}{se(\hat{\beta}_1 - \hat{\beta}_2)} \tag{8.1}
$$
where $se$ is the standard error of the difference (Wooldridge (2016), pp. 124-126). There is more than one way to perform this test. We could extract the relevant elements from the variance-covariance matrix and manually compute the t-statistic. Here, however, a simple trick makes this much easier. Instead of estimating $\hat{\beta}_1$ and $\hat{\beta}_2$, we estimate $\hat{\beta}_1$ and $\delta = \hat{\beta}_1-\hat{\beta}_2$, where $\delta$ is the difference in slopes. In fact, we have everything we need already available.
 
 **Task:** Rerun the interaction regression from the previous task, but replace `cpi_manufacturing` with `cpi`. Compare the results to the former regression. The code is already given below; just click `check`.
```{r results = 'asis', warning = FALSE}
#< task_notest
diff = felm(tfp ~ cpi + cpi_service + tfpleader + tecgap + trend + trade + pmr |
                  id + year | 0 | country, data = dat)

stargazer(diff, interact, 
          type = "html",
          title = "Table 8.1: Rewritten Interaction Regression",
          style = "aer", 
          digits = 3,
          column.labels = c("Rewritten", "Interaction Regression"),
          omit = c("tfpleader", "tecgap", "trend", "trade", "pmr"),
          add.lines = list(c("Full Set of Covariates", rep("Yes", 2)),
                           c("Country-Industry Fixed Effects", 
                             rep("Yes", 2)),
                           c("Time Fixed Effects", rep("Yes", 2))))
#>
```


We can see that the coefficient of `cpi` matches the coefficient of `cpi_manufacturing`, and the coefficient of `cpi_service` is the exact difference between both interaction regression coefficients. For more information on how rewriting a regression model can sometimes be used to test specific hypotheses in a simple way, refer to Wooldridge (2016), p. 126. The difference $\delta$ is significant at the $10\%$ level. The t-statistic is $-1.994$ with a p-value of $0.071$. An issue here is that we have a lot less service-industries than manufacturing industries, which leads to a relatively large standard error. This demonstrates that even if a difference in effects seems practically large, we also should conduct a formal test. Finally, we visualize the difference in slopes using regression anatomy, analogue to exercise 5.

**Task:** Perform regression anatomy for both interaction terms in table 8. Show a scatter plot of the bivariate regressions and include the slopes. The code is already given below; just click `check`.
```{r out.width=700, dpi=700}
#< task_notest
# Regression anatomy
manu_reg = felm(cpi_manufacturing ~ cpi_service + tfpleader + tecgap + trend + trade + pmr + id + year, data = dat)
serv_reg = felm(cpi_service ~ cpi_manufacturing + tfpleader + tecgap + trend + trade + pmr + id + year, data = dat)
dat = mutate(dat, manu_res = resid(manu_reg),
                  serv_res = resid(serv_reg))

library(ggplot2)
library(gridExtra)
# Scatter plot residualized cpi_manufacturing
m = ggplot(data = dat, aes(x = manu_res, y = tfp)) +
  geom_point(colour = "#6794a7", shape = 21, size = 1.0) +
  geom_smooth(method = "lm", formula = y ~ x, 
              se = FALSE, colour = "#d23e4e") +
  annotate("text", x = -0.09, y = 0.25, label = "italic(beta) == 0.13", 
           parse = TRUE, colour = "#d23e4e", size = 4) +
  theme_blue(base_size = 10) +
  labs(title = "Manufacturing", caption = "",
       x = "Residualized cpi_manufacturing", y = "TFP")

# Scatter plot residualized cpi_service
s = ggplot(data = dat, aes(x = serv_res, y = tfp)) +
  geom_point(colour = "#6794a7", shape = 21, size = 1.0) +
  geom_smooth(method = "lm", formula = y ~ x,
              se = FALSE, colour = "#d23e4e") +
  annotate("text", x = -0.09, y = 0.25, label = "italic(beta) == 0.017", 
           parse = TRUE, colour = "#d23e4e", size = 4) +
  theme_blue(base_size = 10) +
  labs(title = "Services", x = "Residualized cpi_service", y = "",
       caption = "Figure 8.0: Regression Anatomy Interactions")
# Combine plots
grid.arrange(m, s, nrow = 1, ncol = 2)
#>
```

Even by looking at the raw data points, we can spot a positive relationship in the left panel. For service industries, we cannot detect such a pattern by eyeballing. How does this result make us more confident that the relationship between competition policy and productivity is of a causal nature? The fact that the observed heterogeneity is consistent with our expectations on how the effects of competition policy should behave, is reassuring. If the driving force behind our results were other omitted policies, these policies had to show the same kind of interaction pattern as competition policy. The authors claim that other public policies, such as labor market policies or fiscal policies, are unlikely to induce such a pattern of heterogeneity. Obviously, this rationale relies solely on introspection and economic reasoning and is therefore not something we can ultimately confirm. In the next section, we briefly review the other findings of the article.

### c) Institutional Heterogeneity

The second type of heterogeneity the authors investigate are potential complementarities between competition policy and the **institutional environment**. The idea is that competition policy should be more effective, if the legal system and judicial institutions are of a better quality. For instance, courts are heavily involved in the enforcement of competition law. A potent legal system should therefore give competition policy more "bite". 

To investigate this hypothesis, the authors interact the CPI with several variables that measure specific aspects of the judicial system. To give a concrete example, one of these variables scores the enforcement costs of contracts across countries. This indicator can be understood as a proxy for the effectiveness of a legal system and its institutions. For instance, if trials take a long time, contract enforcement costs will be high. An interaction regression reveals that competition policy seems to be substantially more effective when enforcement costs are low. For countries with low, medium and high enforcement costs, the estimated coefficients are $0.24$, $0.11$ and $0.09$, respectively. A similar pattern is also visible for other institutional indicators used. For the complete results, refer to table 6 in Buccirossi et al. (2013) p. 1333.

These findings point at substantial heterogeneous effects of competition policy with respect to the institutional and legal environment. This pattern of heterogeneity is consistent with our expectations: Without a functioning legal system, competition policy cannot be particularly effective. Other omitted public policies, such as labor market policies or fiscal policies, do not necessarily interact in such a way with the quality of the legal system. Based on these results, we can more confidently argue that our estimate is probably not driven by other omitted productivity-enhancing policies. The exploitation of heterogeneous effects provides further support that the link between competition policy and productivity can be interpreted in a causal way.

### d) Preliminary Conclusion

Over the course of the first part of this problem set, we aimed at estimating the causal effect of competition policy on productivity growth. We laid out the conceptual problem we have in policy evaluation: performing solid randomized experiments is usually infeasible. Having to rely on observational data leaves us with a set of problems. We tried to address the accompanying issues, in particular omitted variables, by employing econometric methods such as multiple linear regression, fixed effects and instrumental variable estimation. The results of our analysis provide evidence that well-designed competition policy enhances productivity growth. We will add a few words on overall welfare considerations of such policy interventions in the conclusion to this problem set. The upcoming four exercises are dedicated to an economic policy which is often perceived as **contradicting** the goals of competition policy.

## Exercise 9 -- Introduction to Industrial Policy

In this second part of the problem set, we investigate another type of economic intervention: **industrial policy**. Industrial policies cover a wide range of measurements - among the most salient ones are subsidies, tax holidays and tariffs (Harrison and Rodr√≠guez-Clare (2009), p. 4041). Whereas competition policy aims at preserving or enhancing competition, industrial policy is often criticized precisely for obstructing competitive principles (Aiginger (2014), p. 8). By design, many of these policies have a substantial impact on the structure of an economy by favoring some firms and industries over others, possibly in a non-transparent or arbitrary fashion. This result in the manifestation of inefficient structures and distorted market outcomes - according to its critics (Stiglitz et al. (2013), pp. 5-6). So why are governments involved in industrial policy? 

While there may be many motivations, the most prevalent justification in the economic literature is the *infant-industry argument* (Aghion et al. (2015), pp. 1-2). Especially after the Second World War, developing countries tried to catch up economically using a set of industrial policies. Typically, established industries in more advanced countries had an efficiency advantage acquired through learning-by-doing, economies of scale, and so on. Many Infant-industries were not able to compete on global markets because of initially high production cost. Governments tried to shield these industries from foreign pressure by using policy instruments such as subsidies and tariffs. Once the infant-industries had "grown up", that is to say, became competitive, protective measurements were loosened. Of particular interest for such interventions were industries that exhibit economy-wide *externalities*. For instance, Greenwald and Stiglitz (2006) show that tariff protection may be warranted if it promotes the formation of a modern sector, such as advanced manufacturing. The knowledge and technology development in such a sector may "spillover" to a traditional sector, like agriculture, and increase its productivity. Then the short-term efficiency losses through tariff protection might be outweighed by the efficiency gains through economy-wide learning.

The empirical literature shows mixed results when it comes to the effectiveness of industrial policy (see Harrison and Rodr√≠guez-Clare (2009), pp. 4062-4070 for a review). While there are cases where protection has helped establishing competitive industries, there are also counterexamples. The cross-country and cross-industry evidence is also not conclusive. A fundamental problem in the assessment is that industrial policies are frequently imposed for other reasons than mere infant-industry protection. For instance, the purpose of tariffs is also to generate revenue, and subsidies may benefit special interest groups ("rent-seeking"). Isolating the policies intended for infant-industry protection is therefore difficult.  

The scientific discussion on the subject has seen a shift in the past two decades. Instead of asking **if** industrial policy is justified, scholars have increasingly asked **how** industrial policy regimes should be designed in order to be economically beneficial (see, for example, Aiginger (2014) and Rodrik (2004)). The perception became that most governments are involved in some form of industrial policy anyways, so the focus should be on "how to do it right" (Stiglitz et al. (2013), pp. 8-9). 

The article **"Industrial Policy and Competition"** by Aghion et al. (2015) also points in this direction. The authors address the major criticism of industrial policy - its supposed tendency to hamper competition - and provide a novel argument to the literature. According to their claims, there may be **complementarity** between industrial policy and competition. Appropriately designed and executed interventions may preserve of even enhance competition, and as a result, improve economic outcomes. In the following, we examine the empirical approach presented in the article by investigating a set of industrial policies in China. Our aim is to estimate the effects of **competition-friendly** policy interventions on firm-level productivity. We have already introduced most of the econometric and R-related methods needed, so we can go a bit faster through the practical analysis. As before, we begin with an overview of the data.

## Exercise 10 -- Exploring Industrial Policy in China

### a) The Policies

The data used by the authors is part of a comprehensive firm-level dataset, which comprises all medium and large manufacturing enterprises in China. It was collected by the Chinese Bureau of Statistics and covers the years 1998 to 2007. The authors decided to base their econometric analysis on domestically and privately owned enterprises only. Thus, I excluded foreign firms and state owned enterprises beforehand. The file `dat3.rds` contains relevant data to replicate the main findings of the article. Since there are still over 1 million observations, the computation of some tasks may take a bit longer than previously. We first look at some main variables of interest.

**Task:** Load the file `dat3.rds` using the `readRDS()` function and assign it to `dat`. Show the upper entries of the key variables with `head()`. The code is already given below; just click `check`.
```{r}
#< task_notest
library(dplyr)
dat = readRDS("dat3.rds")

dat %>%
  select(id:tariff) %>%
  head(n = 5)
#>
```

The dataset has a panel structure, similar to the data we used previously. However, instead of industries we follow individual firms over time. The first variable `id` is a unique firm identifier. The column `industry` groups firms into industrial subsectors, as defined by the authors (for instance, food products, chemical industry, etc.). All of them are manufacturing-related. A four-digit code `county` identifies the region/city where a firm is located. The observations are annual, indicated by the column `year`. The outcome variable we are interested in is, as before, total factor productivity `tfp`. However, the variable here describes the level of TFP, not growth. 

The remaining variables relate to the four types of industrial policies we want to study: subsidies, tax holidays, low interest loans, and tariffs. The first policy variable `index_subsidy` is a dummy that indicates whether a firm received a subsidy by Chinese authorities in the corresponding year. The full dataset also includes the exact amount of subsidies granted. Similarly, `index_tax` indicates whether a firm received a tax holiday - a temporary reduction or elimination of a tax. While the data contain information on tax payments, the explicit amount of tax holidays is not included. Hence, the authors calculated the difference between the amount a firm ought to pay, as indicated by its statutory tax rate, and the actual tax payment. A positive difference is the presumed amount of tax holidays granted to that firm.

Before we discuss the other two policies, we look at the magnitude and trajectory of subsidies and tax holidays in China. To do so, we aggregate the policy variables on an annual level and compute their means. Since `index_subsidy` and `index_tax` are dummies, their means indicate the proportion of firms that received subsidies and tax holidays, respectively. I prepared the next task to perform the aggregation and to transform the results for further visualization.

**Task:** Compute the annual means for `index_subsidy`, `index_tax`, `interestratio` and `tariff` using `group_by()` and `summarize()`. Transform the output to a "long" format using the `pivot_longer()` function and store the results in `vis`. Show the top five rows of `vis`. The code is already given below; just click `check`.
```{r}
#< task_notest
library(tidyr)
# Aggregate data
vis = dat %>% 
  group_by(year) %>%
  summarize(subsidies = mean(index_subsidy*100, na.rm = TRUE),
            tax_holidays = mean(index_tax*100, na.rm = TRUE),
            interest = mean(interestratio*100, na.rm = TRUE),
            tariffs = mean(tariff, na.rm = TRUE)) %>%
  pivot_longer(-year, names_to = "policy", values_to = "value")

# Show object structure
head(vis, n = 5)
#>
```

We introduce the `summarize()` function below. The data frame `vis` contains a time series of means for each policy, converted to a percentage format. I also included the other two policies, which we discuss below. In order to get an impression for the importance of subsidies and tax holidays in China, we plot a line chart in the next task.

**Task:** Plot the proportion of firms that received subsidies and tax holidays over the sample period using `ggplot()`. The code is already given below; just click `check`.
```{r out.width=600, dpi=700}
#< task_notest
library(ggplot2)
# Filter the two policies and plot a line chart
vis %>% 
  filter(policy %in% c("subsidies", "tax_holidays")) %>%
  ggplot(aes(x = as.numeric(year), y = value, colour = policy,
         label = round(value, digits = 1))) +
  geom_line(size = 1.0) +
  geom_point(size = 1.5) +
  geom_text(nudge_y = 3.5, size = 2.75) + 
  facet_wrap(~ policy, nrow = 2, ncol = 1) +
  scale_x_continuous(breaks = 1998:2007) +
  coord_fixed(ratio = 0.07, ylim = c(0, 50)) +
  theme_green() +
  scale_color_manual(values = colours_custom2) +
  labs(title = "Subsidies and Tax Holidays", x = "Year", y = "Percent",
       caption = "Figure 10.0: Subsidies and Tax Holidays in China")
#>
```

Looking at the upper panel, we see that $8.4$ percent of all firms in the sample received subsidies in 1998. This proportion increased over time until 2004, where it reached a peak with $13.9$ percent. In the final year of the sample, 2007, $11.6$ percent of the studied firms were subsidized. The lower panel shows that tax holidays were a frequent policy instrument. In 1998 about $41.8$ percent of firms paid fewer taxes than their statutory tax rate suggests. This proportion reached a peak in the final year, where almost half of the firms received some form of a tax holiday. 

The other two policies we want to investigate are loans and tariffs. State banks and local governments support firms by providing loans at lower interest rates than regular financial markets. Unfortunately, loans provided by state institutions are not part of the data provided by the Chinese Bureau of Statistics. The authors therefore calculated a proxy variable for the presence of such loans. The data contain information on aggregate interest payments and liabilities. This allows calculating a variable `interestratio`, measuring a firm's average interest rate given by total interest payments over total liabilities. This rate is then used to construct a dummy variable `index_interest`, which indicates whether a firm's interest rate lies below the average of its competitors in the same industry. A value of $1$ therefore suggest that a firm is somewhat likely to hold low interest loans granted by state institutions.

Finally, we study tariffs on final goods. The variable `tariffs` measures the average industry tariff on imports in percent. The authors used World Bank data to calculate this measure. Since tariffs are set on the national level, all firms within the industry share the same value for a given year. In the next task, we plot average interest rate and tariffs over the sample period. The aggregated means of the variables are already part of the `vis` object we created above.

**Task:** Plot the average interest rate and the average tariff over the sample period using the `ggplot()` function. The code is already given below; just click `check`.
```{r out.width=600, dpi=700}
#< task_notest
# Filter the two policies and plot a line chart
vis %>% 
  filter(policy %in% c("interest", "tariffs")) %>%
  ggplot(aes(x = as.numeric(year), y = value, colour = policy,
         label = round(value, digits = 1))) +
  geom_line(size = 1.0) +
  geom_point(size = 1.5) +
  geom_text(nudge_y = 2, size = 2.75) + 
  facet_wrap(~ policy, nrow = 2, ncol = 1) +
  scale_x_continuous(breaks = 1998:2007) +
  coord_fixed(ratio = 0.17, ylim = c(0, 22)) +
  theme_green() +
  scale_color_manual(values = colours_custom2) +
  labs(title = "Interest Rate and Tariffs", x = "Year", y = "Percent",
     caption = "Figure 10.1: Interest Rate and Tariffs in China")
#>
```

The upper panel shows that the average interest rate for Chinese firms has fallen from $6.7$ percent in 1998 to $3.7$ percent in 2007. Of course, we do not particularly care about the exact interest rates in our investigation. However, we can use deviations from the average interest rate to identify firms that are likely to hold low interest loans granted by state institutions.

Looking at the lower panel, we see a substantial decrease in tariffs over the sample period. The average tariff was $18.6$ percent in 1998, whereas in 2007 it was merely $10$ percent. The biggest cut was in 2001 - about $4.5$ percentage points - when China joined the World Trade Organization. This aggregate measure masks the broad variation in tariffs across different industries, though. We want to illustrate this variation and its trajectory. To do so, we extract the tariffs for each industry-year and visualize them. Since each firm within an industry is subject to the same tariff, we need to get rid of duplicate entries in the following. We can use the `summarize()` function of the `dplyr` package to accomplish this task.

#< info "The summarize() function"
The `summarize()` function collapses a data frame to a single row. The function is particularly useful in combination with `group_by()` because we can break down a data frame into subgroups and compute a statistic within each group. To illustrate, consider the code snippet below.
```{r eval=FALSE}
library(dplyr)
dat %>%
  group_by(group_var) %>%
  summarize(average = mean(x1))
```
This call uses a data frame `dat`, groups it on the level `group_var` and computes the mean of a variable `x1` **within** each group. In other words, the call returns a row with the group mean for each subgroup in `group_var`. If you want to learn more about the `summarize()` function, check out Wickham and Grolemund (2016, p. 59) or read the function documentation.
https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/summarise
#>

**Task:** Group `dat` on the industry-year level by passing `industry` and `year` as arguments to `group_by()`. To collapse the long list of identical tariffs within industries, use `summarize()` in conjunction with the `unique()` function. The `unique()` function returns non-duplicate elements of a vector and omits the rest. Store the results in a new variable `tariffs` and show its first five rows. I already gave you part of the solution; just complete/replace the code below and click `check`.
```{r}
#< fill_in
# Insert the correct expressions for the placeholders ___ below.
___ = dat %>% 
  group_by(___) %>% 
  ___(tariff = unique(tariff)) %>% 
  ungroup()

# Show object structure
head(tariffs, n = 5)
#>
tariffs = dat %>% 
  group_by(industry, year) %>% 
  summarize(tariff = unique(tariff)) %>% 
  ungroup()

head(tariffs, n = 5)
```

#< award "dplyr Expert"
Congratulations! You know now how to break down data frames with summarize().
#>

The output contains columns for `year`, `industry` and the corresponding average industry tariff. We can display the data by creating a **ridgeline** plot. Ridgeline plots are useful to visualize changes in distributions over time (see R Graph Gallery (2020) for examples). Their appearance resembles mountain ridges, hence their name. In the next task, we use the `ggridges` package to create the plot.

#< info "The ggridges package"
The `ggridges` package is an addon to the `ggplot2` package. Using similar syntax, it allows creating and customizing ridgeline plots. The package offers `geoms` that can be used similar to other `geoms` in `ggplot()` function calls. Group-level densities can be estimated and visualized using `geom_density_ridges()`, which has an option to customize the bandwidth of the density estimator. If you want to learn more about the package and applications of ridgeline plots, check out the documentation or refer to the R Graph Gallery (2020).
https://cran.r-project.org/web/packages/ggridges/
#>

**Task:** Use `geom_density_ridges()` in conjunction with `ggplot()` to visualize the distribution of tariffs over the entire sample period. The code is already given below; just click `check`.
```{r out.width=600, dpi=700, warning=FALSE}
#< task_notest
library(ggridges)
ggplot(tariffs, aes(x = tariff, y = year)) + 
  geom_density_ridges(colour = "black", fill = "#66BD63", alpha = 0.6,                                          bandwidth = 1) +
  coord_fixed(ratio = 7) +
  scale_x_continuous(breaks = seq(0, 70, by = 10)) +
  theme_green() +
    labs(title = "Distribution of Tariffs", x = "Percent", y = "Year",
         caption = "Figure 10.2: Distribution of Tariffs in China")
#>
```

Note that the first year of the sample is displayed at the bottom of the plot. For each year we can see a density estimate of the distribution of all $61$ aggregate industry tariffs on imports. As noted above, the mean of the tariff distribution has fallen over the years.


#< quiz "spread"
question: The spread of the tariff distribution has...

sc:
    - ... increased over the sample period.
    - ... decreased over the sample period.*
    - ... stayed about the same.
    
success: Great!
failure: Try again.
#>

#< quiz "meantariff"
question: Most tariffs in the second half of the sample period range roughly between...

sc:
    - ... 0 and 5 percent.
    - ... 5 and 15 percent.*
    - ... 15 and 25 percent.
    - ... 25 and 35 percent.
    
success: Great!
failure: Try again.
#>


Over time, the spread of the tariff distribution has become smaller. A big proportion of tariffs range from approximately $5$ to $15$ percent in the later years of the sample. However, there is still substantial variation in tariffs across industries even in 2007. To give two concrete examples, an average tariff of $52$ percent was imposed on tobacco products, whereas fuel merely had an average of $6$ percent (over the entire sample period). For a summary of how tariffs and other policies were set across industries in China, refer to table 2 in Aghion et al. (2015), p. 8.

To sum this section up, we saw that China was actively involved in industrial policy during the sample period. Subsidies and tax holidays were common policy instruments and their scope even increased over the studied time span. The trajectory of tariffs indicates a substantial opening of Chinese markets to manufacturing imports. In the next section, we include the dependent variable of interest in our exploratory analysis.

### b) Simple Correlations across the Policies and Productivity

Similar to the first part of this problem set, we want to assess the effectiveness of policy interventions with respect to productivity outcomes. The dependent variable here is firm-level TFP. The TFP level has been estimated using a method developed by Olley and Pakes (1996). This method accounts for a possible simultaneity bias when estimating the input coefficients of a production function. More details on this topic and how the method was applied to the data at hand can be found in Du et al. (2014, pp. 369-370).

To investigate whether there are associations between industrial policies and firm-level performance, we again make use of Pearson's correlation coefficient. We compute a correlation matrix to investigate possible association patterns among all variables at once. We already introduced the `cor()` function in exercise 4. When passed a data frame, the function returns a matrix containing the pairwise correlation coefficients for all variables in the data frame. 

**Task:** Compute the correlation matrix of the variables `index_subsidy`, `index_tax`, `index_interest`, `tariff` and `tfp`. To do so, combine the `select()` function and the `cor()` function. Because there are missing values in the data, pass the argument `use = "pairwise.complete.obs"` to `cor()`. Store the matrix as `dat_cor`. I already gave you part of the solution; just complete/replace the code below and click `check`.
```{r}
#< fill_in
# Insert the correct expressions for the placeholders ___ below.
dat_cor = dat %>%
  select(___) %>%
  ___
#>
dat_cor = dat %>%
  select(index_subsidy, index_tax, index_interest, tariff, tfp) %>%
  cor(use = "pairwise.complete.obs")
```

Some of the variables in the dataset contain missing values. The `cor()` function returns a value of `NA` ("not available") in such cases. By passing the argument `use = pairwise.complete.obs` we compute the coefficient based solely on observation pairs with no missing values. We can create an appealing visualization of the matrix with the `corrplot` package.

#< info "The corrplot package"
The `corrplot` package offers the `corrplot()` function, which displays correlation matrices. The function provides several arguments to add visual aids, such as heat maps. The functionality of `corrplot()` is particularly useful when investigating correlation patterns across many variables at once. For more information on how to use the function, refer to the package documentation.
https://cran.r-project.org/web/packages/corrplot/index.html
#>

**Task:** Visualize `dat_cor` using the `corrplot()` function. The code is already given below; just click `check`.
```{r out.width=525, dpi=700}
#< task_notest
# Specify colours for heatmap overlay
col = colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#66BD63", "#1A9850"))

# Plot the correlation matrix
library(corrplot)
corrplot(dat_cor, 
         method = "color",
         col = col(200),  
         type = "upper", 
         addCoef.col = "black",
         order = "original",
         tl.col = "black", 
         tl.srt = 45,
         number.digits = 2,
         sig.level = 0.05, 
         insig = "blank", 
         diag = FALSE) 
#>
```

#< award "Visualization Master"
Congratulations! In this exercise, you successfully used a ridgeline plot and a correlation plot to visualize sophisticated data structures.
#>

The figure shows the full set of pairwise correlation coefficients, rounded to two decimals. A heat map is overlaid to magnify the strength of the associations. Diagonal and redundant part of the matrix are removed. We are primarily interested in the last column of the matrix, which indicates the correlation coefficients between each policy instrument and firm-level TFP. Since many factors determine the productivity outcomes of firms, we obviously do not expect to see "large" coefficients. Recall that the policy variables, with the exception of tariffs, are dummies that do not incorporate the exact amount of state support. The results thus show whether targeted firms performed better or worse, on average.


#< quiz "policycorr"
question: Which policies are associated with improved firm productivity?

sc:
    - None.
    - Low interest loans and tariffs.
    - Subsidies and tax holidays.*
    - All four.
    
success: Great!
failure: Try again.
#>


We see a small positive association for subsidies ($0.03$) and tax holidays ($0.06$), indicating that firms targeted with these policies tended to perform better. The other two policies show a negative correlation with TFP. Tariffs show the strongest correlation with a coefficient of $-0.09$. Protected industries tended to perform worse. 
The other three columns show the correlations across the four policy variables. Substantial positive correlation patterns would suggest that some firms tended to receive multiple types of state support simultaneously. Negative correlation coefficients would suggest that the individual policies benefitted distinct sets of firms, with little overlap.


#< quiz "assignmentcor"
question: Is there compelling evidence for any such assignment patterns?

sc:
    - Yes.
    - No.*
    
success: Great!
failure: Try again.
#>


#< award "Industry Politician"
Congratulations! You are no longer a rookie in Chinese industrial policy.
#>

Looking at the matrix, we see that all correlations are fairly close to zero, indicating no such assignment patterns. For the same reasons we have discussed extensively in exercise 4, the observed correlations do not justify causal claims. Part of the empirical literature on industrial policy has tried to establish causal links between industrial policies, economic growth and productivity (see, for example, Criscuolo et al. (2019) and Nunn and Trefler (2010)). As mentioned in the introduction, there exists a wide range of findings contingent upon specific policy implementations, not allowing for broad generalizations (see Harrison and Rodr√≠guez-Clare (2009), pp. 4062-4070 for a review of the literature).

In our analysis, we are not so much interested whether industrial policies generally improve productivity outcomes. We rather focus on whether policy interventions that respect sound principles of competition are (more) productivity-enhancing. That is to say, we are interested in potential **complementarities** between competition and industrial policies. In exercise 11, we briefly present the argument of the authors why a **competition-friendly** industrial policy should be more effective. We also introduce a set of policy variables that quantify the relationship between policy interventions and competition in China.

## Exercise 11 -- Industrial Policy and Competition

### a) The Link between Industrial Policy and Competition

The popularity of industrial policy sharply declined in the 1980s. In the perception of academics and policy advisors, such interventions turned out to be a vehicle for governments to "pick winners" and to distort otherwise efficient market outcomes (Aghion et al. (2011), pp. 2-3). The argument was that policy makers were not equipped to decide which firms or industries had the most potential for future success. Furthermore, industrial policies were to enable special interest groups lobbying for state support, consequently hardening old and uncompetitive structures (Stiglitz et al. (2013), pp. 5-6). Does industrial policy inherently undermine competition?

The authors of the article address this question by focusing on the design of policy interventions. They claim that properly governed industrial policies may preserve of even enhance competition. To bolster their argument, the authors develop a simple theoretical model, which we can briefly summarize as follows: Without industrial policies, innovative firms may choose to diversify, that is, to operate in different industries in order to evade competition. This increases product market concentration and diminishes the incentives for productivity-enhancing innovations. Encouraging innovative firms to operate in the same industry by means of industrial policies may decrease product market concentration in targeted industries. Firms then resort to productivity-enhancing innovations in order to "escape competition" with each other (Aghion et al. (2005), pp. 702-703). Based on these conjectures we can state two hypotheses that guide our empirical analysis in the following:

**1. Industrial policies allocated towards competitive industries should be more effective.**

This hypothesis supposes that subsidizing or protecting firms that operate in a low competition environment is less likely to induce productivity-enhancing innovations. Theory shows that a monopolist values an identical cost reduction innovation less than a competitive firm does (Belleflamme and Peitz (2015), pp. 500-502). The intuition behind this finding is that the monopolist already earns a positive profit, so a cost reduction innovation only replaces an existing profit with a larger one. A firm under perfect competition, however, merely covers its costs. A cost reduction investment will lead to a positive profit. It can be shown that the value a competitive firm places on the exact same innovation is larger compared to a monopolist (this finding is called *monopoly replacement effect*). The first hypothesis suggests that industrial policies should be targeted towards industries where competition is already high. Allocated resources should be more likely to stimulate productivity growth in such industries. The second hypothesis is:

**2. Industrial policies that are more equitably dispersed across firms should be more effective.**

The idea behind this hypothesis is that interventions, which preserve competition, should be more likely to induce productivity-enhancing innovation. To illustrate, consider an extreme case: Suppose the government subsidizes a single firm within an industry ("picking a winner"). This firm could push competitors out of the market by reducing prices below costs. Product market concentration increases and incentives for productivity-enhancing innovations decline. The beneficiary is also not guaranteed to be the most efficient and innovative firm. It is possible that under such a regime mostly firms benefit that pursue the most successful lobbying efforts. On the other hand, industrial policies could be dispersed in a more equitable fashion. For instance, by granting subsidies to all firms that operate in a specific industry. This approach should maintain competition and encourage more firms to innovate in order to "escape competition" with each other (Aghion et al. (2005), pp. 702-703). To test these hypotheses, we first need a measure that captures the intensity of competition.

### b) Measuring Competition using the Lerner Index

The authors quantify the level of competition using the *Lerner index* (Belleflamme and Peitz (2015), pp. 34-35). This index captures market power using the ratio of operating profit less cost of capital over sales:
$$
Lerner = \frac{operating\ profit - cost \ of \ capital}{sales}. \tag{11.0}
$$
It rests on the assumption that under perfect competition, firms should make no profits above costs of capital. Hence, the presence of excess profits indicates some degree of market power. The Lerner index can therefore be viewed as an inverse measure of competition, that is to say, high values correspond to low levels of competition. To quantify the competitive structure of an entire industry, we can construct an aggregate Lerner index using industry-level profits, costs of capital and sales, respectively. Since the index takes on values between $0$ and $1$ (in the case of industry-wide losses assigning a value of zero), we can take the complement:
$$
Competition_{r,j,0} = 1 - Lerner_{r,j,0} \tag{11.1}
$$
where $Competition_{r,j,0}$ measures the intensity of competition in region/county $r$, industry $j$ in the initial period $0$. Hence, we obtain a measure describing the competitive structure across different industry-regions in China (for example, food products industry in Beijing, food products industry in Shanghai, chemical industry in Beijing, and so on). A value of $1$ indicates perfect competition, whereas lower values suggest some degree of market power.

Now consider regressing TFP, our outcome variable, on the level of competition. The explanatory variable might be endogenous in such a regression. It is likely that there is not only a causal link going from competition to productivity, but also a reverse causal link from productivity to competition. To illustrate, consider a firm having a breakthrough regarding an innovation that leads to a rise in its productivity. Such a "productivity shock" may lead to changes in the firm's market conduct, for instance by increasing output, decreasing prices, and so on. These changes may further affect the competitive structure of the industry through a reallocation of market shares, exits of firms, etc. Thus, innovation and competition are *mutually endogenous* (Aghion et al. (2005), p. 708). The OLS estimator would be subject to a simultaneity bias. To address this issue, we use the initial period value $Competition_{r,j,0}$ for all periods of the sample. By using a variable that is predetermined, we block the possible reverse causal link from productivity to competition. Productivity shocks $\varepsilon_{j, r, t}$ in subsequent periods $t = 1,...,t_{max}$ are unlikely to be correlated with the level of competition in the initial period $t = 0$  (Wooldridge (2016), pp. 511-513). Our measure should therefore be exogenous with respect to productivity shocks (see also exercise 4, where we used the lagged CPI for the same reason).

Since we use a subset of the data in this problem set, we cannot replicate the construction of the variable. The Lerner index and other measures we are about to discuss require data on foreign firms and state owned enterprises (the level of competition is also determined by domestic and private firms). The replication dataset includes the Lerner index and other measures we use in the following. In order to get an intuition for the variable, we plot a histogram in the next task.

**Task:** Load the file `dat3.rds` again and assign it to `dat`. Plot a histogram of the variable `competition` for the initial period of the sample `1998`. Include a line indicating the variable's mean. The code is already given below; just click `check`.
```{r out.width=550, dpi=700}
#< task_notest
library(dplyr)
dat = readRDS("dat3.rds")

# Plot histogram
library(ggplot2)
dat %>%
  filter(year == "1998") %>%
  ggplot(aes(x = competition)) + 
  geom_histogram(bins = 100, colour = "#66BD63",
                 fill = "#66BD63", alpha = 0.6) + 
  geom_vline(aes(xintercept = mean(competition, na.rm = TRUE)), 
             color = "#d23e4e", linetype = 2, size = 0.6) +
  theme_green(base_size = 12) +
  coord_cartesian(xlim = c(0.8, 1)) +
  labs(title = "Histogram Competition", subtitle = "", 
       x = "Competition", y = "",
       caption = "Figure 11.0: Histogram Competition in China")
#>
```


#< quiz "assignmentpatterns"
question: Consider the histogram above. Which statement is true?

sc:
    - Most firms operate in highly competitive industries.*
    - Most firms operate in highly uncompetitive industries.
    - High and low intensities of competition are equally common.
    
success: Great!
failure: Try again.
#>


The figure shows the distribution of the inverse aggregated Lerners. Since we use the initial values for all sample periods, it is sufficient to condition on year 1998. Rare values below $0.8$ are truncated in the histogram. We can see that the vast majority of firms in China operated in highly competitive environments, where there are no or only small profits excess costs of capital. Some firms, however, showed substantial markups indicating various degrees of market power. In the next section, we discuss how to connect this measure with the allocation of industrial policies in China.

### c) Targeting Competitive Industries

Recall our first hypothesis. We want to assess whether industrial polices allocated towards competitive industries are (more) effective. To do so, we construct measurements that capture the "competition bias" of policy interventions across different parts of China. That is to say, how much allocations of state support tend to favor competitive industries. For each policy, we calculate a correlation coefficient between industry-level support and the level of competition in that region/city. For subsidies, the variable takes on the following form:
$$
\Omega_{r,t,subs} = Corr(Subsidy_{r,j,t}, Competition_{r,j,0}) \tag{11.2}
$$
where $\Omega_{r,t,subs}$ is the correlation coefficient of aggregated industry subsidies and competition in region $r$ at time $t$. Thus, $\Omega_{r,t,subs}$ quantifies the extent to which local authorities and the government favor competitive industries in their allocation of subsidies. If, for example, in Shanghai subsidies are granted exclusively to firms that operate in highly competitive industries, the variable's value will be close to $1$. Conversely, if only firms with high market power receive subsidies, the variable will take on a value close to $-1$. To address the potential endogeneity of the measure, $\Omega_{r,t,subs}$ is constructed using the initial period level of competition, as described above. For the other policies, we can construct correlation coefficients $\Omega_{r,t,m}$ in the same way:
$$
\Omega_{r,t,tax} = Corr(TaxHolidays_{r,j,t}, Competition_{r,j,0}) \tag{11.3}
$$
$$
\Omega_{r,t,int} = Corr(Interest_{r,j,t}, Competition_{r,j,0}) \tag{11.4}
$$
where $\Omega_{r,t,tax}$ and $\Omega_{r,t,int}$ measure the extent to which tax holidays and low interest loans are allocated towards competitive industries, respectively. Since tariffs are set on the national level, there is only a single industry-wide tariff for all regions across China. However, since competition differs across regions and tariffs change over time, we can also construct a variable for tariffs:
$$
\Omega_{r,t,tariff} = Corr(Tariff_{j,t}, Competition_{r,j,0}) \tag{11.5}
$$
where $\Omega_{r,t,tariff}$ measures the extent to which national tariffs benefit competitive industries. Before we proceed, we ask whether Chinese authorities actually favored competitive industries as part of their policy strategy. Using $\Omega_{r,t,m}$, we can try to answer this question. If Chinese authorities predominantly allocated industrial policies towards competitive industries, we would expect the means of the correlation coefficients to be well above zero. To check this, we compute summary statistics for the four policy variables in the next task.

**Task:** Show summary statistics for `cor_subsidy_lerner`, `cor_tax_lerner`, `cor_interest_lerner`, `cor_tariff_lerner` using the `my_skim()` function. The code is already given below; just click `check`.
```{r}
#< task_notest
library(skimr)
print(my_skim(dat, cor_subsidy, cor_tax, cor_interest, cor_tariff))
#>
```


#< quiz "competition bias"
question: Is there compelling evidence that industrial policies in China favored competitive industries?

sc:
    - Yes.
    - No.*
    
success: Great!
failure: Try again.
#>


Looking at the means of the four variables, we see that their values are fairly close to zero. This indicates that the intensity of competition did not play a major role in the targeting of industrial policies. Tax holidays may be an exception, with an average correlation coefficient of $-0.1$. The negative sign indicates that this policy instrument tended to favor *less* competitive industries. The summary statistics also show that the variables cover the full range from $-1$ to $1$, suggesting that the patterns of interventions varied substantially across different parts of China. Following our first hypothesis, we expect firms located in regions with larger correlation coefficients - that is to say, stronger "competition-bias" of industrial support - to exhibit higher productivity. We will test this conjecture in exercise 12.

### d) Dispersion of Industrial Policies

Recall our second hypothesis. We want to investigate whether equitable dispersion schemes of industrial policies are more effective. The authors measure the concentration of state support using the *Herfindahl index* (Belleflamme and Peitz (2015), pp. 35-36). This index is regularly used to quantify the level of market concentration by using a measure of firm size. We can adjust the Herfindahl index to capture the concentration of industrial support within industries. For subsidies, the variable takes on the following form:
$$
Herf\_subsidy_{i,j,r,t} = \sum_{h \in j, \ h \notin i}{\left(\frac{Subsidy_{i,j,r,t}}{Total \ Subsidies_{j,r,t}}\right)^2} \tag{11.6}
$$
where $Herf\_subsidy_{i,j,r,t}$ is the Herfindahl for firm $i$, industry $j$, region $r$ at time $t$. To illustrate, in the case of only one firm receiving a subsidy within the industry, the variable takes on a value of $1$. This indicates the most concentrated allocation possible. If equal amounts of subsidies are granted to all firms within the industry, the variable converges to zero as the number of firms grows. Low Herfindahls therefore indicate competition-friendly allocations of industrial support.

Similar to the Lerner index, a concern might be the potential endogeneity of the variable. Consider regressing firm-level productivity on the Herfindahl for subsidies. The Herfindahl index might be endogenous in such a regression. Suppose Chinese authorities were more likely to provide subsidies to better performing firms. Possibly, to make their interventions look more potent or because successful firms are better lobbyists. We would overestimate the effectiveness of the policy, since targeted firms performed better anyway. It is also conceivable that less productive firms were more likely to receive subsidies. Chinese authorities could have protected specifically firms that are inefficient (recall the infant-industry argument from exercise 10). We would therefore underestimate the effect, since targeted firms performed worse regardless of the policy. To address the potential endogeneity of the measure, we exclude the own state support in calculating the Herfindahl for firm $i$ from both the numerator and the denominator. The Herfindahl of firm $i$ therefore only depends on the subsidies of other firms within the industry and should consequently be exogenous with respect to firm $i$'s performance. For the other policies, we can construct Herfindahls in the same way:
$$
Herf\_tax_{i,j,r,t} = \sum_{h \in j, \ h \notin i}{\left(\frac{TaxHoliday_{i,j,r,t}}{Total \ TaxHolidays_{j,r,t}}\right)^2} \tag{11.7}
$$
$$
Herf\_interest_{i,j,r,t} = \sum_{h \in j, \ h \notin i}{\left(\frac{Interest_{i,j,r,t}}{Total \ Interest_{j,r,t}}\right)^2} \tag{11.8}
$$
where $TaxHoliday_{i,j,r,t}$ is the amount of taxes saved by firm $i$, and $Interest_{i,j,r,t}$ is the difference between the average interest rate within the industry and firm $i$'s interest rate (for positive differences). Hence, both Herfindahls measure the concentration of tax holidays and low interest loans provided by Chinese authorities. Since tariffs are set on the national level for all firms equivalently, we cannot calculate a measure of dispersion. Akin to the Lerner indexes, we can take the complement of the Herfindahls, so that a value of $1$ indicates a perfectly dispersed (competition-friendly) allocation:
$$
CompHerf\_subsidy_{i,j,r,t} = 1 - Herf\_subsidy_{i,j,r,t} \tag{11.10}
$$
and so on for the other policies. To illustrate that this measure adequately captures the concentration of industrial support, we look at a simple theoretical example in the next two tasks. The code below generates two exemplary industries $A$ and $B$ containing six firms each and a corresponding subsidy allocation. To visualize this example, we plot a bar chart.

**Task:** Run the code below to generate example data on two different industries `A` and `B`. Plot the allocation of subsidies within each industry with a bar chart using `ggplot()`. The code is already given below; just click `check`.
```{r out.width=525, dpi=700}
#< task_notest
# Create two example industries containing six firms each and a subsidy allocation
example = tibble(industry = c(rep("Industry A", 6), rep("Industry B", 6)),
                 firm = as.character(rep(1:6, 2)),
                 subsidy = c(1250, 750, 250, 0, 0, 0,
                             450, 400, 400, 350, 350, 300))

# Visualize example with a bar chart
ggplot(example, aes(x = firm, y = subsidy)) + 
  geom_col(colour = "#66BD63", fill = "#66BD63", alpha = 0.5) +
  facet_wrap(~industry, nrow = 1, ncol = 2) + 
  theme_green(base_size = 12) +
  labs(title = "Example: Subsidies per Firm", x = "Firm", y = "Subsidy",
       caption = "Figure 11.1: Example: Subsidies per Firm")
#>
```


#< quiz "concentration"
question: Which industry is subject to a more concentrated subsidy scheme?

sc:
    - Industry A*
    - Industry B
    
success: Great!
failure: Try again.
#>


When we compare both panels, we see that subsidies in industry $A$ are much more concentrated. Two firms in industry $A$ receive a large part of the total amount of subsidies, whereas industry $B$ shows a more equitable dispersion. To demonstrate that the Herfindahl index captures this difference, we compute the measure for each firm in this example. I prepared a simple function `calc_hi()`, which does the computations in a single call. If you are interested in the code of the function, check out the info box below. 

#< info "The calc_hi() function"
The function `calc_hi` computes the Herfindahls as described by the authors (see Aghion et al. (2015), p. 13). The function takes an allocation as argument. The first step initializes an empty vector of length `x`, which is used to store the individual Herfindahls. A `for()` loop passes through the vector `x` and excludes the own support when calculating the Herfindahl for firm `i`. Finally, the function returns the vector of Herfindahls `hi`.
```{r eval = FALSE}
calc_hi = function(x){
  hi = numeric(length = length(x))
  for(i in seq_along(hi)){
    x_i = x[-i]
    hi[i] = sum((x_i/sum(x_i))^2)
  }
  return(hi)
}
```
If you want to learn more about programming and writing functions in R, refer to Kabakoff (2015, pp. 107-109), Wickham and Grolemund (2016, Chapter 15) or check out the R documentation.
https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/function
#>

**Task:** Compute the Herfindahls for both example industries using `group_by()`, `mutate()` and `calc_hi()`. Plot the allocation of subsidies within each industry with a bar chart as before and overlay the computed Herfindahls. The code is already given below; just click `check`.
```{r out.width=525, dpi=700}
#< task_notest
# Compute Herfindahls for both industries and store the values
example = example %>%
  group_by(industry) %>%
  mutate(hi = calc_hi(subsidy)) %>%
  ungroup()

# Visualize example data with a bar plot and overlay Herfindahls
ggplot(example, aes(x = firm, y = subsidy, 
                    label = round(hi, digits = 2))) + 
  geom_col(colour = "#66BD63", fill = "#66BD63", alpha = 0.5) +
  geom_text(nudge_y = 50, colour = "black") +
  facet_wrap(~industry, nrow = 1, ncol = 2) + 
  theme_green(base_size = 12) +
  labs(title = "Example: Herfindahls per Firm", x = "Firm", y = "Subsidy",
       caption = "Figure 11.2: Example: Herfindahls per Firm")
#>
```

#< award "Visualization Expert"
Congratulations! You successfully used bar charts to visualize data with a categorical component.
#>

The plot shows the computed Herfindahls rounded to two decimals above each firm's subsidy. As expected, industry $A$ shows higher values than Industry $B$. The measure successfully captures different levels of subsidy concentrations. Recall that the values within industries differ because the subsidy of firm $i$ is excluded when calculating the Herfindahl for firm $i$. Taking the complement of the Herfindahl indexes leaves us with an increasing measure for the competition-friendliness of policy allocations. In the next task, we return to the actual data and look at the dispersion schemes of industrial policies in China.

**Task:** Show a histogram of the variables `compherf_subsidy`, `compherf_tax` and `compherf_interest` using `ggplot()` and combine them with `grid.arrange()`. Also include the variable's means. The computation might take a few seconds longer than before; just click `check`.
```{r out.width=600, dpi=700, warning=FALSE, message=FALSE}
#< task_notest
library(gridExtra)
# Histogram subsidies
s = ggplot(dat, aes(x = compherf_subsidy)) + 
  geom_histogram(bins = 30, colour = "#66BD63", 
                 fill = "#66BD63", alpha = 0.6) + 
  geom_vline(aes(xintercept = mean(compherf_subsidy, na.rm = TRUE)), 
             color = "#d23e4e", linetype = 2, size = 0.6) +
  theme_green() + labs(title = "CompHerf_subsidy", x = "", y = "")

# Histogram tax holidays
t = ggplot(dat, aes(x = compherf_tax)) + 
  geom_histogram(bins = 30, colour = "#66BD63", 
                 fill = "#66BD63", alpha = 0.6) + 
  geom_vline(aes(xintercept = mean(compherf_tax, na.rm = TRUE)), 
             color = "#d23e4e", linetype = 2, size = 0.6) +
  theme_green() + labs(title = "CompHerf_tax", x = "", y = "")

# Histogram low interest loans
i = ggplot(dat, aes(x = compherf_interest)) + 
  geom_histogram(bins = 30, colour = "#66BD63", 
                 fill = "#66BD63", alpha = 0.6) + 
  geom_vline(aes(xintercept = mean(compherf_interest, na.rm = TRUE)), 
             color = "#d23e4e", linetype = 2, size = 0.6) +
  theme_green() +
  labs(title = "CompHerf_interest", x = "", y = "", 
       caption = "Figure 11.3: Histograms inverse Herfindahls")

# Combine histograms in one plot
grid.arrange(s, t, i, nrow = 3, ncol = 1)
#>
```


#< quiz "herfindahlzero"
question: Why are there so many data points with a Herfindahl of zero?

sc:
    - There are missing values in some industries.
    - Not all industries receive government support.*
    
success: Great!
failure: Try again.
#>

#< quiz "equity"
question: Which policy seems to be dispersed in the least equitable fashion?

sc:
    - Subsidies*
    - Tax holidays
    - Low interest loans
    
success: Great!
failure: Try again.
#>


Note that `compherf_subsidy`, `compherf_tax` and `compherf_interest` are the reverse Herfindahls, so values close to $1$ indicate competition-friendly allocations. Looking at the histograms, we see that all policy variables cover virtually the full range of possible values. Obviously, there are large numbers of industries where no firm received state support, leading to a reverse Herfindahl of $0$. Among the policies, subsidies seem to be the most concentrated, whereas tax holidays and low interest loans appear to be more dispersed. Following our second hypothesis, we expect firms within industries that are subject to more equitable dispersion schemes to exhibit higher productivity.

To summarize the discussion on the policy variables, consider the difference between the two measures we introduced: The correlation coefficients $\Omega_{m,r,t}$ capture allocation patterns of industrial policies **between** industries, whereas the reverse Herfindahls $CompHerf_{i,j,m,r,t}$ capture allocation patterns **within** industries. The former relates to the question "which industry to support"; the latter relates to the question "how to support". For both measures, we hypothesize that **competition-friendly** allocations have a (more) positive effect on productivity outcomes. In exercise 12, we estimate an econometric model to test this conjecture.

## Exercise 12 -- Econometric Model and Results II

### a) The Econometric Model

The model we want to estimate is similar to the model we know from exercise 5. Broadly speaking, we regress productivity outcomes on the policy variables, a set of controls, and perform the fixed effects transformation. The outcome variable $ln \ TFP_{i,j,r,t}$ is the natural logarithm of TFP for firm $i$, sector $j$, region $r$ at time $t$. Using the log allows for an intuitive interpretation of the coefficients, as we will see below. The equation takes on the following form:
$$
ln \ TFP_{i,j,r,t} = \alpha_m \Omega_{m,r,t} + \beta_m CompHerf_{i,j,m,r,t} \\\ + \theta_1 Z_{i,j,r,t} + \theta_2 S_{j,t} + l_i + d_t + \epsilon_{i,j,r,t} \tag{12.0}
$$
where $\Omega_{m,r,t}$ and $CompHerf_{i,j,r,m,t}$ are the policy variables we introduced previously. Recall that the former measures the correlation of industrial support with competition in region $r$ at time $t$, and the latter measures the dispersion of industrial support for firm $i$, industry $j$, region $r$ at time $t$. Our primary interest lies in obtaining the coefficients $\alpha_m$ and $\beta_m$, which we consider the effects of a "competition-friendly" industrial policy.

The model also includes a vector of firm-level controls $Z_{i,j,r,t}$ and a vector of industry-level controls $S_{j,t}$. We discuss these covariates below. To account for unobserved time-invariant heterogeneity across firms, we include firm fixed effects $l_i$. Time fixed effects $d_t$ account for productivity shocks that are common to all firms in the same period. The next task displays the control variables.

**Task:** Load `dat3.rds` again. Show the names of the controls with `select()` and the `names()` function. The code is already given below; just click `check`.
```{r}
#< task_notest
library(dplyr)
dat = readRDS("dat3.rds")

# Display the names of the control variables
dat %>% 
  select(competition, competition_square, index_subsidy:index_interest, 
         exportshare_industry:lnfwTariff) %>%
  names()
#>
```

Since our model in exercise 5 also had productivity outcomes as dependent variable, we highlight some similarities and some differences in the selection of covariates in our discussion. The first control is `competition`. We described how the Lerner index was used to construct the variable. Recall that our specification on the subject of competition policy did not include a control for the level of competition. In exercise 5, we elaborated on the issue of *bad controls* - variables we should not control for (Angrist and Pischke (2009), pp. 63-67). We argued that the intent of competition policy is to enhance competition. Thus, holding the level of competition constant makes little sense if we want to estimate the effect of competition policy. Here, however, we control for the level of competition.


#< quiz "competition covariate"
question: Why do we control for competition? Choose the answer that seems most important.

sc:
    - Explaining more variation in the dependent variable can decrease standard errors.
    - We can learn about the general relationship between competition and productivity.
    - Omitting the variable could bias our results.*

success: Great!
failure: Try again.
#>


While all three answers are correct, the third one is most important. To explain why, consider the causal graph below. 

#< preknit
```{r echo=FALSE, warning=FALSE, results = 'asis', out.width=700, dpi=700}
suppressPackageStartupMessages(library(knitr))
include_graphics("C:/Users/ThinkPad User/Documents/CompetitionPolicy/Problem Set/comp.jpg")
```
#>

For a less convoluted discussion, we refer to the variables by their abbreviations in the following. We assume that both $IP$ and $C$ have a causal effect on $TFP$. We are primarily interested in potential complementarities between $IP$ and $C$ - that is, whether the effect of $IP$ on $TFP$ is **larger** for higher values of $C$. The framework of *directed acyclic graphs* is *non-parametric*. Causal effects represented by edges can be arbitrarily complex functions $f(.)$ (Morgan and Winship (2015), p. 90). Therefore, we do not have to add any new edges or annotations to the graph to capture the notion of complementarities. By including $C$ in the graph, we already allow the effect of $IP$ to depend on the level of $C$. 

Recall that we use the initial period level of $C$ to block possible reverse causal links from $TFP$ to $C$. By using predetermined values, we know that there cannot be any causal link from $TFP$ or $IP$ to $C$. This leaves us with a possible link from $C$ to $IP$. Suppose $C$ influences the assignment of $IP$ in some way. $C$ immediately qualifies as a relevant omitted variable, since there is an association with both $IP$ and $TFP$. Omitting $C$ would induce bias and inconsistency, just as described in exercise 4. The generic situation displayed in the graph is also referred to as *confounding* (Morgan and Winship (2015), pp. 82-83). If a third factor influences a cause and an outcome, the effect of the cause is said to be *confounded*. Where competition was a mediator in our assessment on competition policy, here it is a confounder that should be controlled for. By using the initial level of competition, we know that competition does not qualify as a channel through which industrial policy affects TFP in our estimation.

We also add a quadratic term `competition_square` to the model. This is a more flexible approach that allows the marginal effect of competition to change for different levels of competition (Wooldridge (2016), pp. 173-177). The authors here follow Aghion et al. (2005) who found a non-linear effect of competition that takes on an "inverted U-shape" (the effect is positive until high levels of competition where the marginal effect on innovation is actually negative). Such a functional relationship can be approximated by including a quadratic term.

We already discussed the three variables `index_subsidy`, `index_tax` and `index_interest` in exercise 11. Recall that these are dummy variables indicating whether a firm received subsidies, tax holidays and low interest loans, respectively. We should include these variables since they are part of the total effect of industrial policy on TFP. Competition and industrial policies might have purely additive effects with no complementarities. In principle, we could instead control for the exact amount of state support, allowing for a more precise quantification of the policy effects (which policy has the biggest impact on productivity, etc.). The authors do not explain why they chose to control for the dummies though.

#< info "Alternative Modeling Strategy"
Alternatively, the authors could have modeled the complementarity between industrial policies and competition using interaction terms (Wooldridge (2016), pp. 177-179). Such an approach could reveal the effects of industrial policies conditional on different levels of competition. For subsidies, we could estimate:
$$
ln \ TFP_{i,j,r,t} = \beta_0 + \beta_1 Subs_{i,j,r,t} + \beta_2  Comp_{j,r,0} + \theta_{s,c} Subs_{i,j,r,t}\times Comp_{j,r,0} + ... + \epsilon_{i,j,t} \tag{12.1}
$$
where $Subs_{i,j,r,t}$ is the amount of subsidies granted to firm $i,j,r$ at time $t$ and $Comp_{j,r,0}$ is the level of initial competition within the industry. Using this specification, we could test $H_0:\theta_{s,c} = 0$ against the one-sided alternative
$H_1: \theta_{s,c} > 0$. A significant and positive interaction term would provide evidence that there is indeed complementarity between both variables. The authors do not elaborate why they chose to use the correlation coefficients $\Omega_{m,r,t}$ to  capture the complementarity between competition and industrial policy in their analysis. One benefit of their approach is that the correlation measure is easier to interpret than interaction terms. Unless $\theta_{s,c}$ in $12.1$ is zero, we have to describe the relationship between subsidies and productivity as a function of competition and cannot simply provide one coefficient.
#>

The control `exportshare_industry` measures the proportion of exports to total value added of an industry. Clearly, we would expect industries with a strong focus on exports to exhibit higher productivity. Although we only analyze privately owned enterprises, some of them show minority state participation measured by `stateshare`. The next three variables relate to the potential effects of foreign direct investment (FDI) on productivity (Du et al., 2014, 2012). The idea is that the presence of foreign capital might create knowledge and technology spillovers to domestic firms that are not part of regular business transactions ("externalities"). In fact, a central part of China's industrial strategy focused on attracting FDI. For instance, during the studied period, a large proportion of foreign enterprises benefitted from a corporate tax rate of $15$ percent, whereas the tax rate for Chinese firms was $33$ percent (Du et al. (2014), p. 366). 

Industrial policies intended to attract FDI might be correlated with the policy instruments we care about. Controlling for these effects is reasonable, since we might otherwise attribute productivity-enhancing spillover effects to subsidies, tax holidays, etc. The covariates `horizontal`, `backward` and `forward` are measures for the presence of foreign capital. The variable `horizontal` measures the share of foreign capital (in value added) in the same industry as firm $i$ ("horizontal spillover effects"). The other two variables measure the foreign presence in the industry supplied by firm $i$ ("backward spillover effects"), and the supplying industry of firm $i$ ("forward spillover effects"), respectively. This approach recognizes that spillovers can potentially occur between firms in the same industry, but also between buyers and sellers. Refer to Du et al. (2014, 2012) for an assessment on this topic using the same Chinese dataset. The final three controls are the logs of tariffs on final goods. Akin to the aforementioned variables, firm $i$'s industry tariff is given by `lnTariff`, the tariffs of the supplied and supplying industries of firm $i$ are given by `lnbwTariff` and `lnfwtariff`, respectively.

### b) OLS Results

Now that we have discussed the covariates, we estimate the model in $(12.0)$ using OLS. We first consider subsidies, tax holidays and loans individually, before we perform a regression that includes all policy variables. The exception is `cor_tariff`, which we include in all estimations. Since we consider tariffs to be an important control, we should not leave out `cor_tariff`. The variable is part of the total effect of tariffs on productivity. In the upcoming task, we begin with both subsidy measures.

**Task:** Regress `tfp` on `cor_subsidy`, `compherf_subsidy` and the controls. Specify the variables `id` and `year` as fixed effects and cluster the standard errors on the industry-level indicated by `industry`. Assign the results to `reg_sub` and show them in a `stargazer` table. I already gave you part of the solution; just complete/replace the code below and click `check`.
```{r results = 'asis', warning = FALSE, message = FALSE}
#< fill_in
library(lfe)
# Insert the correct expressions for the placeholders ___ below.
reg_sub = felm(tfp ~ ___ + ___ + cor_tariff + competition + competition_square + index_subsidy + index_tax + index_interest + exportshare_industry + stateshare + horizontal + backward + forward + lnTariff + lnbwTariff + lnfwTariff | id + year | 0 | industry, data = dat)

# Organize results in a table
library(stargazer)
stargazer(reg_sub,
          title = "Table 12.0: Subsidies",
          type = "html", 
          style = "aer", 
          digits = 3,
          omit = c("exportshare_industry", "stateshare", "index_subsidy", 
                   "index_tax", "index_interest", "lnTariff", "horizontal",
                   "backward", "forward", "lnbwTariff", "lnfwTariff"),
          add.lines = list(c("Full Set of Covariates", rep("Yes", 1)),
                           c("Firm Fixed Effects", rep("Yes", 1)),
                           c("Time Fixed Effects", rep("Yes", 1))))
#>
library(lfe)
reg_sub = felm(tfp ~ cor_subsidy + compherf_subsidy + cor_tariff + competition + competition_square + index_subsidy + index_tax + index_interest + exportshare_industry + stateshare + horizontal + backward + forward + lnTariff + lnbwTariff + lnfwTariff | id + year | 0 | industry, data = dat)

library(stargazer)
stargazer(reg_sub,
          title = "Table 12.0: Subsidies",
          type = "html", 
          style = "aer", 
          digits = 3,
          omit = c("exportshare_industry", "stateshare", "index_subsidy", 
                   "index_tax", "index_interest", "lnTariff", "horizontal",
                   "backward", "forward", "lnbwTariff", "lnfwTariff"),
          add.lines = list(c("Full Set of Covariates", rep("Yes", 1)),
                           c("Firm Fixed Effects", rep("Yes", 1)),
                           c("Time Fixed Effects", rep("Yes", 1))))
```


I omitted most covariates in the table above for better readability. We first consider the coefficient for `cor_subsidy`. Recall that the variable measures the extent to which subsidies are allocated towards competitive industries. The estimate is $0.01$. Since TFP was transformed using the natural log, the coefficient has a percentage interpretation (this type of model is called a *log-level model*, see Wooldridge (2016), pp. 39 and 171-173). Thus, a perfect allocation of subsidies towards competitive industries, leading to a correlation coefficient of $1$, is estimated to increase firm-level productivity by about $0.1$ percent. The coefficient is not statistically different from zero, so we do not further interpret it here.

#< info "Discrepancy in the CSEs"
The standard errors in the article are about a quarter lower than in our estimations. The authors use an alternative estimator for CSE available in STATA. The general results are not strongly affected by the choice of the estimator, so we continue to use the CSE provided by `felm()` here. For the estimates reported in the article, refer to Aghion et al. (2015) p. 15.
#>

The second variable, `compherf_subsidy`, measures the degree to which subsidies are more equitably dispersed within industries. The estimated coefficient is $0.041$ and significant at the $1\%$ level. This suggests that a perfectly dispersed set of subsidies, leading to an inverse Herfindahl of $1$, is estimated to increase productivity by about $4.1$ percent. A one standard deviation increase in `compherf_subsidy` is associated with an increase of productivity by $1.4$ percent. Is this effect economically relevant? Recall our discussion on the meaning of productivity back in exercise 2. Productivity measures output relative to input and is therefore an efficiency parameter. Thus, we can interpret an increase of productivity by $4.1$ percent as follows:

- Using the same amount of factor inputs (labor hours, machine hours, etc.) firms are able to produce $4.1$ percent more output. Alternatively:
- Firms can reduce their factor inputs by about $4.1$ percent and still produce the same output.

Hence, the productivity gains implied by the effect seem to be meaningful from an economic point of view. To substantiate this claim, we can contrast this value with the average productivity growth in the sample. If the estimated effect is not negligible compared to, say, an average year of technological progress, we might conclude that effect is practically relevant. To perform this comparison, we compute the average firm-level TFP growth over the sample period. Since TFP has a logarithmic form, we can approximate productivity growth $\Delta TFP_{i,j,r,t}$ of firm $i,j,r$ at time $t$ as follows:
$$
\Delta TFP_{i,j,r,t} \approx ln \ TFP_{i,j,r,t} - ln \ TFP_{i,j,r,t-1}. \tag{12.2}
$$
For each firm in the sample, we compute the annual differences in logarithmic productivity to obtain estimates for firm-level productivity growth (Wooldridge (2016), p. 637). After that, we average these results for each year, and then over the entire sample period. Of course, this approach intends to provide an order of magnitude and is not an exact calculation. There may be more precise ways to aggregate the data, for instance by using firm sizes as weights or by relying on the geometric mean. I already prepared the next task to perform the computations. We already used all necessary functions in previous exercises.

**Task:** Compute the average firm-level TFP growth using `group_by()`, `mutate()` and `summarize()`. The code is already given below; just click `check`. The computations might take a few seconds. 
```{r}
#< task_notest
# Compute TFP growth for each firm
dat = dat %>% 
  group_by(id) %>% 
  mutate(g = tfp - lag(tfp, n = 1, order_by = year)) %>%
  ungroup()

# Compute annual TFP growth, then for the entire sample period
dat %>% 
  group_by(year) %>%
  summarize(g_annual = mean(g, na.rm = TRUE)) %>%
  summarize(tfp_growth = mean(g_annual, na.rm = TRUE))
#>
```

The average annual TFP growth over the sample period is about $4.86$ percent. Note that this number is probably an optimistic estimate, since the data is an *unbalanced panel*. Less innovate and productive firms may be more likely to exit the market, so there are no follow-up observations to compute productivity growth for less efficient firms. This introduces a *selection bias* due to "attrition" (Kennedy (2008), p. 289). For our main analysis, this should not be a problem. 

Recall that our OECD data from the first part of this problem set indicated an average TFP growth of about $1$ percent for manufacturing and service industries. Both datasets cover almost the same years. Therefore, the trajectory of Chinese manufacturing firms indicates substantially faster productivity growth compared to firms in OECD countries. For our comparison, we can conclude that a coefficient of $0.041$ for `compherf_subsidy` is not slim when contrasted with the productivity growth of about $0.0486$ per year. A transition from the least equitable to the most equitable dispersion scheme appears to stimulate innovation roughly comparable to an average year of productivity growth.

These results suggests that a less concentrated subsidy scheme leads to statistically and practically relevant improvements in productivity outcomes. However, the allocation of subsidies towards competitive industries (`cor_subsidy`) is not significantly improving productivity. In the next task, we focus on tax holidays.

**Task:** Regress `tfp` on `cor_tax`, `compherf_tax` and the controls. Specify the variables `id` and `year` as fixed effects and cluster the standard errors on the industry-level. Assign the results to `reg_tax` and show them using the `stargazer()` function. The code is already given below; just click `check`.
```{r results = 'asis', warning = FALSE}
#< task_notest
# Regression using tax holiday variables
reg_tax = felm(tfp ~ cor_tax + compherf_tax + cor_tariff + competition + competition_square + index_subsidy + index_tax + index_interest + exportshare_industry + stateshare + horizontal + backward + forward + lnTariff + lnbwTariff + lnfwTariff | id + year | 0 | industry, data = dat)

# Display Results
stargazer(reg_tax,
          type = "html",
          title = "Table 13.1: Tax Holidays",
          style = "aer", 
          digits = 3,
          column.labels = c("Tax Holidays"),
          omit = c("exportshare_industry", "stateshare", "index_subsidy", 
                   "index_tax", "index_interest", "lnTariff", "horizontal",
                   "backward", "forward", "lnbwTariff", "lnfwTariff"),
          add.lines = list(c("Full Set of Covariates", rep("Yes", 1)),
                           c("Firm Fixed Effects", rep("Yes", 1)),
                           c("Time Fixed Effects", rep("Yes", 1))))
#>
```


#< quiz "tax"
question: Interpret the coefficient of the correlation coefficient for tax holidays and competition. The estimate suggests that allocating tax holidays towards competitive industries...

sc:
    - ... increases productivity by about 1.5 percent.
    - ... increases productivity growth by about 1.5 percentage points.
    - ... decreases productivity by about 1.5 percent.*
    - ... decreases productivity growth by about 1.5 percentage points.

success: Great!
failure: Try again.
#>


Looking at the results, we see that the coefficient for `cor_tax` is estimated to be $-0.015$. The negative sign suggests that allocating tax holidays towards competitive industries **decreases** productivity by $1.5$ percent. This is the opposite direction of what we would expect, given our first hypothesis. The coefficient for `compherf_tax` is estimated to be $0.103$, which indicates that granting tax holidays in a perfectly equitable fashion increases productivity by roughly $10.3$ percent. A one standard deviation increase in `compherf_tax` is associated with a $2.2%$ higher firm-level performance, which is about half of the average annual productivity growth in the sample. Both coefficients are significant at the $1\%$ level. The next task focuses on low interest loans.

**Task:** Regress `tfp` on `cor_interest`, `compherf_interest` and the controls. Specify the variables `id` and `year` as fixed effects and cluster the standard errors on the industry-level. Assign the results to the variable `reg_int` and show them using the `stargazer()` function. The code is already given below; just click `check`.
```{r results = 'asis', warning = FALSE}
#< task_notest
# Regression using Interest variables
reg_int = felm(tfp ~ cor_interest + compherf_interest + cor_tariff + competition + competition_square + index_subsidy + index_tax + index_interest + exportshare_industry + stateshare + horizontal + backward + forward + lnTariff + lnbwTariff + lnfwTariff | id + year | 0 | industry, data = dat)

# Display Results
stargazer(reg_int,
          type = "html",
          title = "Table 13.2: Low Interest Loans",
          style = "aer", 
          digits = 3,
          column.labels = c("Low Interest"),
          omit = c("exportshare_industry", "stateshare", "index_subsidy", 
                   "index_tax", "index_interest", "lnTariff", "horizontal",
                   "backward", "forward", "lnbwTariff", "lnfwTariff"),
          add.lines = list(c("Full Set of Covariates", rep("Yes", 1)),
                           c("Firm Fixed Effects", rep("Yes", 1)),
                           c("Time Fixed Effects", rep("Yes", 1))))
#>
```


#< quiz "interest"
question: Interpret the coefficient of the reverse Herfindahl for low interest loans. The estimate suggests that a perfectly dispersed allocation of low interest loans within industries...

sc:
    - ... increases productivity by about 1.3 percent.
    - ... increases productivity by about 0.085 percent.
    - ... increases productivity by about 0.013 percent.
    - ... increases productivity by about 8.5 percent.*

success: Great!
failure: Try again.
#>


First, we consider `cor_interest`. The authors used the average industry interest rate to compute the variable. We have to be careful here: **more** industrial support is reflected by **lower** interest rates. Hence, the anticipated sign of the coefficient should be negative given our first hypothesis. The coefficient is $0.013$, suggesting an improved productivity of $1.3$ percent when interest rates are **higher**. This result does not provide evidence that the allocation of low interest loans to more competitive industries leads to improved outcomes. 

For the calculation of the reverse Herfindahl index, the authors used the difference between the average industry interest rate and the interest rate of firm $i$ (for firms below the average). The magnitude of this difference therefore is a proxy for the amount of state support granted to firm $i$ in terms of low interest loans by state banks or government authorities (see exercise 10). Hence, the anticipated direction of the correlation coefficient is positive (like the other reverse Herfindahls). The effect is estimated to be $0.085$, indicating that a perfect dispersion of low interest loans within industries leads to improved productivity by about $8.5$ percent. A one standard deviation increase is associated with $2.4$ percent higher productivity. The result is significant at the $1\%$ level. Again, the effect appears to be relevant when compared to the annual productivity growth of $0.0486$. In the upcoming task, we consider all policy variables at once.

**Task:** Regress `tfp` on the full set of policy variables and controls. Specify the variables `id` and `year` as fixed effects and cluster the standard errors on the industry-level. Assign the results to `reg_main` and show the results of `reg_main`, `reg_sub`, `reg_tax` and `reg_int` in a `stargazer` table. The code is already given below; just click `check`.
```{r results = 'asis', warning = FALSE}
#< task_notest
# Full set of policy variables
reg_main = felm(tfp ~ cor_subsidy + compherf_subsidy + cor_tax + compherf_tax + cor_interest + compherf_interest + cor_tariff + competition + competition_square + index_subsidy + index_tax + index_interest + exportshare_industry + stateshare + horizontal + backward + forward + lnTariff + lnbwTariff + lnfwTariff | id + year | 0 | industry, data = dat)

# Display results
stargazer(reg_main, reg_sub, reg_tax, reg_int,
          type = "html", 
          title = "Table 13.3: Industrial Policies - Main Results",
          style = "aer", 
          digits = 3,
          column.labels = c("All", "Subsidies", "Tax", "Interest"),
          omit = c("exportshare_industry", "stateshare", "horizontal",
                   "backward", "forward", "lnbwTariff", "lnfwTariff"),
          add.lines = list(c("Full Set of Covariates", rep("Yes", 4)),
                           c("Firm Fixed Effects", rep("Yes", 4)),
                           c("Time Fixed Effects", rep("Yes", 4))))
#>
```


The first column shows the regression results using all policies. Beginning with the correlation measures, we see that the coefficients stay practically the same. The policy variable we have not yet discussed is `cor_tariff`.


#< quiz "cor tariff"
question: Consider table 13.3. The data suggest that allocating tariffs towards competitive industries...

sc:
    - ... decreases productivity.*
    - ... increases productivity.

success: Great!
failure: Try again.
#>


The coefficient ranges from $-0.031$ to $-0.016$ and is significant in two estimations. The data do not provide evidence that allocating tariffs towards already competitive industries increases productivity (and potentially even has a negative effect). The authors interpret this result as being consistent with the perception that protecting already competitive industries with tariffs does not incentivize innovation and cost reduction investments. Overall, the data do not provide evidence that allocating resources specifically towards competitive industries is beneficial to firm performance. It is important to stress again that the correlation coefficients quantify potential complementarities. It is very well possible that the policies overall increase productivity. However, there appears to be no **additional** benefit of specifically favoring competitive industries.

#< info "Inaccuracies in the article"
In their article, the authors depict a more generous image of the "benefits" implied by the correlation measures. Firstly, they claim that allocating subsidies towards competitive industry increases productivity (Aghion et al. (2015), p. 16 and p. 23). Although the coefficient of the variable `cor_subsidy` is positive, it is very small ($0.00009$ in the full model) and not significantly different from zero in any of the regressions.

Secondly, the authors interpret the coefficient of `cor_tax`, although having a negative sign, as a positive effect (Aghion et al. (2015), p. 16 and p. 23). This interpretation is contradicted by the author's description of how the variable was constructed. They also specifically state that coefficient should be positive (Aghion et al. (2015), p. 14).

Regarding `cor_interest` and `cor_tariff`, the authors recognize that the estimated coefficients are not consistent with the predictions made by their theoretical framework (Aghion et al. (2015), p. 16). Overall, the provided evidence does not justify the claims made in the abstract and the conclusion of the article with respect to the allocation of industrial policies towards competitive industries (Aghion et al. (2015), p. 1 and p. 23). The presented supplementary analyses, which we omit in this problem set, do not change this matter.
#>

Continuing with the reverse Herfindahl measures, we see little change when comparing the results. All three variables remain still positive and highly significant, with slightly attenuated coefficients. The effects of equitable resource distributions across firms seems to have an economically relevant impact on firm-level performance. The data appear to be consistent with our second hypothesis. We will further discuss potential improvements upon equitable dispersion schemes in the next section. We conclude with a brief discussion of the most important controls.   

The estimated coefficients of `competition` and `competition_square` suggest that the marginal effect of competition is positive up until a turning point, indicated by the positive and negative sign, respectively (the turning point is estimated to be about $0.93$, see Wooldridge (2016), pp. 173-177 for a discussion on quadratics in regression models). This finding is consistent with Aghion et al. (2005) describing the relationship as an "inverted U-shape". At some point, intense competition might lead to less innovation and productivity growth. An interpretation of this finding could be that in very fierce competition, laggard firms have fewer incentives to innovate because they do not anticipate post-innovation rents (Aghion et al. (2005), p. 702).

The coefficients are not individually significant in our full model (however, they are significant in the original article using the alternative CSEs, see Aghion et al. (2015) p. 15). Here we encounter again the issue of little variation in an explanatory variable leading to relatively large standard errors (see also exercise 5). Recall that we only use initial period Lerners to describe the competitive environment over the entire sample period. By using the fixed effects transformation, all variables that are constant over time are projected out. So how can we still estimate the competition effect? As it turns out, some firms (about $18000$) **switched** the industry in which they operate at least once. This provides some variation in the variable since the intensity of competition across industries differs.

Carrying on with the policy variables, the coefficient of `index_subsidy` suggests an increased productivity of $0.7$ to $0.8$ percent for firms that received subsidies. Tax holidays are associated with about $2$ percent higher productivity (The percentage approximation we use here only works for small coefficients, the exact effect is given by $100 \cdot e^{(\beta_{coef})}-1$, see Kennedy (2008), p. 237). The negative sign of `index_interest` suggests that firms which benefitted from low interest loans might be between $1$ and $1.4$ percent less productive. For `lnTariff` we see that tariffs are positively correlated with firm performance, yet the coefficients are insignificant. Hence, after controlling for several variables, subsidies and tax holidays are still associated with superior firm performance. In the final section of our analysis, we investigate potential improvements upon equitable dispersion schemes of industrial policies.

### c) Targeting Innovative Enterprises

Recall why the dispersion of industrial policies within industries matters. We argued that supporting more firms should be more likely to foster innovation compared to targeting a single firm or a small subset of firms. Dispersed intervention patterns may maintain competition and incentivize more firms to innovate. The reverse Herfindahl measures capture this notion by taking on higher values for "broader" dispersion schemes. Given an industry, the complement of the Herfindahl index is maximized by providing the same amount of state aid to each firm. However, is this the best possible approach to allocate industrial policies? 

In the following, we ask whether we can improve on equitable dispersion schemes by specifically targeting firms that are most productive or most likely to innovate. In other words, we could allocate more resources towards a subset of firms that uses these resources more efficiently. The authors propose two targeting strategies. First, they point at literature suggesting that the largest firms also tend to be the most productive ones (for reasons such as experience effects, economies of scale, etc.). Thus, we could allocate more resources towards larger firms. To test this conjecture, we simply utilize modified Herfindahl indexes. For each policy, we can weight the support granted to firm $i$ with firm $i$'s size, as measured by employee count. This leads to higher reverse Herfindahls if the allocations of resources favor large firms. The replication data contain weighted variables for subsidies, tax holidays and loans, which we use in the next task.

**Task:** Modify the previous regression by replacing the unweighted Herfindahls with the variables `compherf_subsidy_size`, `compherf_tax_size` and `compherf_interest_size`. Store the results in the variable `reg_size`. The code is already given below; just click `check`.
```{r}
#< task_notest
# Regression weightsize
reg_size = felm(tfp ~ compherf_subsidy_size + compherf_tax_size + compherf_interest_size + cor_subsidy + cor_tax + cor_interest + cor_tariff + competition + competition_square + index_subsidy + index_tax + index_interest + exportshare_industry + stateshare + horizontal + backward + forward + lnTariff + lnbwTariff + lnfwTariff | id + year | 0 | industry, data = dat)
#>
```

If such a scheme were more effective, we would expect the coefficients on the modified policy variables to be larger than on their unweighted counterparts. Before we look at the results, we introduce the second targeting scheme. As an alternative, the authors propose to favor specifically young firms in the allocation. This might promote market entries of new innovative firms and decrease market concentration. To test this premise, we can modify the reverse Herfindahls again. Instead of weighting by firm size, we weight by the inverse of firm age ($\frac{1}{age} =\frac{1}{1}, \frac{1}{2},...$). Resources allocated towards older firms therefore contribute less to the reverse Herfindahls. The data also contain variables weighted according to this approach.

**Task:** Modify the previous regression by replacing the Herfindahl measures with the variables `compherf_subsidy_age`, `compherf_tax_age` and `compherf_interest_age`. Store the results in the variable `reg_age`. Compare the results to `reg_size` and `reg_main`. The code is already given below; just click `check`.
```{r results = 'asis', warning = FALSE}
#< task_notest
# Regression weightage
reg_age = felm(tfp ~ compherf_subsidy_age + compherf_tax_age + compherf_interest_age + cor_subsidy + cor_tax + cor_interest + cor_tariff + competition + competition_square + index_subsidy + index_tax + index_interest + exportshare_industry + stateshare + horizontal + backward + forward + lnTariff + lnbwTariff + lnfwTariff | id + year | 0 | industry, data = dat)

# Show results
stargazer(reg_size, reg_age, reg_main,
          type = "html", 
          title = "Table 13.4: Allocation of Industrial Policies",
          style = "aer", 
          digits = 3,
          column.labels = c("Size", "Age", "Unweighted"),
          omit = c("cor_subsidy", "cor_tax", "cor_interest", "cor_tariff", 
                   "competition", "competition_square", "index_subsidy", 
                   "index_tax", "index_interest", "exportshare_industry", 
                   "stateshare", "lnTariff", "horizontal", "backward", 
                   "forward", "lnbwTariff", "lnfwTariff"),
          add.lines = list(c("Full Set of Covariates", rep("Yes", 3)),
                           c("Firm Fixed Effects", rep("Yes", 3)),
                           c("Time Fixed Effects", rep("Yes", 3))))
#>
```


I excluded the covariates in the table above. The left column shows the coefficients of the Herfindahls weighted by firm size. All three coefficients decrease in magnitude compared to their unweighted counterparts displayed in the third column. Favoring large firms seems not to improve upon equitable dispersions. Looking at the second column, we see that the effect of subsidies increases about 3-fold when weighted by the inverse of firm age. This suggests that favoring young firms in the allocation of subsidies might lead to improved outcomes. The other two coefficients decrease slightly in magnitude, indicating no potential for improvements.

Summing up, the data suggest that equitable dispersions seem to have the most sizable impact on firm performance. According to these results, a broad and non-discriminatory way to support firms is best suited to foster innovation. We found some evidence that particularly young firms are a driver for productivity when targeted with subsidies.

### d) Final Notes on the Article

As final step in their analysis, the authors investigate how industrial policies affect the **reallocation of market shares**. The approach we employed thus far focused on intra-firm learning by linking industrial policy allocations to firm-level productivity. However, aggregate productivity gains can also be achieved by shifting market shares towards more efficient firms (Aghion and Schankerman (2004), p. 801). To investigate this mechanism, the authors redo their analysis using the Herfindahl measures and an aggregate measure of TFP on the industry-region level. Their results indicate that equitable dispersions of tax holidays and low interest loans significantly contributed to the reallocation of market shares towards more productive firms. Nevertheless, the major driver of productivity gains in the Chinese case remains intra-firm learning. The reallocation component merely accounts for about $5$ percent of aggregate productivity growth (Aghion et al. (2015), pp. 20-23).

Based on the insights of their analysis, the authors conclude that the debate on industrial policies should indeed be held along the lines of appropriate design and governance, and not for or against the desirability of industrial policy (Aghion et al. (2015), p. 25). In the upcoming exercise, we conclude this problem set.

## Exercise 13 -- Conclusion

Throughout the first part of this problem set, we aimed at estimating the causal effect of competition policy on productivity growth. To replicate the main findings in Buccirossi et al. (2013), we employed a range of econometric methods such as multiple linear regression, instrumental variable estimation and the exploitation of heterogeneous effects. We found strong evidence that good competition policy fosters productivity growth. In the second part, we explored two ways of designing an industrial policy that conforms to sound principles of competition. Following Aghion et al. (2015), we saw that competition-friendly distributions of subsidies, tax holidays and low interest loans appear to be more beneficial to firm performance. We have not found compelling evidence that targeting specifically competitive industries leads to improved outcomes.

What both articles do not provide, however, is a *cost-benefit analysis*. Even if we could establish that a policy increases productivity, this would not confirm that the intervention is also improving social welfare (Harrison and Rodr√≠guez-Clare (2009), pp. 4041-4042). For instance, subsidies and the costs of competition authorities have to be raised by the government. These expenses, and their associated opportunity costs, have to be compensated by the productivity gains induced by the policies to make society better off.

Finally, let us briefly consider an example to demonstrate that what we have discussed thus far is a recent issue, and not just a scholarly discussion. For the European Commission, industrial policy is "back on the political agenda" (Aiginger (2014), p. 24). In the 1990s, there was little interest in expanding industrial policy, and in particular, sectoral measures (i.e. discriminatory, industry-specific measures) were frowned upon. This changed especially in the aftermath of the financial and economic crisis of 2007/2008 (Aiginger (2014), pp. 7-13).

The Commission‚Äôs new approach to industrial policy explicitly implicates both horizontal and sectoral measures (European Commission (2010), pp. 1-4). Primary goal is to improve the competitiveness of firms to keep a potent industrial base in Europe. Especially manufacturing is perceived as a major driver for innovation and economic stability. Moreover, the new strategy also recognizes several goals beyond productivity and GDP considerations, such as the transformation to a sustainable and more inclusive economy. As of 2020, the European Commission presented its New Industrial Strategy for Europe. A notable statement is the desire to increase the strategic autonomy of the EU by means of industrial policy. The dependence on others for critical materials, infrastructure, etc. shall be reduced (European Commission (2020), p. 13-14).

Political actors have recognized the area of conflict between industrial policy and competition policy. The European Commission stresses that distortionary interventions and protectionism are not part of the new approach to industrial policy (European Commission (2020), p. 3). An article accompanying the OECD Policy Roundtable Competition Policy, Industrial Policy and National Champions" expresses it clearly:

> "Where and when industrial policy co-exists with competition policy, industrial policy should be respectful with sound competition principles." (OECD (2009) p. 12)

**Task:** Now check your hard-earned awards. I hope you enjoyed this problem set!
```{r optional=TRUE}
#< task_notest
awards()
#>
```

## Exercise References -- References

### Bibliography

- Aghion, Philippe; Boulanger, Julian; Cohen, Elie. (2011). *Rethinking Industrial Policy*. Bruegel Policy Brief. Issue 2011/04.

- Aghion, Philippe; Cai, Jing; Dewatripont, Mathias; Du, Luosha; Harrison, Ann; Legros, Patrick. (2015). *Industrial Policy and Competition*. American Economic Journal: Macroeconomics. 7(4): pp. 1-32.

- Aghion, Philippe; Schankerman, Mark (2004). *On the Welfare Effects and Political Economy of Competition-enhancing Policies*. The Economic Journal. 114: pp. 800‚Äì824.

- Aghion, Philippe; Bloom, Nick; Blundell, Richard; Griffith, Rachel; Howitt, Peter. (2005). *Competition and Innovation: An Inverted-U Relationship*. Quarterly Journal of Economics. 120: pp. 701‚Äì728.

- Aiginger, Karl. (2014). *Industrial Policy for a sustainable growth path*. WWWforEurope Policy Paper 13.

- Angrist, Joshua D.; Pischke, J√∂rn-Steffen (2009). *Mostly Harmless Econometrics. An Empiricist's Companion*. Princeton University Press. 

- Baker, Jonathan B. (2003). *The Case for Antitrust Enforcement*. Journal of Economic Perspectives. 17: pp. 27-50.

- Belleflamme, Paul; Peitz, Martin. (2015). *Industrial Organization: Markets and Strategies*. Second Edition. Cambridge University Press.

- Buccirossi, Paolo; Ciari, Lorenzo; Duso, Tomaso; Spagnolo, Giancarlo; and Vitale, Cristiana (2011). *Measuring the Deterrence Properties of Competition Policy: The Competition Policy Indexes*. Journal of Competition Law & Economics. 7(1): pp. 165-205.

- Buccirossi, Paolo; Ciari, Lorenzo; Duso, Tomaso; Spagnolo, Giancarlo; and Vitale, Cristiana (2013). *Competition Policy and Productivity Growth: an empirical Assessment*. The Review of Economics and Statistics. 95(4): pp. 1324-1336.

- Cameron, Colin A.; Miller, Douglas L. (2015). *A Practitioner‚Äôs Guide to Cluster-Robust Inference*. Journal of Human Resources. 50(2): pp. 317-372.

- Crandall, Robert W.; Winston, Clifford (2003). *Does Antitrust Policy Improve Consumer Welfare? Assessing the Evidence*. Journal of Economic Perspectives. 17: pp. 3-26.

- Criscuolo, Chiara; Martin, Ralf; Overman, Henry G.; Van Reenen, John. (2019). *Some Causal Effects of an Industrial Policy*. American Economic Review. 109(1): pp. 48-85.

- Du, Luosha; Harrison, Ann; Jefferson, Gary. (2012). *Testing for horizontal and vertical foreign investment spillovers in China, 1998-2007*. Journal of Asian Economics. 23: pp. 234-243.

- Du, Luosha; Harrison, Ann; Jefferson, Gary. (2014). *FDI Spillovers and Industrial Policy: The Role of Tariffs and Tax Holidays*. World Development. 64: pp. 366-383.

- European Commission. (2010). *Communication from the European Commission to the European Parliament, the Council, the European Economic and Social Committee and the Committee of the Regions. An Integrated Industrial Policy for the Globalization Era. Putting Competitiveness and Sustainability at Center Stage*. COM(2010) 614 final.

- European Commission. (2020). *Communication from the European Commission to the European Parliament, the Council, the European Economic and Social Committee and the Committee of the Regions. A new Industrial Strategy for Europe*. COM(2020) 102 final.

- Field, Andy; Miles, Jeremy; Field, Zoe. (2012). *Discovering Statistics using R*. SAGE Publications Ltd.

- Greene, W.H. (2003). *Econometric Analysis*. Fifth Edition. New Jersey. Pearson Education, Inc.

- Greenwald, Bruce; Stiglitz, Joseph E. (2006). *Helping Infant Industries Grow: Foundations of Trade Policies for Developing Countries*. American Economic Review 96(2): pp. 141-146.

- Griffith, Rachel; Redding, Stephen; Van Reenen, John. (2004). *Mapping the Two Faces of R&D: Productivity Growth in a Panel of OECD Industries*. Review of Economics and Statistics. 86(4): pp. 883‚Äì895.

- Harrison, Ann E.; Rodr√≠guez-Clare, Andr√©s. (2009). *Trade, Foreign Investment, and Industrial Policy for Developing Countries*. Handbook of Development Economics (5): pp. 4039-4198.

- Kabacoff, Robert I. (2015). *R in Action. Data Analysis and Graphics with R. Second Edition*. Manning Publications Co.

- Kennedy, Peter. (2008). *A Guide to Econometrics*. Sixth Edition. Blackwell Publishing Ltd.

- Morgan, Stephen L.; Winship, Christopher (2015). *Counterfactuals and Causal Inference. Methods and Principles for Social Research*. Second Edition. Cambridge University Press.

- Nickell, Stephen J. (1996). *Competition and Corporate Performance*. Journal of Political Economy. 104: pp. 724-746.

- Nunn, Nathan; Trefler, Daniel. (2010). *The Structure of Tariffs and Long-Term Growth*. American Economic Journal: Macroeconomics. 2(4): pp. 158‚Äì194.

- OECD. (2009). *Competition Policy, Industrial Policy and National Champions*. Policy Roundtables.

- Olley, Steven; Pakes, Ariel. (1996). *The Dynamics of Productivity in the Telecommunications Equipment Industry*. Econometrica. 64(6): pp. 1263-1297.

- Pearl, Judea; Mackenzie, Dana (2018). *The Book of Why. The New Science of Cause and Effect*. Penguin Books.

- Rodrik, Dani. (2004). *Industrial Policy for the Twenty-First Century*. Faculty Research Working Papers Series. RWP04-047.

- Stiglitz, Joseph E.; Lin, Justin Yifu; Monga, C√©lestin. (2013). *The Rejuvenation of Industrial Policy*. Policy Research Working Paper. 6628. The World Bank.

- United Nations Statistical Comission (2002). *International Standard Industrial Classification of All Economic Activities ISIC Rev. 3.1*.

- Voigt, Stefan (2006). *The Economic Effects of Competition Policy: Cross-Country Evidence Using Four New Indicators*. Journal of Developement Studies. 45: pp. 1225-1248.

- Wickham, Hadley; Grolemund, Garret (2016). *R for Data Science. Import, Tidy, Transform, Visualize, And Model Data*. O'Reilly Media, Inc.

- Wooldridge, J.M. (2016). *Introductory Econometrics: A Modern Approach*. 6th Edition. Boston, MA [i.a.]: Cengage Learning.

### R Packages

- Gaure, Simen. (2020). *lfe: Linear Group Fixed Effects*. R Package Version 2.8-5. https://cran.r-project.org/web/packages/lfe/index.html 

- Hlavac, Marek. (2020). *stargazer: Well-Formatted Regression and Summary Statistics Tables*. R package version 5.2.2. https://cran.r-project.org/web/packages/stargazer/index.html

- Kranz, Sebastian. (2020). *RTutor: Creating R exercises with automatic assement of student's solutions*. R package version 2020.4.05. https://github.com/skranz/RTutor 

- Tennekes, Martijn. (2020). *treemap: Treemap Visualization*. R Package Version 2.4-2. https://cran.r-project.org/web/packages/treemap/index.html

- Wei, Taiyun. (2020). *corrplot: Visualization of a Correlation Matrix*. R Package Version 0.84. https://cran.r-project.org/web/packages/corrplot/index.html

- Wickham, Hadley. (2020). *dplyr: A Grammar of Data Manipulation*. R Package Version 1.0.0. https://cran.r-project.org/web/packages/dplyr/index.html

- Wickham, Hadley. (2020). *ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics*. R Package Version 3.3.1. https://cran.r-project.org/web/packages/ggplot2/index.html 

- Wickham, Hadley. (2020). *tidyr: Tidy Messy Data*. R Package Version 1.1.0. https://cran.r-project.org/web/packages/tidyr/index.html

- Wilke, Claus O. (2020). *ggridges: Ridgeline Plots in 'ggplot2'*. R Package Version
0.5.2. https://cran.r-project.org/web/packages/ggridges/index.html

- Xie, Yihui (2020). *knitr: A General-Purpose Package for Dynamic Report Generation in R*. R Package Version 1.28. https://cran.r-project.org/web/packages/knitr/index.html

### Websites

- *Competition Act*. (1998). The National Archives of the UK Government. https://www.legislation.gov.uk/ukpga/1998/41/contents

- *Enterprise Act*. (2002). The National Archives of the UK Government. https://www.legislation.gov.uk/ukpga/2002/40/contents

- *The R Graph Gallery*. (2020). Treemaps. https://www.r-graph-gallery.com/treemap.html

- *The R Graph Gallery*. (2020). Ridgeline Plots. https://www.r-graph-gallery.com/ridgeline-plot.html
